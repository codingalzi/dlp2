{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5d5eb9f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4266a616",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987b9789",
   "metadata": {},
   "source": [
    "- 텍스트 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2741bfbf",
   "metadata": {},
   "source": [
    "- 단어 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f499a470",
   "metadata": {},
   "source": [
    "- 트랜스포머 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0de45cc",
   "metadata": {},
   "source": [
    "- 시퀀스-투-시퀀스 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce5abec",
   "metadata": {},
   "source": [
    "- 영어-한국어 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5395f00",
   "metadata": {
    "id": "x8OD2TTxZ8Om",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 자연어 처리 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c0e9cad",
   "metadata": {
    "id": "rTBmxXQxZ8On"
   },
   "source": [
    "- 컴퓨터 프로그래밍 언어: 파이썬, 자바, C, C++, C#, 자바스크립트 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019cd19a",
   "metadata": {
    "id": "rTBmxXQxZ8On"
   },
   "source": [
    "- 자연어: 일상에서 사용되는 한국어, 영어 등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc39f743",
   "metadata": {
    "id": "rTBmxXQxZ8On"
   },
   "source": [
    "- 자연어의 특성상 정확한 분석을 위한 알고리즘 구현 매우 어려움"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3654b9db",
   "metadata": {
    "id": "rTBmxXQxZ8On"
   },
   "source": [
    "- 딥러닝 기법이 활용 이전: 별로 성공적이지 못함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0443f76b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc92310c",
   "metadata": {},
   "source": [
    "- 1990년대: **언어의 이해**가 아닌 **통계적으로 유용한 정보를 예측** 연구 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2382cd9",
   "metadata": {},
   "source": [
    "- 텍스트 분류: \"이 텍스트의 주제는?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac28f6d",
   "metadata": {},
   "source": [
    "- 내용 필터링: \"욕설 사용?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58954c39",
   "metadata": {},
   "source": [
    "- 감성 분석: \"그래서 내용이 긍정이야 부정이야?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a20edc2",
   "metadata": {},
   "source": [
    "- 언어 모델링: \"이 문장 다음에 어떤 단어가 와야 할까?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56d1039",
   "metadata": {},
   "source": [
    "- 번역: \"이거를 한국어로 어떻게 말해?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36958573",
   "metadata": {},
   "source": [
    "- 요약: \"이 기사를 한 줄로 요약해 볼래?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10130069",
   "metadata": {
    "id": "RjKUDDVfZ8On",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 머신러닝 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0c6074",
   "metadata": {
    "id": "Y6rI3gM1Z8Oo"
   },
   "source": [
    "- 1990 - 2010년대 초반: \n",
    "    결정트리, 로지스틱 회귀 모델이 주로 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873b6d0d",
   "metadata": {
    "id": "Y6rI3gM1Z8Oo"
   },
   "source": [
    "- 2014-2015: LSTM 등 시퀀스 처리 알고리즘 활용 시작"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85a08f14",
   "metadata": {
    "id": "Y6rI3gM1Z8Oo"
   },
   "source": [
    "- 2015-2017: (양방향) 순환신경망이 기본적으로 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00452044",
   "metadata": {
    "id": "Y6rI3gM1Z8Oo"
   },
   "source": [
    "- 2017-2018: 트랜스포머<font size='2'>transformer</font> 아키텍처가 많은 난제들을 해결함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f35657",
   "metadata": {
    "id": "Y6rI3gM1Z8Oo"
   },
   "source": [
    "- 2022 이후: 트랜스포머 아키텍처를 이용한 GPT의 혁명적 발전"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6fe62e9",
   "metadata": {
    "id": "B48vvr6KZ8Oo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 텍스트 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb928a4",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-EmbeddingLayer-a.png\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb6926",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 텍스트 벡터화 단계"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a547c20",
   "metadata": {},
   "source": [
    "- **텍스트 표준화**<font size='2'>text standardization</font>: 소문자화, 마침표 제거 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bf074f",
   "metadata": {},
   "source": [
    "- **토큰화**<font size='2'>tokenization</font>: 기본 단위의 **유닛**<font size='2'>units</font>으로 쪼개기.\n",
    "    문자, 단어, 단어 집합 등이 토큰으로 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d33d4d",
   "metadata": {},
   "source": [
    "- **어휘 색인화**<font size='2'>vocabulary indexing</font>: 토큰 각각을 하나의 정수 색인(인덱스)으로 변환."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0a1907",
   "metadata": {
    "id": "hAD2s0DaZ8Op",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 텍스트 표준화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0df147fb",
   "metadata": {
    "id": "K8EaqY5xZ8Op"
   },
   "source": [
    "- 모두 소문자화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d25b6b",
   "metadata": {
    "id": "K8EaqY5xZ8Op"
   },
   "source": [
    "- `.`, `;`, `?`, `'` 등 특수 기호 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5470193",
   "metadata": {
    "id": "K8EaqY5xZ8Op"
   },
   "source": [
    "- 특수 알파벳 변환: \n",
    "    - \"&eacute;\"를 \"e\"로, \n",
    "    - \"&aelig;\"를 \"ae\"로 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a6f5777",
   "metadata": {
    "id": "K8EaqY5xZ8Op"
   },
   "source": [
    "- 동사/명사의 기본형 활용: \n",
    "    - \"cats\"를 \"[cat]\"로, \n",
    "    - \"was staring\"과 \"stared\"를 \"[stare]\"로 등등."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "878d7660",
   "metadata": {
    "id": "yQRj19aQZ8Op",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "다음 두 텍스트를 표준화를 통해 동일한 텍스트로 변환 가능\n",
    "\n",
    "    \"sunset came. i was staring at the Mexico sky. Isnt nature splendid??\"\n",
    "\n",
    "    \"Sunset came; I stared at the M&eacute;xico sky. Isn't nature splendid?\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4376cbe",
   "metadata": {
    "id": "ZoxhNN-RZ8Op"
   },
   "source": [
    "표준화 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e08709",
   "metadata": {
    "id": "g7lZN8sQZ8Oq"
   },
   "source": [
    "    \"sunset came i [stare] at the mexico sky isnt nature splendid\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be1f2f53",
   "metadata": {
    "id": "dVL9JLyNZ8Oq",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6247abfc",
   "metadata": {
    "id": "3lpJ96j0Z8Oq"
   },
   "source": [
    "- 단어 기준 토큰화\n",
    "    - 공백으로 구분된 단어들로 쪼개기. \n",
    "    - 경우에 따라 동사 어근과 어미를 구분하기도 함: \"star+ing\", \"call+ed\" 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7116b3c",
   "metadata": {
    "id": "3lpJ96j0Z8Oq"
   },
   "source": [
    "- N-그램 토큰화\n",
    "    - N-그램 토큰: 연속으로 위치한 N 개(이하)의 단어 묶음\n",
    "    - 예제: \"the cat\", \"he was\" 등은 2-그램(바이그램) 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621cd0f7",
   "metadata": {
    "id": "3lpJ96j0Z8Oq"
   },
   "source": [
    "- 문자 기준 토큰화\n",
    "    - 하나의 문자를 하나의 토큰으로 지정.\n",
    "    - 텍스트 생성, 음성 인식 등에서 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6ed42e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 단어 주머니"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2fdcb2a",
   "metadata": {
    "id": "5H6-1RjBZ8Oq"
   },
   "source": [
    "\"the cat sat on the mat.\" 에 대한 바이그램과 3-그램 주머니"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfc7bf50",
   "metadata": {
    "id": "nQwzf0c3Z8Oq"
   },
   "source": [
    "- 2-그램(바이그램) 주머니\n",
    "\n",
    "    ```\n",
    "    {\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
    "    \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f295839",
   "metadata": {
    "id": "hscQaMsgZ8Or"
   },
   "source": [
    "- 3-그램 주머니\n",
    "\n",
    "    ```\n",
    "    {\"the\", \"the cat\", \"cat\", \"cat sat\", \"the cat sat\",\n",
    "    \"sat\", \"sat on\", \"on\", \"cat sat on\", \"on the\",\n",
    "    \"sat on the\", \"the mat\", \"mat\", \"on the mat\"}\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b29292",
   "metadata": {
    "id": "3tCbQJOAZ8Or",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 어휘 색인화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76012495",
   "metadata": {
    "id": "H36AKaAIZ8Or"
   },
   "source": [
    "- 훈련셋에 포함된 모든 토큰들의 색인(인덱스) 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ee7e63",
   "metadata": {
    "id": "H36AKaAIZ8Or"
   },
   "source": [
    "- 보통 사용 빈도가 높은 1만, 2만, 또는 3만 개의 단어만 대상\n",
    "\n",
    "    ```python\n",
    "    from tensorflow.keras.datasets import imdb\n",
    "    \n",
    "    imdb.load_data(num_words=10000)\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02d45c16",
   "metadata": {
    "id": "H36AKaAIZ8Or"
   },
   "source": [
    "- 텍스트를 단어 색인으로 구성된 리스트로 변환\n",
    "\n",
    "    ```python\n",
    "    [1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 0, 0, 0, 0, 0]\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e01828",
   "metadata": {
    "id": "s34AVqltZ8Or",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### `TextVectorization` 층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cf9c48",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "```python\n",
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    standardize='lower_and_strip_punctuation',  # 기본값\n",
    "    split='whitespace',                         # 기본값\n",
    "    ngrams=None,                                # 기본값\n",
    "    output_mode='int',                          # 기본값\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "905b382d",
   "metadata": {
    "id": "s34AVqltZ8Or",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 텍스트 벡터화 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96639b8",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "```python\n",
    ">>> dataset = [\n",
    "...     \"I write, erase, rewrite\",\n",
    "...     \"Erase again, and then\",\n",
    "...     \"A poppy blooms.\",\n",
    "... ]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b1b220",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "어휘 색인은 `adapt()` 메서드를 이용하여 만든다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c7a3c3",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "```python\n",
    ">>> text_vectorization.adapt(dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c841a72d",
   "metadata": {
    "id": "s34AVqltZ8Or",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "생성된 어휘 색인은 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "749fab15",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "```python\n",
    ">>> vocabulary = text_vectorization.get_vocabulary()\n",
    ">>> vocabulary\n",
    "['',\n",
    " '[UNK]',\n",
    " 'erase',\n",
    " 'write',\n",
    " 'then',\n",
    " 'rewrite',\n",
    " 'poppy',\n",
    " 'i',\n",
    " 'blooms',\n",
    " 'and',\n",
    " 'again',\n",
    " 'a']\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935a9540",
   "metadata": {
    "id": "s34AVqltZ8Or",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "생성된 어휘 색인을 활용하여 새로운 텍스트를 벡터화 해보자."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "798e1fea",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "```python\n",
    ">>> test_sentence = \"I write, rewrite, and still rewrite again\"\n",
    ">>> encoded_sentence = text_vectorization(test_sentence)\n",
    ">>> print(encoded_sentence)\n",
    "tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57240ea0",
   "metadata": {
    "id": "s34AVqltZ8Or",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "벡터화된 텐서로부터 텍스트를 복원하면 표준화된 텍스트가 생성된다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de77b47",
   "metadata": {
    "id": "s34AVqltZ8Or"
   },
   "source": [
    "```python\n",
    ">>> inverse_vocab = dict(enumerate(vocabulary))\n",
    ">>> decoded_sentence = \" \".join(inverse_vocab[int(i)] for i in encoded_sentence)\n",
    ">>> print(decoded_sentence)\n",
    "i write rewrite and [UNK] rewrite again\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e856917b",
   "metadata": {
    "id": "BGXhJe5XZ8Ou",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### IMDB 영화 후기 데이터셋 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961a7a7a",
   "metadata": {
    "id": "KaO7ilwSZ8Ou",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**과정 1: 데이터셋 다운로드 후 압축 풀기**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037ab02c",
   "metadata": {
    "id": "6uzRLQDcZ8Ou"
   },
   "source": [
    "```\n",
    "aclImdb/\n",
    "...test/\n",
    "......pos/\n",
    "......neg/\n",
    "...train/\n",
    "......pos/\n",
    "......neg/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ded6e1",
   "metadata": {
    "id": "IfeoV0YCZ8Ov",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**과정 2: 검증셋 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e932696c",
   "metadata": {},
   "source": [
    "```\n",
    "aclImdb/\n",
    "...test/\n",
    "......pos/\n",
    "......neg/\n",
    "...train/\n",
    "......pos/\n",
    "......neg/\n",
    "...val/\n",
    "......pos/\n",
    "......neg/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbb1c17",
   "metadata": {
    "id": "fljBC3puZ8Ov",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**과정 3: 텐서 데이터셋 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af0f4f19",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e41g6BtPZ8Ov",
    "outputId": "b40fd231-c8d5-4ac9-86df-a291b3b40e64"
   },
   "source": [
    "```python\n",
    "from tensorflow.keras.utils import text_dataset_from_directory\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = text_dataset_from_directory(\"aclImdb/train\", \n",
    "                                        batch_size=batch_size)\n",
    "\n",
    "val_ds = text_dataset_from_directory(\"aclImdb/val\", \n",
    "                                     batch_size=batch_size)\n",
    "\n",
    "test_ds = text_dataset_from_directory(\"aclImdb/test\", \n",
    "                                      batch_size=batch_size)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c30acbc",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**과정 4: 텍스트 벡터화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87c4992",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "max_length = 600    # 후기 길이 제한\n",
    "max_tokens = 20000  # 단어 사용빈도 제한\n",
    "\n",
    "text_vectorization = layers.TextVectorization(\n",
    "                            max_tokens=max_tokens, \n",
    "                            output_mode=\"int\",\n",
    "                            output_sequence_length=max_length,\n",
    "                        )\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7293e702",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**어휘 색인화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87eca4f6",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "# 어휘 색인 생성 대상 훈련셋 후기 텍스트 데이터셋\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "# 어휘 색인\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fdab6fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**데이터셋 벡터화**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb90ded1",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "# 후기를 길이가 2만인 정수들의 리스트로 변환\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49f1bb8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "tf.Tensor(\n",
    "[   11     7     4  8614    18    38     9   139   138   197   640    12\n",
    "    30    22   167     6  3035     2    86  3146   664    19    12   291\n",
    "    11    14  2400  2996    13    55   322   429    11    19   172     4\n",
    "   337    35   116   230   172     4  1107     2   196  1562    14    12\n",
    "    10   399     9   100     9    14   478    46  1368   162    31    47\n",
    "   509    56     2  7585   645    66   733     5   239  1428     1    17\n",
    "     2    86    18     3    56    47   645    12    23    66     6    28\n",
    "     ...\n",
    "     ...\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0\n",
    "     0     0     0     0     0     0     0     0     0     0     0     0], \n",
    "shape=(600,), dtype=int64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f903172",
   "metadata": {
    "id": "HLeiiilMZ8Ou",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 단어 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2247893c",
   "metadata": {},
   "source": [
    "```python\n",
    "vocab_size = 7 # 총 어휘 수\n",
    "embed_dim = 4  # 단어별로 4 개의 특성 파악.\n",
    "\n",
    "embedding_layer = ayers.Embedding(input_dim=vocab_size, \n",
    "                                  output_dim=embed_dim)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7e50e62",
   "metadata": {
    "id": "6uzRLQDcZ8Ou"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-EmbeddingLayer.png\" style=\"width:80%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32643e0d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**임베딩 행렬**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd639710",
   "metadata": {
    "id": "6uzRLQDcZ8Ou",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "```python\n",
    "embedding_matrix = [[-0.012, 0.005, 0.008, 0.001],\n",
    "                    [0.236, -0.141, 0.000, 0.045],\n",
    "                    [0.006, 0.652, 0.270, -0.556],\n",
    "                    [0.305, 0.569, -0.028, 0.496],\n",
    "                    [0.421, 0.195, -0.058, 0.477],\n",
    "                    [0.844, -0.001, 0.763, 0.201],\n",
    "                    [0.466, -0.326, 0.884, 0.007]]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bada148",
   "metadata": {
    "id": "6uzRLQDcZ8Ou"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-EmbeddingLayer.png\" style=\"width:80%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51dddca",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**단어 임베딩의 의미**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e6f0c33",
   "metadata": {
    "id": "6uzRLQDcZ8Ou"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-WordEmbeddings-b.png\" style=\"width:95%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdfc2673",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **단어 임베딩 활용법**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5a7bbc",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "vocab_size = 20000 # 총 어휘 수\n",
    "embed_dim = 256    # 단어별로 256 개의 특성 파악.\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 단어 임베딩 실행\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "\n",
    "# 트랜스포머 인코더: 텍스트 내용 파악\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "# 출력층\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edea80f5",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 트랜스포머 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae30943e",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b89159",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-05.png\" style=\"width:60%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfa792bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **자연어 처리에서의 셀프 어텐션**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de7c7a1",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-self_attention.jpg\" style=\"width:80%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99c077ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 케라스 `MultiHeadAttention` 층"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c389a8",
   "metadata": {},
   "source": [
    "```python\n",
    "num_heads = 2 # 두 개의 셀프 어텐션 동시 진행. 각각 다른 문맥을 파악.\n",
    "embed_dim = 256 # 단어 임베딩된 벡터의 길이\n",
    "\n",
    "mha_layer = MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
    "outputs = mha_layer(inputs, inputs, inputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b226f2",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **질문-열쇠-값<font size='2'>query-key-value</font>**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b60d8df",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-07.png\" style=\"width:80%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b5b60e2",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 멀티헤드 어텐션"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213d7152",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-08.png\" style=\"width:60%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d111efe",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-multiheadattention01.png\" style=\"width:60%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643fe80c",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 트랜스포머 인코더"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a30c8293",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-09.png\" style=\"width:30%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1629297",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 트랜스포머 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b87316",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-14.png\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "497b7730",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **트랜스포머 인코더 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61810f2",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim  # 예를 들어 256\n",
    "        self.dense_dim = dense_dim  # 예를 들어 32\n",
    "        self.num_heads = num_heads  # 예를 들어 2\n",
    "        \n",
    "        # 어텐션 층 지정\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        # 밀집층 블록 지정\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        # 층 정규화 지정\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    # 트랜스포머 인코더의 순전파\n",
    "    def call(self, inputs, mask=None):\n",
    "        ...\n",
    "        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528269ae",
   "metadata": {
    "colab_type": "code",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        ...\n",
    "        \n",
    "    # 트랜스포머 인코더의 순전파\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    # 트랜스포머 인코더의 속성 지정\n",
    "    # 훈련된 모델을 저장할 때 활용됨\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eae85da",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **트랜스포머 인코더 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0123a655",
   "metadata": {},
   "source": [
    "```python\n",
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "# (600, 256) 모양의 단어 임베딩된 텍스트 텐서를\n",
    "# 길이가 256인 1차원 어레이로 변환.\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af4c8dc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 단어 위치 임베딩"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991a785c",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1] # 문장의 단어 수.\n",
    "        positions=tf.range(start=0, limit=length, delta=1) \n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions \n",
    "\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82416ade",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **단어위치 인식 트랜스포머 인코더**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd32766",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad0f3c8a064f687cbf898a0868fd45ba1c7e928ac8a0404f7c241d812ddc1e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
