{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 고급 컴퓨터 비전"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 주요 내용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 합성곱 신경망의 주요 활용 분야(컴퓨터 비전)\n",
    "    - 이미지 분류\n",
    "    - 이미지 분할\n",
    "    - 객체 탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 합성곱 신경망 기본 아키텍처\n",
    "    - 잔차 연결\n",
    "    - 배치 정규화\n",
    "    - 채널 분리 합성곱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 컴퓨터 비전 주요 과제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/computer_vision_tasks.png\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 기타 다양한 활용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 이미지 유사도 측정(image similarity scoring)\n",
    "- 키포인트 탐지(keypoint detection)\n",
    "- 자세 추정(pose estimation)\n",
    "- 3D 메쉬 추정(3D mesh estimation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://github.com/Jeff-sjtu/HybrIK/raw/main/assets/hybrik.png\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 객체 탐지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 작동원리 설명: 핸즈온 머신러닝 14장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 기초 활용법: [RetinaNet 활용 객체 탐지](https://keras.io/examples/vision/retinanet/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 주요 활용 예제: [YOLOv5](https://github.com/ultralytics/yolov5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 이미지 분할"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 시맨틱 분할(semantic segmentation)\n",
    "\n",
    "- 인스턴스 분할(instance segmentation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/instance_segmentation.png\" style=\"width:100%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Oxford-IIIT 애완동물 데이터셋"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터셋 크기: 7,390"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클래스(범주) 개수: 37"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 클래스별 사진 수: 약 200 장"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 사진별 라벨: 종과 품종, 머리 표시 경계상자, 트라이맵 분할<font size='2'>trimap segmentation</font> 마스크 등 4 종류로 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 트라이맵 분할 마스크"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 원본 사진과 동일한 크기의 칼라 사진\n",
    "- 1, 2, 3 셋 중에 하나의 값만 사용\n",
    "\n",
    "    - 1: 동물의 몸에 해당하는 픽셀\n",
    "    - 2: 배경에 해당하는 픽셀\n",
    "    - 3: 동물과 배경을 구분하는 경계에 해당하는 픽셀"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://www.robots.ox.ac.uk/~vgg/data/pets/pet_annotations.jpg\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 이미지 분할 모델 구성: 다운 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "code"
   },
   "source": [
    "```python\n",
    "inputs = keras.Input(shape=(200, 200, 3,))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "x = layers.Conv2D(64, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(128, 3, strides=2, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2D(256, 3, strides=2, padding=\"same\", activation=\"relu\")(x)\n",
    "x = layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 이미지 분할 모델 구성: 업 샘플링"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "code",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "```python\n",
    "x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(256, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(128, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.Conv2DTranspose(64, 3, activation=\"relu\", padding=\"same\", strides=2)(x)\n",
    "\n",
    "outputs = layers.Conv2D(3, 3, activation=\"softmax\", padding=\"same\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "input_1 (InputLayer)         [(None, 200, 200, 3)]     0         \n",
    "_________________________________________________________________\n",
    "rescaling (Rescaling)        (None, 200, 200, 3)       0         \n",
    "_________________________________________________________________\n",
    "conv2d (Conv2D)              (None, 100, 100, 64)      1792      \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 100, 100, 64)      36928     \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 50, 50, 128)       73856     \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 50, 50, 128)       147584    \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 25, 25, 256)       295168    \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 25, 25, 256)       590080    \n",
    "_________________________________________________________________\n",
    "conv2d_transpose (Conv2DTran (None, 25, 25, 256)       590080    \n",
    "_________________________________________________________________\n",
    "conv2d_transpose_1 (Conv2DTr (None, 50, 50, 256)       590080    \n",
    "_________________________________________________________________\n",
    "conv2d_transpose_2 (Conv2DTr (None, 50, 50, 128)       295040    \n",
    "_________________________________________________________________\n",
    "conv2d_transpose_3 (Conv2DTr (None, 100, 100, 128)     147584    \n",
    "_________________________________________________________________\n",
    "conv2d_transpose_4 (Conv2DTr (None, 100, 100, 64)      73792     \n",
    "_________________________________________________________________\n",
    "conv2d_transpose_5 (Conv2DTr (None, 200, 200, 64)      36928     \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)            (None, 200, 200, 3)       1731      \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 출력값과 타깃"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력값 모양: `(200, 200, 3)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 출력층으로 사용된 `Conv2D` 층의 소프트맥스 활성화 함수: \n",
    "    0, 1, 2 중에 하나의 값을 예측해야 하기에 3 개의 출력맵으로 구성된 텐서에 대해 \n",
    "    텐서의 마지막 차원, 즉 픽셀별로 작동."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델의 손실함수: `sparse_categorical_crossentropy` 지정. \n",
    "    타깃 정수 0, 1, 2가 각각 `(1, 0, 0)`, `(0, 1, 0)`, `(0, 0, 1)` 에 상응하도록 작동"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 업샘플링 작동법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 검정 테두리로 표시된 `2x2` 모양의 텐서에 포함된 하나의 항목에 초록색 바탕의 `3x3` 모양의 필터를 적용하여\n",
    "    왼편에 위치한 파랑색 바탕의 `3x3` 모양의 텐서 생성\n",
    "- 이 과정을 `2x2` 텐서의 모든 항목에 대해 적용.\n",
    "    단, 생성되는 `3x3` 모양의 텐서는 지정된 보폭 크기만큼 오른쪽 또는 아래로 이동시키며 겹침.\n",
    "- 중첩되는 영역에 포함된 항목은 해당 항목에 대해 계산된 모든 값들의 합으로 지정."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<p><div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch09-conv2d_transpose01.png\" style=\"width:400px;\"></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 모델 컴파일과 훈련"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.compile(optimizer=\"rmsprop\", \n",
    "              loss=\"sparse_categorical_crossentropy\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"oxford_segmentation\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "history = model.fit(train_input_imgs, train_targets,\n",
    "                    epochs=50,\n",
    "                    callbacks=callbacks,\n",
    "#                    batch_size=64, # 고성능 GPU 활용\n",
    "                    batch_size=16,  # 저성능 GPU 활용\n",
    "                    validation_data=(val_input_imgs, val_targets))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 훈련 결과: `batch_size = 16`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0JfmCB2pYvG",
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<p><div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch09-training02.png\" style=\"width:500px;\"></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 훈련 결과: `batch_size = 64`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0JfmCB2pYvG"
   },
   "source": [
    "<p><div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch09-training01.png\" style=\"width:500px;\"></div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 모델 예측"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0JfmCB2pYvG"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/09-06.png\" style=\"width:700px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## CNN 아키텍처 주요 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 모델 아키텍처: 모델 설계 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 심층 신경망 모델을 구성할 때 매우 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 신경망 모델은 주어진 문제와 데이터셋에 따라 적절한 층을 적절하게 구성해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 적절한 모델 아키텍처를 사용할 수록 적은 양의 데이터로 보다 빠르게 좋은 성능의 모델을 얻을 가능성이 높아짐"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 좋은 모델 아키텍처와 관련되어 정해진 이론은 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대신 많은 경험을 통한 직관이 모델 구성에 보다 중요한 역할 수행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 신경망 모델의 아키텍처: 모듈(블록), 계층, 재활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모듈(블록)을 재활용하여 쌓아 올린 계층 구조 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분의 합성곱 신경망 모델은 **특성 피라미드** 형식의 계층적 구조 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch08-vgg16.png\" style=\"width:700px;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 합성곱 신경망 주요 아키텍처 유형"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 블록을 연결하는 계층을 높게 쌓으면 그레이디언트 소실 문제 등으로 인해 모델의 훈련이 잘 이뤄지지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 이를 해결하는 다양한 해결책이 개발되었음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 여기서는 합성곱 신경망 구성에 사용되는 세 개의 주요 아키텍처 유형 소개\n",
    "    - 잔차 연결<font size='2'>residual connections</font>\n",
    "    - 배치 정규화<font size='2'>batch normalization</font>\n",
    "    - 채널 분리 합성곱<font size='2'>depthwise separable convolutions</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 잔차 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 그레이디언트 소실 문제: 층을 높이 쌓았을 때 역전파 과정에서 전달되어야 하는 손실값(오차)의 소실되는 문제\n",
    "- 잔차 연결로 해결 가능\n",
    "- 잔차 연결 아키텍처: 2015년 ILSVRC 이미지 분류 경진대회의 1등 모델에 사용됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/09-09.png\" style=\"width:40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 잔차 연결 핵심: 모양 맞추기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 맥스풀링을 사용하지 않는 경우\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "residual = x\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)  # padding 사용\n",
    "residual = layers.Conv2D(64, 1)(residual)                       # 필터 수 맞추기\n",
    "x = layers.add([x, residual])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 맥스풀링을 사용하는 경우\n",
    "\n",
    "```python\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "residual = x\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D(2, padding=\"same\")(x)                   # 맥스풀링\n",
    "residual = layers.Conv2D(64, 1, strides=2)(residual)            # 보폭 사용\n",
    "x = layers.add([x, residual])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "code",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "def residual_block(x, filters, pooling=False):\n",
    "    residual = x\n",
    "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    if pooling:                          # 맥스풀링 사용하는 경우\n",
    "        x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
    "        residual = layers.Conv2D(filters, 1, strides=2)(residual)\n",
    "    elif filters != residual.shape[-1]:  # 필터 수가 변하는 경우\n",
    "        residual = layers.Conv2D(filters, 1)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "    return x\n",
    "\n",
    "x = residual_block(x, filters=32, pooling=True)\n",
    "x = residual_block(x, filters=64, pooling=True)\n",
    "x = residual_block(x, filters=128, pooling=False)\n",
    "x = layers.GlobalAveragePooling2D()(x) # 채널 별로 하나의 값(채널 평균값) 선택\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://miro.medium.com/max/770/1*4T4y3kI0R9Alk_2pe6B4Pg.png\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 배치 정규화: 2015년에 발표된 한 논문에서 소개됨\n",
    "\n",
    "- 다음 층으로 넘겨주기 전에 정규화 진행\n",
    "\n",
    "    ```python\n",
    "    normalized_batch = (batch - np.mean(batch, axis=...)) / np.std(batch, axis=...)`\n",
    "    ```\n",
    "    \n",
    "- 케라스의 `layers.BatchNormalization` 층이 배치 정규하를 지원\n",
    "\n",
    "- ResNet50, EfficientNet, Xception 모델 등은 배치 정규화 없이는 제대로 훈련되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp/master/notebooks/images/batchNormalization.jpg\" style=\"width:60%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 정규화 사용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "x = ...\n",
    "x = layers.Conv2D(32, 3, use_bias=False)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = ...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 배치 정규화 작동 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 정규화는 특성맵 단위로 이뤄지며 정규화에 사용되는 평균값과 분산은 배치 단위로 계산됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `use_bias=False` 옵션: 배치 정규화에 의해 어차피 데이터의 평균값을 0으로 만들기에 굳이 편향 파라미터가 필요 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 활성화 함수 사용 위치: 배치 정규화 이후 실행 권장. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 모델 미세조정과 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화 층이 포함된 모델을 미세조정할 때 배치 정규화 층도 함께 동결(freeze)할 것 권장됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 배치 정규화 층을 동결하면 재활용되는 모델이 훈련될 때 계산한 데이터셋의 평균값과 분산이 대신 활용됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미세조정의 경우 훈련셋의 데이터가 모델이 기존에 훈련했을 때의 훈련셋과 비슷하다고 전제함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Xception 모델 (2017)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp/master/notebooks/images/xception.jpg\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 채널 분리 합성곱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 케라스의 `SeparableConv2D` 층은 `Conv2D` 층보다 적은 수의 가중치 파라미터를 사용하지만 성능이 좀 더 좋은 모델 생성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 채널 분리 합성곱<font size='2'>depthwise separable convolution</font>라 불림"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 2017년 Xception 모델 논문에서 소개됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SeparableConv2D 작동법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/09-10.png\" style=\"width:70%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Conv2D 작동 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://miro.medium.com/max/428/1*sYpl-7LlrtlOKW8RvlyKOg.png\" style=\"width:28%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SeparableConv2D 작동 원리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://miro.medium.com/max/604/1*JwCJCgN2UreEn3U1nwVj8Q.png\" style=\"width:40%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 층의 파라미터 개수 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경우 1: `3x3` 모양의 필터 64개를 3개의 채널을 갖는 입력 데이터에 사용할 경우\n",
    "\n",
    "    - `Conv2D`의 경우: `3*3*3*64 = 1,728`\n",
    "    - `SeparableConv2D`의 경우: `3*3*3 + 3*64 = 219`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 경우 2: `3x3` 모양의 필터 64개를 10개의 채널을 갖는 입력 데이터에 사용할 경우\n",
    "\n",
    "    - `Conv2D`의 경우: `3*3*10*64 = 5,760`\n",
    "    - `SeparableConv2D`의 경우: `3*3*10 + 10*64 = 730`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 채널 분리 합성곱 층의 약점"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 채널 분리 합성곱의 연산 CUDA에서 제대로 지원되지 않음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GPU를 사용하더라도 기존 `Conv2D` 층만을 사용한 모델에 비해 학습 속도에 별 차이가 없음"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 하지만 적은 수의 파라미터를 사용하기에 일반화 성능이 보다 좋은 모델을 구현한다는 점이 매우 중요"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CUDA와 cuDNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- CUDA(Compute Unified Device Architecture)\n",
    "    - CPU와 GPU를 동시에 활용하는 병렬 컴퓨팅을 지원하는 플랫폼\n",
    "    - C, C++, Fortran 등의 저수준 언어 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cuDNN(CUDA Deep Neural Network): CUDA를 활용하여 딥러닝 알고리즘의 실행을 지원하는 라이브러리\n",
    "    - Conv2D 등 특정 딥러닝 알고리즘에 대해서만 최적화됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 예제: 미니 Xception 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미니 Xception 모델을 직접 구현하여 강아지-고양이 이항분류 작업 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 구현에 사용되는 기법\n",
    "    - 모듈을 쌓을 수록 필터 수는 증가시키고, 텐서 크기는 작게.\n",
    "    - 층의 유닛은 적게, 모듈은 높게.\n",
    "    - 잔차 연결을 활용\n",
    "    - 모든 합성곱 층 이후에는 배치 정규화를 적용\n",
    "    - 채널 분리 합성곱 신경망 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 앞서 언급된 모든 기법은 컴퓨터 비전 프로젝트 일반에 적용될 수 있음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [DeepLabV3 모델](https://arxiv.org/abs/1802.02611): Xception 모델을 이용하는 2021년 기준 최고의 이미지 분할 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "# 데이터 증식 층\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "# 입력층\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "# 하나의 Conv2D 은닉층\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "# SeparableConv2D, BatchNormalization, MaxPooling2D 층으로 구성된 모듈 쌓기\n",
    "# 잔차 연결 활용\n",
    "for size in [32, 64, 128, 256, 512]:   # 필터 수\n",
    "    residual = x\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # 잔차 연결\n",
    "    residual = layers.Conv2D(\n",
    "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "```python\n",
    "# 마지막 은닉층은 GlobalAveragePooling2D과 Dropout\n",
    "x = layers.GlobalAveragePooling2D()(x)    # flatten 역할 수행(채널 별 평균값으로 구성)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 출력층\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# 모델 지정\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/HighResolutionFigures/figure_9-11.png\" style=\"width:80%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 성능 높이기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [하이퍼파라미터 조성: 그리드 탐색, 랜덤 탐색](https://codingalzi.github.io/handson-ml3/end2end_ml_project.html#id25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [앙상블 학습](https://codingalzi.github.io/handson-ml3/ensemble_learning_random_forests.html)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "collapsed_sections": [],
   "name": "dlp09_part01_image_segmentation",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "33b995ba57ec8806df76ad412cbfca6e91844af7e84c0aab5f00a2382a2b11c8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
