{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZZS6o0Z8Oj"
      },
      "source": [
        "# 11장 자연어처리 1부"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8OD2TTxZ8Om"
      },
      "source": [
        "## 11.1 자연어처리 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48vvr6KZ8Oo"
      },
      "source": [
        "## 11.2 텍스트 벡터화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLeiiilMZ8Ou"
      },
      "source": [
        "## 11.3 단어 모음 표현법: 집합과 시퀀스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uzRLQDcZ8Ou"
      },
      "source": [
        "여기서는 IMDB 영화 후기 데이터를 이용하여 두 모델 방식의 \n",
        "활용법과 차이점을 소개한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGXhJe5XZ8Ou"
      },
      "source": [
        "### 11.3.1 IMDB 영화 후기 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfwyzgmXZ8Ou"
      },
      "source": [
        "이전과는 달리 여기서는 IMDB 데이터셋을 직접 다운로드하여 전처리하는 \n",
        "과정을 살펴본다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaO7ilwSZ8Ou"
      },
      "source": [
        "준비 과정 1: 데이터셋 다운로드 압축 풀기\n",
        "\n",
        "압축을 풀면 아래 구조의 디렉토리가 생성된다.\n",
        "\n",
        "```\n",
        "aclImdb/\n",
        "...train/\n",
        "......pos/\n",
        "......neg/\n",
        "...test/\n",
        "......pos/\n",
        "......neg/\n",
        "```\n",
        "\n",
        "`train`의 `pos`와 `neg` 서브디렉토리에 각각 12,500개의 긍정과 부정 후기가\n",
        "포함되어 있다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScCXFlWZ8Ou",
        "outputId": "e7b26c14-5a5e-42ed-87d4-80b8770a1079"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9232k      0  0:00:08  0:00:08 --:--:-- 16.3M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gzs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMRKwRM9Z8Ou"
      },
      "source": [
        "`aclImdb/train/unsup` 서브디렉토리는 필요 없기에 삭제한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "2xsKw4vPZ8Ou"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !rm -r aclImdb/train/unsup\n",
        "else: \n",
        "    import shutil\n",
        "    unsup_path = './aclImdb/train/unsup'\n",
        "    shutil.rmtree(unsup_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rZ3a9ETZ8Ou"
      },
      "source": [
        "긍정 후기 하나의 내용을 살펴보자.\n",
        "모델 구성 이전에 훈련 데이터셋을 살펴 보고\n",
        "모델에 대한 직관을 갖는 과정이 항상 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK5FRFX0Z8Ou",
        "outputId": "e0a84704-6ebf-4615-ba72-6eeb55bd6f4f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !cat aclImdb/train/pos/4077_10.txt\n",
        "else:\n",
        "    with open('aclImdb/train/pos/4077_10.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeoV0YCZ8Ov"
      },
      "source": [
        "준비 과정 2: 검증셋 준비\n",
        "\n",
        "훈련셋의 20%를 검증셋으로 떼어낸다.\n",
        "이를 위해 `aclImdb/val` 디렉토리를 생성한 후에\n",
        "긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "xX30tWVBZ8Ov"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)            # val 디렉토리 생성\n",
        "    files = os.listdir(train_dir / category)\n",
        "    \n",
        "    random.Random(1337).shuffle(files)         # 훈련셋 무작위 섞기\n",
        "    \n",
        "    num_val_samples = int(0.2 * len(files))    # 20% 지정 후 검증셋으로 옮기기\n",
        "    val_files = files[-num_val_samples:]\n",
        "    \n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBC3puZ8Ov"
      },
      "source": [
        "준비 과정 3: 텐서 데이터셋 준비\n",
        "\n",
        "`text_dataset_from_directory()` 함수를 이용하여 \n",
        "훈련셋, 검증셋, 테스트셋을 준비한다. \n",
        "자료형은 모두 `Dataset`이며, 배치 크기는 32를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41g6BtPZ8Ov",
        "outputId": "be03ba4b-3d47-4247-ddcc-dc18a33e1172"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLi_rBmlZ8Ov"
      },
      "source": [
        "각 데이터셋은 배치로 구분되며\n",
        "입력은 `tf.string` 텐서이고, 타깃은 `int32` 텐서이다.\n",
        "크기는 모두 32이며 지정된 배치 크기이다.\n",
        "예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G05FSfO1Z8Ov",
        "outputId": "36ad1c64-0b78-46b5-dfd9-938e1447a284"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'****SPOILERS**** The film \"Sniper\" is undoubtedly based on the exploits of legendary US Marine sniper Carlos \"Gunny\" Hathcock. The unassuming soft-spoken Mister Rogers look-alike who ran up a score of as much as 300 confirmed and unconfirmed Vietcong and North Vietnamese military kills during his two tours in \"Nam\".Which shows just how deadly and effective a trained military sniper really is.<br /><br />Tom Berenger is cool clam and deadly as Sgt.Thomas Beckett who\\'s at the end of his career as a top US Marine sniper but who later in the movie realizes that a life as a civilian will be pointless. Since there\\'s nothing outside for him to do with his skills that he learned in the US Marines unless he decides to become a mob hit-man. Backett reluctantly accepts his fate as a lifetime professional killer for his country.<br /><br />The story of the film \"Sniper\" is focused on Sgt.Beckett with the assistance of former sharp shooting silver medalist and US government agent Richard Miller, Billy Zane, being sent deep inside the Panamanian jungle. The two snipers are to take out rebel General Miguel Alveraze, Frederick Miraglittoa, and Colombian drug king-pin Raul Ochoa, Carlos Alveraze, who\\'s supporting him in a planned a military take-over of the country. <br /><br />We see earlier in the movie Sgt. Beckett scope and take out a rebel leader which I feel was the best scene in \"Sniper\". For it shows step by step how Sgt. Beckett with the help of his spotter Cpt. Papich, Aden Young, does his job. There\\'s also a sub-plot that was later aborted in the movie about a rebel sniper DeSilva, Eward Wiley, who was stalking Beckett and who later killed Papich as they were both waiting to be lifted out of the jungle by a military helicopter. You would have thought that a deadly cat and mouse was being played out between the two that would culminate when the movie ended but Sgt. Beckett had no trouble at all in dispatching DeSilva early in the film by using an unsuspecting Miller as bait.<br /><br />What hurt the movie the most was ironically the last fifteen or so minutes when the story went from a one shot one kill sniper movie to a Rambo-like ending with Sgt. Beckett and Agent Miller fighting off an entire battalion of rebels with bullets flying as thick as a London fog.<br /><br />\"Sniper\" is still well worth watching for the fact that it tells the story about a person who until now has not really been glamorized in war movies: A solitary killer who kills with the precision and skill of a master diamond cutter or accomplished neurosurgeon and who does it in total secrecy.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    \n",
        "    # 예제: 첫째 배치의 첫째 후기\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAVLNfkAZ8Ov"
      },
      "source": [
        "### 11.3.2 단어주머니 기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke4S9dNAZ8Ov"
      },
      "source": [
        "**방식 1: 유니그램 멀티-핫 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv53srVkZ8Ov"
      },
      "source": [
        "`TextVectorization` 클래스의 `output_mode=\"multi_hot\"` 옵션을 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "VqPH5YjlZ8Ov"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "# 어휘색인 생성\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It6Yxh3BZ8Ow"
      },
      "source": [
        "생성된 어휘색인을 이용하여 훈련셋, 검증셋, 테스트셋 모두 벡터화한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "kjDRTte3Z8Ow"
      },
      "outputs": [],
      "source": [
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvWbCZP_Z8Ow"
      },
      "source": [
        "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
        "`max_tokens=20000`으로 지정하였기에 모든 문장은 길이가 2만인 벡터로 변환되었다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgkB1GBiZ8Ow",
        "outputId": "0b0e2bc9-db72-4017-f7e0-4a7294e42747"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5MpMNs0Z8Ow"
      },
      "source": [
        "*밀집 모델 활용*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylif_1jYZ8Ow"
      },
      "source": [
        "단어주머니 모델로 여기서는 밀집 모델을 사용한다. \n",
        "`get_model()` 함수가 컴파일 된 단순한 밀집 모델을 반환한다.\n",
        "모델의 출력값은 긍정일 확률이며, \n",
        "최상위 층의 활성화 함수로 `sigmoid`를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "H0F71SW_Z8Ow"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # 긍정일 확률 계산\n",
        "    \n",
        "    model = keras.Model(inputs, outputs)\n",
        "    \n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A72xJj81Z8Ow",
        "outputId": "8a0cc5a0-4c2f-4751-e433-e2f851de2d07"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e797rZZfZ8Ow"
      },
      "source": [
        "밀집 모델 훈련과정은 특별한 게 없다.\n",
        "훈련 후 테스트셋에 대한 정확도가 89% 보다 조금 낮게 나온다.\n",
        "최고 성능의 모델이 테스트셋에 대해 95% 정도 정확도를 내는 것보다는 낮지만\n",
        "무작위로 찍는 모델보다는 훨씬 좋은 모델이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_HTlND7Z8Ow",
        "outputId": "ec0e919b-9d08-4c12-ee3e-015cc0563548"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 10s 12ms/step - loss: 0.4097 - accuracy: 0.8288 - val_loss: 0.2863 - val_accuracy: 0.8892\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2772 - accuracy: 0.8979 - val_loss: 0.2845 - val_accuracy: 0.8942\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2424 - accuracy: 0.9151 - val_loss: 0.2947 - val_accuracy: 0.8942\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2307 - accuracy: 0.9214 - val_loss: 0.3101 - val_accuracy: 0.8910\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2229 - accuracy: 0.9246 - val_loss: 0.3275 - val_accuracy: 0.8892\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2197 - accuracy: 0.9274 - val_loss: 0.3413 - val_accuracy: 0.8874\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2174 - accuracy: 0.9291 - val_loss: 0.3532 - val_accuracy: 0.8874\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2150 - accuracy: 0.9312 - val_loss: 0.3692 - val_accuracy: 0.8800\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2134 - accuracy: 0.9324 - val_loss: 0.3730 - val_accuracy: 0.8814\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2130 - accuracy: 0.9325 - val_loss: 0.3788 - val_accuracy: 0.8824\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2882 - accuracy: 0.8874\n",
            "Test acc: 0.887\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSR-OuWBZ8Ow"
      },
      "source": [
        "**방식 2: 바이그램 멀티-핫 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-6ZgkkZZ8Ox"
      },
      "source": [
        "`TextVectorization` 클래스의 `ngrams=N` 옵션을 이용하면\n",
        "N-그램들로 이루어진 어휘색인을 생성할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "RI1QftoAZ8Ox"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNSDESNZ8Ox"
      },
      "source": [
        "어휘색인 생성과 훈련셋, 검증셋, 테스트셋의 벡터화 과정은 동일하다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "IwK6WQY4Z8Ox"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmDQyWhEZ8Ox"
      },
      "source": [
        "훈련 후 테스트셋에 대한 정확도가 90%를 조금 웃돌 정도로 많이 향상되었다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Flk8dIZ8Ox",
        "outputId": "1941bddf-84d1-4ae3-ad4c-f51bd1ea4975"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 8s 12ms/step - loss: 0.3749 - accuracy: 0.8504 - val_loss: 0.2578 - val_accuracy: 0.9020\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2433 - accuracy: 0.9147 - val_loss: 0.2616 - val_accuracy: 0.9052\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2013 - accuracy: 0.9330 - val_loss: 0.2757 - val_accuracy: 0.9068\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1919 - accuracy: 0.9405 - val_loss: 0.2893 - val_accuracy: 0.9058\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1866 - accuracy: 0.9451 - val_loss: 0.3058 - val_accuracy: 0.9066\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1784 - accuracy: 0.9475 - val_loss: 0.3146 - val_accuracy: 0.9052\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1781 - accuracy: 0.9489 - val_loss: 0.3304 - val_accuracy: 0.9036\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1728 - accuracy: 0.9499 - val_loss: 0.3346 - val_accuracy: 0.9064\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1708 - accuracy: 0.9526 - val_loss: 0.3473 - val_accuracy: 0.9068\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1874 - accuracy: 0.9521 - val_loss: 0.3534 - val_accuracy: 0.9030\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2617 - accuracy: 0.8997\n",
            "Test acc: 0.900\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31J9DJI6Z8Ox"
      },
      "source": [
        "**방식 3: 바이그램 TF-IDF 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRL6wVHKZ8Ox"
      },
      "source": [
        "`output_mode=\"tf_idf\"` 옵션을 사용하면 TF-IDF 인코딩을 지원한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "fo-6bcsmZ8Ox"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5BGCzr0Z8Oy"
      },
      "source": [
        "훈련 후 테스트셋에 대한 정확도가 다시 89% 아래로 내려간다.\n",
        "여기서는 별 도움이 되지 않았지만 많은 텍스트 분류 모델에서는 1% 정도의 성능 향상을 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgz5cvX9Z8Oy",
        "outputId": "79bacf29-675c-4dc8-bb47-c8f578a6768d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 0.5317 - accuracy: 0.7610 - val_loss: 0.3231 - val_accuracy: 0.8602\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3488 - accuracy: 0.8464 - val_loss: 0.3242 - val_accuracy: 0.8618\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2959 - accuracy: 0.8753 - val_loss: 0.3097 - val_accuracy: 0.8914\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2741 - accuracy: 0.8859 - val_loss: 0.3273 - val_accuracy: 0.8938\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2566 - accuracy: 0.8967 - val_loss: 0.3236 - val_accuracy: 0.8960\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2363 - accuracy: 0.9067 - val_loss: 0.3174 - val_accuracy: 0.9014\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2282 - accuracy: 0.9097 - val_loss: 0.3281 - val_accuracy: 0.8928\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 5ms/step - loss: 0.2205 - accuracy: 0.9137 - val_loss: 0.3306 - val_accuracy: 0.8862\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2171 - accuracy: 0.9149 - val_loss: 0.3401 - val_accuracy: 0.8828\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2032 - accuracy: 0.9189 - val_loss: 0.3451 - val_accuracy: 0.8854\n",
            "782/782 [==============================] - 8s 9ms/step - loss: 0.3125 - accuracy: 0.8834\n",
            "Test acc: 0.883\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3uJTQVlJSy"
      },
      "source": [
        "### 11.3.3 시퀀스 활용법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZ34Qg7lJSy"
      },
      "source": [
        "**정수 벡터 데이터셋 준비**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoVifu3DlJSy"
      },
      "source": [
        "훈련셋의 모든 후기 문장을 정수들의 벡터로 변환한다.\n",
        "단, 후기 문장이 최대 600개의 단어만 포함하도록 한다. \n",
        "또한 사용되는 어휘는 빈도 기준 최대 2만 개로 제한한다. \n",
        "\n",
        "- `max_length = 600`\n",
        "- `max_tokens = 20000`\n",
        "- `output_sequence_length=max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "orh58qTYlJSy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94MVnSHlJSy"
      },
      "source": [
        "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
        "`output_sequence_length=600`으로 지정하였기에 모든 문장은 단어를 최대 600개에서\n",
        "잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.\n",
        "물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다. \n",
        "또한 벡터에 사용된 정수는 2만보다 작은 값이며, \n",
        "이는 빈도가 가장 높은 2만개의 단어만을 대상(`max_tokens=20000`)으로 했기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B-SbA7rlJSy",
        "outputId": "62cf9c30-eb37-48c0-ecf0-a02895377212"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 600)\n",
            "inputs.dtype: <dtype: 'int64'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(\n",
            "[    1    37   226  4343   265    37 19719     6  2781   539     8    11\n",
            "   957   929    18     2    20   182    39     9    14    91     8  3729\n",
            "     6    32   144  2972   775    36  4330  2781   223    14  1626    65\n",
            "   321   429    42   246    99   501   518   544     8    11    29     4\n",
            "   354  7407  2781   329     7   259     6   215   142    37     2  2662\n",
            "   745     1     6   325    46     3   132     2   113     7  2123     3\n",
            "  5230     2     1    16   462    34   429  1453  6041   656   127     4\n",
            "   481     1     2   795    24    49    94     2    18    38   632     3\n",
            "    85     3     5   258   207   157    39 11520 16065   385     4  8887\n",
            "   825  2075    49   572     6     2   115   612   382     4    18    16\n",
            "   250     4   164   341   126    16     4    50   468     3   132   320\n",
            "    31     9    94     9     4  1689   809    17 15135  1083  3493   282\n",
            "   207    34   429 10121    15  3485    21     2   395     1   458     8\n",
            "   328  6900     1     4   220    68    16   929    18   455     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in int_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ejQHih-lJS0"
      },
      "source": [
        "#### 11.3.3.1 원-핫 단어 벡터"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2O4G_ijMh0mm"
      },
      "source": [
        "아래 코드는 `tf.one_hot()` 함수를 전처리로 활용하는 순환 신경망 모델을 정의한다. 이렇게 하면 정수 벡터를 바로 모델의 입력값으로 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ydEYjxlJS0",
        "outputId": "a3d57fe2-7ddc-4929-f9bf-5b9bc4649ada"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # None 대신 600을 사용해도 됨\n",
        "\n",
        "# 원-핫 인코딩\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)  # (600, 20000) 모양의 출력값 생성\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bqet0GflJS0"
      },
      "source": [
        "모델 훈련이 매우 느리다. \n",
        "이유는 입력 데이터가 너무 많은 특성을 갖기 때문이다. \n",
        "입력 데이터 하나의 모양과 특성 수는 다음과 같다.\n",
        "\n",
        "- 모양: `(600, 20000)`\n",
        "- 특성 수: `600 * 20,000 = 12,000,000`\n",
        "\n",
        "양방향 LSTM은 엄청난 양의 반복을 실행하기에 당연히 훈련 시간이 길어진다.\n",
        "게다가 훈련된 모델의 성능이 별로 좋지 않다.\n",
        "테스테셋에 대한 정확도가 87% 정도에 불과해서\n",
        "바이그램 모델보다 성능이 낮다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPqEPwkLlJS1"
      },
      "source": [
        "**주의사항**: 모델 훈련과정을 한 번 보기만 하려면 `epochs=1`로 설정하는 것을 권장한다.\n",
        "책에서는 원래 `epochs=10`을 사용하였는데 컴퓨터 성능에 따라 몇 시간이 소요될 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTTTH31ylJS1",
        "outputId": "5fe69509-5c7d-4d61-c263-b51f414422b8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 174s 269ms/step - loss: 0.5356 - accuracy: 0.7442 - val_loss: 0.3696 - val_accuracy: 0.8668\n",
            "782/782 [==============================] - 104s 132ms/step - loss: 0.3697 - accuracy: 0.8661\n",
            "Test acc: 0.866\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOlU0yElJS1"
      },
      "source": [
        "#### 11.3.3.2. 단어 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiogA5TlJS1"
      },
      "source": [
        "아래 코드는 단어 임베딩을 모델 구성에 직접 활용하는 것을 보여준다.\n",
        "여전히 양방향 LSTM 층을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY_Vv2chlJS1",
        "outputId": "25f8dc7c-9980-4b18-8d79-8b6593d11100"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 39s 57ms/step - loss: 0.4960 - accuracy: 0.7717 - val_loss: 0.3461 - val_accuracy: 0.8650\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.3082 - accuracy: 0.8842 - val_loss: 0.3679 - val_accuracy: 0.8440\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.2491 - accuracy: 0.9124 - val_loss: 0.3051 - val_accuracy: 0.8814\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.2025 - accuracy: 0.9286 - val_loss: 0.3188 - val_accuracy: 0.8796\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 55ms/step - loss: 0.1719 - accuracy: 0.9409 - val_loss: 0.3827 - val_accuracy: 0.8526\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.1433 - accuracy: 0.9531 - val_loss: 0.3221 - val_accuracy: 0.8844\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.1216 - accuracy: 0.9583 - val_loss: 0.4284 - val_accuracy: 0.8836\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.1001 - accuracy: 0.9686 - val_loss: 0.4181 - val_accuracy: 0.8778\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.0797 - accuracy: 0.9758 - val_loss: 0.4422 - val_accuracy: 0.8802\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.0709 - accuracy: 0.9787 - val_loss: 0.3886 - val_accuracy: 0.8736\n",
            "782/782 [==============================] - 23s 29ms/step - loss: 0.3604 - accuracy: 0.8614\n",
            "Test acc: 0.861\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")  # None 대신 600을 사용해도 됨\n",
        "\n",
        "# 단어 임베딩\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8yQOrDlJS1"
      },
      "source": [
        "훈련은 원-핫 인코딩 방식보다 훨씬 빠르게 이루어지며 성능은 87% 정도로 비슷하다. \n",
        "바이그램 모델보다 성능이 여전히 떨어지는 이유 중에 하나는 후기에 사용된 단어의 수를 600개로 제한하였기 때문이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUNZKa1JlJS2"
      },
      "source": [
        "**패딩과 마스킹**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2XbN_LlJS2"
      },
      "source": [
        "아래 코드는 마스킹을 활용하는 방식을 보여준다.\n",
        "\n",
        "- `mask_zero=True` 옵션: 마스킹 옵션 켜기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsAh7IpUlJS2",
        "outputId": "31c969e7-b782-4f0f-d79b-d73a8682f992"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 50s 68ms/step - loss: 0.4091 - accuracy: 0.8112 - val_loss: 0.2817 - val_accuracy: 0.8882\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 40s 64ms/step - loss: 0.2375 - accuracy: 0.9084 - val_loss: 0.2685 - val_accuracy: 0.8944\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 40s 64ms/step - loss: 0.1741 - accuracy: 0.9343 - val_loss: 0.3120 - val_accuracy: 0.8926\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.1369 - accuracy: 0.9502 - val_loss: 0.2894 - val_accuracy: 0.8894\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 37s 60ms/step - loss: 0.1042 - accuracy: 0.9634 - val_loss: 0.3503 - val_accuracy: 0.8844\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 40s 64ms/step - loss: 0.0750 - accuracy: 0.9743 - val_loss: 0.3850 - val_accuracy: 0.8890\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 41s 66ms/step - loss: 0.0557 - accuracy: 0.9826 - val_loss: 0.4003 - val_accuracy: 0.8798\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 41s 65ms/step - loss: 0.0424 - accuracy: 0.9860 - val_loss: 0.5170 - val_accuracy: 0.8824\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 40s 65ms/step - loss: 0.0339 - accuracy: 0.9890 - val_loss: 0.5783 - val_accuracy: 0.8768\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 40s 65ms/step - loss: 0.0235 - accuracy: 0.9919 - val_loss: 0.5472 - val_accuracy: 0.8782\n",
            "782/782 [==============================] - 26s 30ms/step - loss: 0.3043 - accuracy: 0.8774\n",
            "Test acc: 0.877\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\") # None 대신 600을 사용해도 됨\n",
        "\n",
        "# 마스킹 활용 단어 임베딩\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrSKzVyklJS2"
      },
      "source": [
        "모델 성능이 88% 정도로 살짝 향상된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARMW4aZzlJS2"
      },
      "source": [
        "#### 11.3.3.3. GloVe 단어 임베딩"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay48poX2lJS2"
      },
      "source": [
        "[glove.6B.zip](http://nlp.stanford.edu/data/glove.6B.zip) 파일을 다운로드해서 압축을 푼다.\n",
        "그러면 다음 세 개의 파일이 생성된다.\n",
        "\n",
        "- `glove.6B.50d.txt` (약 1 GB)\n",
        "- `glove.6B.100d.txt` (약 670 MB)\n",
        "- `glove.6B.200d.txt` (약 330 MB)\n",
        "- `glove.6B.300d.txt` (약 160 MB)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC-wYwztlJS2",
        "outputId": "d9705634-8ebf-4cee-d0f5-2c16919a95e7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-30 14:52:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-11-30 14:52:53--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-11-30 14:52:54--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  5.01MB/s    in 2m 40s  \n",
            "\n",
            "2022-11-30 14:55:35 (5.15 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -q glove.6B.zip\n",
        "else: \n",
        "    try: \n",
        "        import wget, zipfile\n",
        "    except ModuleNotFoundError: \n",
        "        !pip install wget\n",
        "        \n",
        "    import wget, zipfile\n",
        "    wget.download('http://nlp.stanford.edu/data/glove.6B.zip')\n",
        "    with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "총 40만 개의 단어 정보를 포함한 `glove.6B.100d.txt` 파일을 이용하여 단어 임베딩을 실행하기 위해\n",
        "먼저 파일 내용을 사전으로 가져온다.\n",
        "\n",
        "- `embeddings_index`: 키(key)는 단어, 값(value)은 단어와 연관된 길이가 100인 벡터의 쌍으로\n",
        "이뤄진 사전을 가리킨다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMCuC4cslJS2",
        "outputId": "e5dac214-0b09-442e-c32f-64eacbbc8b27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9YWmdEllJS2"
      },
      "source": [
        "단어 임베딩에 사용될 (20000, 100) 모양의 행렬을 준비한다.\n",
        "행렬의 각 행은 훈련셋 영화 후기에 포함된 단어 각각에 해당하는\n",
        "길이가 100인 벡터로 구성도다.\n",
        "각 단어와 연결된 벡터는 `embeddings_index`를 이용해서 구한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "qH5olPZNlJS2"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:  # 굳이 필요 없음. 이유는 len(vocabulary)가 2만이기 때문임.\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGYQV_4jlJS3"
      },
      "source": [
        "`Embedding` 객체를 생성할 때 앞서 생성한 `embedding_matrix`를 \n",
        "단어 임베딩의 기준으로 사용하도록 지정한다.\n",
        "\n",
        "- `Constant`: 단어 임베딩의 기준으로 사용될 행렬을 상수로 지정한다.\n",
        "- `trainable=False`: 단어 임베딩의 기준이 모델 훈련 과정 내내 고정시킨다. \n",
        "    즉, 단어 임베딩에 대한 별도 훈련은 시키지 않는다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "oPgZRjjalJS3"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f78ea03",
      "metadata": {},
      "source": [
        "모델 구성은 임베딩 층만 제외하고 이전과 동일하다.\n",
        "훈련 결과가 특별히 나아지지는 않는다. 이유는 훈련 데이터셋이 충분히 크기에 굳이 외부 임베딩 파일을\n",
        "이용하지 않아도 자체적으로 단어들 사이의 관계를 잘 찾기 때문이다.\n",
        "하지만 훈련셋이 더 작은 경우 잘 훈련된 임베딩 파일을 이용하면 보다 효율적으로 훈련이 진행된다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XryyZ10lJS3",
        "outputId": "e53292a8-9398-404e-da79-0bbcb143bd30"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 64)               34048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 43s 57ms/step - loss: 0.5712 - accuracy: 0.6942 - val_loss: 0.4389 - val_accuracy: 0.7982\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 32s 52ms/step - loss: 0.4521 - accuracy: 0.7958 - val_loss: 0.4646 - val_accuracy: 0.7952\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.3976 - accuracy: 0.8242 - val_loss: 0.3759 - val_accuracy: 0.8298\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.3604 - accuracy: 0.8442 - val_loss: 0.3402 - val_accuracy: 0.8484\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.3326 - accuracy: 0.8590 - val_loss: 0.3208 - val_accuracy: 0.8622\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3132 - accuracy: 0.8697 - val_loss: 0.3277 - val_accuracy: 0.8610\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.2951 - accuracy: 0.8771 - val_loss: 0.3116 - val_accuracy: 0.8704\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.2798 - accuracy: 0.8870 - val_loss: 0.3043 - val_accuracy: 0.8736\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.2670 - accuracy: 0.8925 - val_loss: 0.2999 - val_accuracy: 0.8784\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.2533 - accuracy: 0.8982 - val_loss: 0.2972 - val_accuracy: 0.8780\n",
            "782/782 [==============================] - 21s 24ms/step - loss: 0.2936 - accuracy: 0.8751\n",
            "Test acc: 0.875\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# GloVe 단어 임베딩 활용\n",
        "embedded = embedding_layer(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "6c86b3592b6800d985c04531f2c445f0fa6967131b8dd6395a925f7622e55602"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
