{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9장 컴퓨터 비전을 위한 고급 딥러닝 2부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**감사말**: 프랑소와 숄레의 [Deep Learning with Python, Second Edition](https://www.manning.com/books/deep-learning-with-python-second-edition?a_aid=keras&a_bid=76564dff) 9장에 사용된 코드에 대한 설명을 담고 있으며 텐서플로우 2.6 버전에서 작성되었습니다. 소스코드를 공개한 저자에게 감사드립니다.\n",
    "\n",
    "**tensorflow 버전과 GPU 확인**\n",
    "- 구글 코랩 설정: '런타임 -> 런타임 유형 변경' 메뉴에서 GPU 지정 후 아래 명령어 실행 결과 확인\n",
    "\n",
    "    ```\n",
    "    !nvidia-smi\n",
    "    ```\n",
    "\n",
    "- 사용되는 tensorflow 버전 확인\n",
    "\n",
    "    ```python\n",
    "    import tensorflow as tf\n",
    "    tf.__version__\n",
    "    ```\n",
    "- tensorflow가 GPU를 사용하는지 여부 확인\n",
    "\n",
    "    ```python\n",
    "    tf.config.list_physical_devices('GPU')\n",
    "    ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "## 9.3 합성곱 신경망 기본 아키텍처"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 아키텍처**는 모델 설계방식을 의미하며\n",
    "딥러닝 모델을 구성할 때 매우 중요하다.\n",
    "주어진 데이터셋과 해결해야 문제에 따라 적절한 층을 적절하게 구성해서\n",
    "모델을 구현해야 한다.\n",
    "좋은 모델 아키텍처를 사용할 수록 적은 양의 데이터을 이용하여 보다 빠르게\n",
    "좋은 성능의 모델을 얻을 가능성이 높아진다. \n",
    "아쉽게도 좋은 모델 아키텍처와 관련된 이론은 없으며\n",
    "많은 경험을 통한 직관이 보다 중요한 역할을 수행한다. \n",
    "\n",
    "여기서는 실전에서 최고 성능을 발휘한 합성곱 신경망 몇 개를 이용하여\n",
    "주요 합성곱 신경망 모델의 기본이 되는 아키텍처 3 개를 살펴본다.\n",
    "\n",
    "- 잔차 연결(residual connections)\n",
    "- 배치 정규화(batch normalization)\n",
    "- 채널 분리 합성곱(depthwise separable convolutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 9.3.1 블록, 계층, 재활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모든 유명한 합성곱 신경망 모델은 **블록(모듈)**을 **계층**적으로 쌓아 올린 구조를 갖는다.\n",
    "여기서 블록(모듈)은 여러 개의 층(레이어)으로 구성되며, 하나의 블록(모듈)이 여러 번 **재활용**되기도 한다. \n",
    "예를 들어, 8장에서 다룬 VGG16 모델은 \"Conv2D, Conv2D, MaxPooling2D\" 로 구성된 블록(모듈)을 \n",
    "계층적으로 구성하였다. \n",
    "\n",
    "대부분의 합성곱 신경망 모델의 또다른 특징는 **특성 피라미드** 형식의 계층적 구조를 사용하는 점이다.\n",
    "VGG16의 경우에 필터 수를 32, 64, 128 등으로 수를 늘리는 반면에 특성맵(feature maps)의 \n",
    "크기는 그에 상응하게 줄여 나간다(아래 그림 참조)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/09-08.png\" style=\"width:80%;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python(Manning MEAP)](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**좁은 층 깊은 신경망 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "일반적으로 많은 유닛이 포함된 층을 몇 게 쌓는 것보다\n",
    "적은 유닛이 포함된 층을 높이 쌓을 때 모델의 성능이 좋아진다. \n",
    "하지만 층을 높이 쌓을 수록\n",
    "전달되어야 하는 손실값(오차)의 **그레이디언트 소실 문제**(vanishing gradient problem)가 \n",
    "발생하여 이를 극복해야 하는 아키텍처(설계방식)을 사용해야 한다.\n",
    "이를 위한 대표적인 아키텍처는 **잔차 연결**(residual connections)이다.\n",
    "\n",
    "**참고**: 층을 통과할 때 어쩔 수 없이 발생하는 노이즈(잡음) 때문에 그레이디언트 소실 문제가 발생한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 9.3.2 잔차 연결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "블록(모듈)의 입력값을 블록을 통과하여 생성된 출력값과 합쳐서 다음 블록으로 전달하는 아키텍처이다(아래 그림 참조).\n",
    "이 방식을 통해 블록의 입력값에 대한 정보가 보다 정확하게 상위 블록에 전달되어,\n",
    "그레이디언트 소실 문제를 해결하는 데에 많은 도움을 준다.\n",
    "실제로 잔차 연결을 이용하면 블록을 매우 높게 쌓아도 모델 훈련이 가능하다.\n",
    "\n",
    "**참고**: 잔차 연결 아키텍처는 2015년에 소개된 ResNet 계열의 모델에서 처음 사용되었으며,\n",
    "2015년 ILSVRC 이미지 분류 경진대회에서 1등을 차지했다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/09-09.png\" style=\"width:30%;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python(Manning MEAP)](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**잔차 연결 핵심: 모양(shape) 맞추기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "잔차 연결을 사용할 때 주의해야할 기본사항은 블록의 입력텐서와 출력테서의 모양을 맞추는 일이다.\n",
    "이때 맥스풀링 사용여부에 따라 보폭(`strides`)의 크기가 달라진다.\n",
    "\n",
    "먼저, 맥스풀링을 사용하지 않는 경우엔 `Conv2D`에서 사용된 필터 수를 맞추는 데에만 주의하면 된다.\n",
    "\n",
    "- `Conv2D` 층: `padding=\"same\"` 옵션을 사용하여 모양을 유지\n",
    "- 필터 수가 변하는 경우: 잔차에 `Conv2D` 층을 이용하여 필터 수를 맞춤. 필터 크기는 `1x1` 사용.\n",
    "    활성화 함수는 없음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "\n",
    "residual = x\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)  # padding 사용\n",
    "\n",
    "residual = layers.Conv2D(64, 1)(residual)                       # 필터 수 맞추기\n",
    "\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "맥스풀링을 사용하는 경우엔 보폭을 활용해야 한다.\n",
    "\n",
    "- 잔차에 `Conv2D` 층을 적용할 때 보폭 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Conv2D(32, 3, activation=\"relu\")(inputs)\n",
    "\n",
    "residual = x\n",
    "\n",
    "x = layers.Conv2D(64, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "x = layers.MaxPooling2D(2, padding=\"same\")(x)                   # 맥스풀링\n",
    "\n",
    "residual = layers.Conv2D(64, 1, strides=2)(residual)            # 보폭 사용\n",
    "\n",
    "x = layers.add([x, residual])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예제**\n",
    "\n",
    "아래 코드는 잔차 연결을 사용하는 활용법을 보여준다.\n",
    "맥스풀링과 필터 수에 따른 구분을 사용함에 주의하라."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# 입력층\n",
    "inputs = keras.Input(shape=(32, 32, 3))\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "\n",
    "# 은닉층\n",
    "def residual_block(x, filters, pooling=False):\n",
    "    residual = x\n",
    "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    x = layers.Conv2D(filters, 3, activation=\"relu\", padding=\"same\")(x)\n",
    "    if pooling:                          # 맥스풀링 사용하는 경우\n",
    "        x = layers.MaxPooling2D(2, padding=\"same\")(x)\n",
    "        residual = layers.Conv2D(filters, 1, strides=2)(residual)\n",
    "    elif filters != residual.shape[-1]:  # 필터 수가 변하는 경우\n",
    "        residual = layers.Conv2D(filters, 1)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "    \n",
    "    return x\n",
    "\n",
    "x = residual_block(x, filters=32, pooling=True)\n",
    "x = residual_block(x, filters=64, pooling=True)\n",
    "x = residual_block(x, filters=128, pooling=False)\n",
    "\n",
    "x = layers.GlobalAveragePooling2D()(x) # 채널 별로 하나의 값(채널 평균값) 선택\n",
    "\n",
    "# 출력층\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# 모델 설정\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 구성 요약은 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "rescaling (Rescaling)           (None, 32, 32, 3)    0           input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 32)   896         rescaling[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 32)   9248        conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 32)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 32)   128         rescaling[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 32)   0           max_pooling2d_1[0][0]            \n",
      "                                                                 conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 64)   18496       add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 64)   36928       conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 8, 8, 64)     2112        add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_3 (Add)                     (None, 8, 8, 64)     0           max_pooling2d_2[0][0]            \n",
      "                                                                 conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 128)    73856       add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 128)    147584      conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 128)    8320        add_3[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 8, 8, 128)    0           conv2d_13[0][0]                  \n",
      "                                                                 conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 128)          0           add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            129         global_average_pooling2d[0][0]   \n",
      "==================================================================================================\n",
      "Total params: 297,697\n",
      "Trainable params: 297,697\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 9.3.3 배치 정규화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**정규화**(normalization)는 다양한 모양의 샘플을 정규화를 통해 보다 유사하게 만들어\n",
    "모델의 학습을 도와주고 훈련된 모델의 일반화 성능을 올려준다.\n",
    "지금까지 살펴 본 정규화는 모델의 입력 데이터를 전처리 과정에서 평균을 0으로, \n",
    "표준편차를 1로 만드는 방식이었다. \n",
    "이는 데이터셋이 정규분포를 따른다는 가정 하에 진행된 정규화였다.\n",
    "아래 그림은 주택가격 예측 데이터의 특성 중에 주택가격과 건축년수를 정규화한 경우(오른편)와 그렇지 않는 경우(왼편)의\n",
    "데이터 분포의 변화를 보여준다.\n",
    "\n",
    "**참고**: 정규분포를 따르지 않는 데이터에 대한 분석은 기본적으로 머신러닝(딥러닝) 기법을 적용할 수 없다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://miro.medium.com/max/770/1*4T4y3kI0R9Alk_2pe6B4Pg.png\" style=\"width:70%;\"></div>\n",
    "\n",
    "그림 출처: [Batch Normalization — Speed up Neural Network Training](https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 입력 데이터셋에 대한 정규화는 출력값의 정규화를 전혀 보장하지 않는다.\n",
    "따라서 다음 층으로 넘겨주기 전에 정규화를 먼저 진행하면 보다 훈련이 잘 될 수 있다.\n",
    "더 나아가 출력값을 먼저 정규화한 후에 활성화 함수를 적용할 때 보다 좋은 성능의 모델이 구현될 수 있음이\n",
    "발혀지기도 했다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp/master/notebooks/images/batchNormalization.jpg\" style=\"width:70%;\"></div>\n",
    "\n",
    "그림 출처: [Batch Normalization — Speed up Neural Network Training](https://medium.com/@ilango100/batch-normalization-speed-up-neural-network-training-245e39a62f85)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**배치 정규화**(batch normalization)가 바로 앞서 설명한 기능을 대신 처리하며,\n",
    "2015년에 발표된 한 논문에서 소개되었다. \n",
    "케라스의 경우 `layers.BatchNormalization` 층이 배치 정규하를 지원한다.\n",
    "\n",
    "배치 정규화로 인한 모델 성능 향상에 대한 구체적인 이론은 아직 존재하지 않는다.\n",
    "다만 경험적으로 많은 합성곱 신경망 모델의 성능에 도움을 준다는 사실만 알려져 있을 뿐이다.\n",
    "잔차 연결과 함께 배치 정규화 또한 모델 훈련과정에 그레이디언트 역전파에 도움을 주어\n",
    "매우 깊은 딥러닝 모델의 훈련에 도움을 준다.\n",
    "예를 들어, ResNet50, EfficientNet, Xception 모델 등은 배치 정규화 없이는 제대로 훈련되지 않는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**배치 정규화 사용법**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`BatchNormalization` 층을 `Conv2D`, `Dense` 등 임의의 층 다음에 사용할 수 있다.\n",
    "주로 사용되는 형식은 다음과 같다.\n",
    "\n",
    "```python\n",
    "x = ...\n",
    "x = layers.Conv2D(32, 3, use_bias=False)(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Activation(\"relu\")(x)\n",
    "x = ...\n",
    "```\n",
    "\n",
    "- `use_bias=False` 옵션: 배치 정규화에 의해 어차피 데이터의 평균값을 0으로 만들기에 굳이 편향(bias) 파라미터를\n",
    "    훈련 과정 중에 따로 학습시킬 이유가 없다. 따라서 학습되어야 할 파라미터 수가 아주 조금 줄어들어\n",
    "    학습 속도가 그만큼 빨라진다.\n",
    "- 활성화 함수 사용 위치: 배치 정규화 이후에 활성화 함수를 실행한다. \n",
    "    이를 통해 `relu()` 활성화 함수의 기능을 극대화할 수 있다(고 주장된다)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 미세조정(fine-tuning)과 배치 정규화**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "배치 정규화 층이 포함된 모델을 미세조정할 때 해당 배치 정규화 층을 동결(freeze)할 것을 추천한다. \n",
    "미세조정의 경우 모델 파라미터가 급격하게 변하지 않기에 배치 정규화에 사용된 평균값과 표준편차를 굳이\n",
    "업데이트할 필요는 없기 때문이다(라고 추정된다). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예제**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 그림은 2017년에 소개된 Xception 모델의 구조를 보여준다. \n",
    "빨강색 사각형으로 표시된 부분에 `BatchNormalization` 층이 사용되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp/master/notebooks/images/xception.jpg\" style=\"width:100%;\"></div>\n",
    "\n",
    "그림 출처: [Xception: Deep Learning with Depthwise Separable Convolutions](https://arxiv.org/abs/1610.02357)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 9.3.4 채널 분리 합성곱"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스의 `SeparableConv2D` 층은 `Conv2D` 층보다\n",
    "적은 수의 가중치 파라미터를 사용하여 보다 적은 양의 계산으로 성능이 좀 더 좋은 모델을 생성한다. \n",
    "2017년 Xception 모델 논문에서 소개되었으며 당시 최고의 이미지 분류 성능을 보였다.\n",
    "\n",
    "**참고**: 최신 이미지 분류 모델의 성능은 \n",
    "[Image Classification on ImageNet](https://paperswithcode.com/sota/image-classification-on-imagenet)을 \n",
    "참조한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SeparableConv2D 작동법**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`SeparableConv2D`는 필터를 채널 별로 적용한 후\n",
    "나중에 채널 별 결과를 합친다.\n",
    "이렇게 작동하는 층이 **채널 분리 합성곱**(depthwise separable convolution) 층이며\n",
    "아래 그림처럼 채널 별로 생성된 결과를 합친 후 `1x1` 합성곱 신경망을 통과시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/09-10.png\" style=\"width:70%;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python(Manning MEAP)](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SeparableConv2D 작동 원리**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이미지에 저장된 정보가 채널 별로 서로 독립적이라는 가정을 사용한다. \n",
    "따라서 채널 별로 서로 다른 필터를 사용한 후 결과들을 `1x1` 모양의 필터를 사용하여 합친다.\n",
    "이때 원하는 종류의 채널 수 만큼의 `1x1` 모양의 필터를 사용하여\n",
    "다양한 정보를 추출한다.\n",
    "\n",
    "`Conv2D`와 `SeparableConv2D`의 서로 다른 작동과정은 다음과 같이 설명된다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `Conv2D` 작동 원리\n",
    "    - `padding=\"same\"` 옵션 사용한 경우\n",
    "    - 필터 1개 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://miro.medium.com/max/428/1*sYpl-7LlrtlOKW8RvlyKOg.png\" style=\"width:28%;\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `SeparableConv2D` 작동 원리\n",
    "    - `padding=\"same\"` 옵션 사용한 경우\n",
    "    - 필터 1개 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://miro.medium.com/max/604/1*JwCJCgN2UreEn3U1nwVj8Q.png\" style=\"width:60%;\"></div>\n",
    "\n",
    "그림 출처: [Depth-wise Convolution and Depth-wise Separable Convolution](https://medium.com/@zurister/depth-wise-convolution-and-depth-wise-separable-convolution-37346565d4ec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**학습가능한 파라미터 수 비교**\n",
    "\n",
    "채널 분리 합성곱 신경망이 `Conv2D` 층을 사용하는 경우보다 몇 배 이상 적은 수의\n",
    "파라미터를 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경우 1(위 그림): `3x3` 모양의 필터 64개를 3개의 채널을 갖는 \n",
    "입력 데이터에 사용할 경우 학습가능한 파라미터 수는 각각 다음과 같다.\n",
    "\n",
    "- `Conv2D`의 경우: `3*3*3*64 = 1,728`\n",
    "- `SeparableConv2D`의 경우: `3*3*3 + 3*64 = 219`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경우 2: `3x3` 모양의 필터 64개를 10개의 채널을 갖는 \n",
    "입력 데이터에 사용할 경우 학습가능한 파라미터 수는 각각 다음과 같다.\n",
    "\n",
    "- `Conv2D`의 경우: `3*3*10*64 = 5,760`\n",
    "- `SeparableConv2D`의 경우: `3*3*10 + 10*64 = 730`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "경우 2: `3x3` 모양의 필터 64개를 64개의 채널을 갖는 \n",
    "입력 데이터에 사용할 경우 학습가능한 파라미터 수는 각각 다음과 같다.\n",
    "\n",
    "- `Conv2D`의 경우: `3*3*64*64 = 36,864`\n",
    "- `SeparableConv2D`의 경우: `3*3*64 + 64*64 = 4,672`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SeparableConv2D의 약점**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "채널 분리 합성곱에 사용되는 알고리즘이 CUDA에서 지원되지 않는다.\n",
    "따라서 GPU를 사용하더라도 기존 `Conv2D` 층만을 사용한 모델에 비해\n",
    "학습 속도에 별 차이가 없다. \n",
    "즉 채널 분리 합성곱이 비록 훨씬 적은 수의 파라미터를 학습에 사용하지만\n",
    "이로 인해 시간상의 이득은 주지 않는다.\n",
    "하지만 적은 수의 파라미터를 사용하기에 일반화 성능이 보다 좋은 모델을\n",
    "구현한다는 점이 매우 중요하다.\n",
    "\n",
    "**참고**: CUDA와 cuDNN\n",
    "\n",
    "- CUDA(Compute Unified Device Architecture)\n",
    "    - CPU와 GPU를 동시에 활용하는 병렬 컴퓨팅을 지원하는 플랫폼\n",
    "    - C, C++, Fortran 등의 저수준 언어 활용\n",
    "- cuDNN(CUDA Deep Neural Network): CUDA를 활용하여 딥러닝 알고리즘의 실행을 지원하는 라이브러리\n",
    "    - Conv2D 등 특정 딥러닝 알고리즘에 대해서만 최적화됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**예제**: Xception 모델\n",
    "\n",
    "케라스에서 지원하는 Xception 모델의 구성은 2017년 모델과 조금 다르지만\n",
    "기본적으로 `SeparableConv2D`와 `BatchNormalizaiton` 층을 효율적으로 활용한\n",
    "블록을 잔차 연결과 함께 사용하여 깊게 쌓은 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "model = keras.applications.xception.Xception(\n",
    "    include_top=True, weights='imagenet', input_tensor=None,\n",
    "    input_shape=None, pooling=None, classes=1000,\n",
    "    classifier_activation='softmax'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"xception\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_4 (InputLayer)            [(None, 299, 299, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1 (Conv2D)           (None, 149, 149, 32) 864         input_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_bn (BatchNormaliza (None, 149, 149, 32) 128         block1_conv1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv1_act (Activation)   (None, 149, 149, 32) 0           block1_conv1_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2 (Conv2D)           (None, 147, 147, 64) 18432       block1_conv1_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_bn (BatchNormaliza (None, 147, 147, 64) 256         block1_conv2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "block1_conv2_act (Activation)   (None, 147, 147, 64) 0           block1_conv2_bn[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1 (SeparableConv2 (None, 147, 147, 128 8768        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv1_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_act (Activation (None, 147, 147, 128 0           block2_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2 (SeparableConv2 (None, 147, 147, 128 17536       block2_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block2_sepconv2_bn (BatchNormal (None, 147, 147, 128 512         block2_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 74, 74, 128)  8192        block1_conv2_act[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block2_pool (MaxPooling2D)      (None, 74, 74, 128)  0           block2_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 74, 74, 128)  512         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 74, 74, 128)  0           block2_pool[0][0]                \n",
      "                                                                 batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_act (Activation (None, 74, 74, 128)  0           add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1 (SeparableConv2 (None, 74, 74, 256)  33920       block3_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv1_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_act (Activation (None, 74, 74, 256)  0           block3_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2 (SeparableConv2 (None, 74, 74, 256)  67840       block3_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block3_sepconv2_bn (BatchNormal (None, 74, 74, 256)  1024        block3_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 37, 37, 256)  32768       add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block3_pool (MaxPooling2D)      (None, 37, 37, 256)  0           block3_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 37, 37, 256)  1024        conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 37, 37, 256)  0           block3_pool[0][0]                \n",
      "                                                                 batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_act (Activation (None, 37, 37, 256)  0           add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1 (SeparableConv2 (None, 37, 37, 728)  188672      block4_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv1_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_act (Activation (None, 37, 37, 728)  0           block4_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2 (SeparableConv2 (None, 37, 37, 728)  536536      block4_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block4_sepconv2_bn (BatchNormal (None, 37, 37, 728)  2912        block4_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 19, 19, 728)  186368      add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block4_pool (MaxPooling2D)      (None, 19, 19, 728)  0           block4_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 19, 19, 728)  2912        conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 19, 19, 728)  0           block4_pool[0][0]                \n",
      "                                                                 batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_act (Activation (None, 19, 19, 728)  0           add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_act (Activation (None, 19, 19, 728)  0           block5_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_act (Activation (None, 19, 19, 728)  0           block5_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block5_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block5_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block5_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 19, 19, 728)  0           block5_sepconv3_bn[0][0]         \n",
      "                                                                 add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_act (Activation (None, 19, 19, 728)  0           add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_act (Activation (None, 19, 19, 728)  0           block6_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_act (Activation (None, 19, 19, 728)  0           block6_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block6_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block6_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block6_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 19, 19, 728)  0           block6_sepconv3_bn[0][0]         \n",
      "                                                                 add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_act (Activation (None, 19, 19, 728)  0           add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_act (Activation (None, 19, 19, 728)  0           block7_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_act (Activation (None, 19, 19, 728)  0           block7_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block7_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block7_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block7_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 19, 19, 728)  0           block7_sepconv3_bn[0][0]         \n",
      "                                                                 add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_act (Activation (None, 19, 19, 728)  0           add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_act (Activation (None, 19, 19, 728)  0           block8_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_act (Activation (None, 19, 19, 728)  0           block8_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block8_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block8_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block8_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 19, 19, 728)  0           block8_sepconv3_bn[0][0]         \n",
      "                                                                 add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_act (Activation (None, 19, 19, 728)  0           add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv1_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv1_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_act (Activation (None, 19, 19, 728)  0           block9_sepconv1_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv2_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv2_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_act (Activation (None, 19, 19, 728)  0           block9_sepconv2_bn[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3 (SeparableConv2 (None, 19, 19, 728)  536536      block9_sepconv3_act[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block9_sepconv3_bn (BatchNormal (None, 19, 19, 728)  2912        block9_sepconv3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "add_12 (Add)                    (None, 19, 19, 728)  0           block9_sepconv3_bn[0][0]         \n",
      "                                                                 add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_act (Activatio (None, 19, 19, 728)  0           block10_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_act (Activatio (None, 19, 19, 728)  0           block10_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block10_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block10_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block10_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_13 (Add)                    (None, 19, 19, 728)  0           block10_sepconv3_bn[0][0]        \n",
      "                                                                 add_12[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_act (Activatio (None, 19, 19, 728)  0           block11_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_act (Activatio (None, 19, 19, 728)  0           block11_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block11_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block11_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block11_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_14 (Add)                    (None, 19, 19, 728)  0           block11_sepconv3_bn[0][0]        \n",
      "                                                                 add_13[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_act (Activatio (None, 19, 19, 728)  0           block12_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv2_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_act (Activatio (None, 19, 19, 728)  0           block12_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3 (SeparableConv (None, 19, 19, 728)  536536      block12_sepconv3_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block12_sepconv3_bn (BatchNorma (None, 19, 19, 728)  2912        block12_sepconv3[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "add_15 (Add)                    (None, 19, 19, 728)  0           block12_sepconv3_bn[0][0]        \n",
      "                                                                 add_14[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_act (Activatio (None, 19, 19, 728)  0           add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1 (SeparableConv (None, 19, 19, 728)  536536      block13_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv1_bn (BatchNorma (None, 19, 19, 728)  2912        block13_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_act (Activatio (None, 19, 19, 728)  0           block13_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2 (SeparableConv (None, 19, 19, 1024) 752024      block13_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block13_sepconv2_bn (BatchNorma (None, 19, 19, 1024) 4096        block13_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 10, 10, 1024) 745472      add_15[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block13_pool (MaxPooling2D)     (None, 10, 10, 1024) 0           block13_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 10, 10, 1024) 4096        conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_16 (Add)                    (None, 10, 10, 1024) 0           block13_pool[0][0]               \n",
      "                                                                 batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1 (SeparableConv (None, 10, 10, 1536) 1582080     add_16[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_bn (BatchNorma (None, 10, 10, 1536) 6144        block14_sepconv1[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv1_act (Activatio (None, 10, 10, 1536) 0           block14_sepconv1_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2 (SeparableConv (None, 10, 10, 2048) 3159552     block14_sepconv1_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_bn (BatchNorma (None, 10, 10, 2048) 8192        block14_sepconv2[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "block14_sepconv2_act (Activatio (None, 10, 10, 2048) 0           block14_sepconv2_bn[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           block14_sepconv2_act[0][0]       \n",
      "__________________________________________________________________________________________________\n",
      "predictions (Dense)             (None, 1000)         2049000     avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 22,910,480\n",
      "Trainable params: 22,855,952\n",
      "Non-trainable params: 54,528\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 9.3.5 미니 Xception 모델"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "미니 Xception 모델을 직접 구현하여 강아지-고양이 이항분류 작업을 실행해본다.\n",
    "\n",
    "모델 구현에 사용되는 기법을 정리하면 다음과 같다.\n",
    "\n",
    "- 적절한 층으로 구성된 블록을 쌓아 모델을 구성한다.\n",
    "- 블록을 쌓을 수록 필터 수는 증가시키고, 텐서 크기는 감소시킨다.\n",
    "- 층의 유닛은 적게 유지하고 블록은 높게 쌓는다.\n",
    "- 잔차 연결을 활용한다.\n",
    "- 모든 합성곱 층 이후에는 배치 정규화를 적용한다.\n",
    "- 채널 분리 합성곱 신경망을 활용한다.\n",
    "\n",
    "**참고**: 여기서는 비록 이미지 분류 모델을 예제로 활용하지만\n",
    "앞서 언급된 모든 기법은 컴퓨터 비전 프로젝트 일반에 적용될 수 있다.\n",
    "예를 들어 [DeepLabV3 모델](https://arxiv.org/abs/1802.02611)은 \n",
    "Xception 모델을 이용하는 2021년 기준 최고의 이미지 분할 모델이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**이미지 다운로드 및 데이터 적재** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "사용하는 데이터셋은 [8장](https://codingalzi.github.io/dlp/notebooks/dlp08_intro_to_dl_for_computer_vision.html)에서 \n",
    "사용한 캐글(kaggle)의 강아지-고양이 데이터셋이며,\n",
    "이미지 다운로드와 훈련셋 등의 적재는 8장에서 사용한 방식과 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 캐글 인증서 업로드: 구글 코랩 사용하는 경우"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "이어지는 코드는 kaggle.json 파일이 현재 디렉토리에 있다고 가정함.\n",
      "아니면 직접 지정된 폴더에 kaggle.json 파일을 저장해야 함.\n"
     ]
    }
   ],
   "source": [
    "# 구글 코랩: 캐글 인증서 업로드\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    from google.colab import files\n",
    "    files.upload()\n",
    "\n",
    "print('이어지는 코드는 kaggle.json 파일이 현재 디렉토리에 있다고 가정함.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 캐글 인증서 저장: kaggle.json 파일이 현재 작업 디렉토리에 있다고 가정함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, pathlib\n",
    "\n",
    "# kaggle 인증서 현재 저장 위치\n",
    "where_kaggle_json = pathlib.Path(\"kaggle.json\")\n",
    "\n",
    "# kaggle 인증서를 사용자 홈디렉토리의 \".kaggle/\" 디렉토리로 옮기기\n",
    "if where_kaggle_json.is_file():\n",
    "    # 홈디렉토리 경로 지정\n",
    "    homeDir = pathlib.Path.home()\n",
    "    kaggleDir = homeDir / \".kaggle\"\n",
    "    kaggleJsonFile = kaggleDir / \"kaggle.json\"\n",
    "\n",
    "    # \".kaggle\" 디렉토리 존재 여부 확인. 없으면 생성.\n",
    "    if not kaggleDir.is_dir():\n",
    "        os.makedirs(kaggleDir)\n",
    "\n",
    "    # \"kaggle.json\" 파일 존재 여부 확인. 없으면 복사.\n",
    "    if not kaggleJsonFile.is_file():\n",
    "        shutil.copyfile(src=where_kaggle_json, \n",
    "                        dst=kaggleJsonFile)\n",
    "        os.chmod(kaggleJsonFile, 0o600)\n",
    "else:\n",
    "    print(\"kaggle.json 파일을 지정된 사용자 홈폴더의 '.kaggle' 폴더에 저장하세요!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 강아지-고양이 이미지셋 다운로드 및 압축 해제"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dogs-vs-cats.zip to C:\\Users\\gslee\\Documents\\GitHub\\dlp\\notebooks\\test\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/812M [00:00<?, ?B/s]\n",
      "  0%|          | 1.00M/812M [00:00<06:50, 2.07MB/s]\n",
      "  0%|          | 2.00M/812M [00:00<04:24, 3.21MB/s]\n",
      "  0%|          | 3.00M/812M [00:00<03:00, 4.70MB/s]\n",
      "  1%|          | 5.00M/812M [00:00<01:56, 7.25MB/s]\n",
      "  1%|          | 6.00M/812M [00:01<01:49, 7.72MB/s]\n",
      "  1%|          | 8.00M/812M [00:01<01:23, 10.1MB/s]\n",
      "  1%|1         | 10.0M/812M [00:01<01:09, 12.0MB/s]\n",
      "  1%|1         | 12.0M/812M [00:01<01:03, 13.2MB/s]\n",
      "  2%|1         | 14.0M/812M [00:01<00:59, 14.0MB/s]\n",
      "  2%|1         | 16.0M/812M [00:01<00:56, 14.7MB/s]\n",
      "  2%|2         | 18.0M/812M [00:01<00:55, 15.0MB/s]\n",
      "  2%|2         | 20.0M/812M [00:02<00:55, 15.0MB/s]\n",
      "  3%|2         | 22.0M/812M [00:02<00:54, 15.2MB/s]\n",
      "  3%|2         | 24.0M/812M [00:02<00:53, 15.4MB/s]\n",
      "  3%|3         | 26.0M/812M [00:02<00:55, 14.9MB/s]\n",
      "  3%|3         | 28.0M/812M [00:02<00:58, 14.0MB/s]\n",
      "  4%|3         | 30.0M/812M [00:03<01:54, 7.17MB/s]\n",
      "  4%|3         | 32.0M/812M [00:03<01:42, 8.00MB/s]\n",
      "  4%|4         | 34.0M/812M [00:03<01:43, 7.90MB/s]\n",
      "  4%|4         | 35.0M/812M [00:03<02:06, 6.43MB/s]\n",
      "  4%|4         | 36.0M/812M [00:04<02:28, 5.48MB/s]\n",
      "  5%|4         | 37.0M/812M [00:04<02:16, 5.97MB/s]\n",
      "  5%|4         | 39.0M/812M [00:04<01:47, 7.57MB/s]\n",
      "  5%|5         | 41.0M/812M [00:04<01:26, 9.36MB/s]\n",
      "  5%|5         | 43.0M/812M [00:04<01:14, 10.9MB/s]\n",
      "  6%|5         | 45.0M/812M [00:04<01:06, 12.1MB/s]\n",
      "  6%|5         | 47.0M/812M [00:05<01:01, 13.1MB/s]\n",
      "  6%|6         | 49.0M/812M [00:05<00:56, 14.1MB/s]\n",
      "  6%|6         | 51.0M/812M [00:05<00:54, 14.7MB/s]\n",
      "  7%|6         | 53.0M/812M [00:05<00:52, 15.0MB/s]\n",
      "  7%|6         | 55.0M/812M [00:05<01:09, 11.4MB/s]\n",
      "  7%|7         | 57.0M/812M [00:05<01:02, 12.7MB/s]\n",
      "  7%|7         | 60.0M/812M [00:06<00:50, 15.6MB/s]\n",
      "  8%|7         | 63.0M/812M [00:06<00:41, 19.0MB/s]\n",
      "  8%|8         | 66.0M/812M [00:06<00:45, 17.4MB/s]\n",
      "  8%|8         | 68.0M/812M [00:06<01:02, 12.4MB/s]\n",
      "  9%|8         | 70.0M/812M [00:07<01:27, 8.84MB/s]\n",
      "  9%|9         | 74.0M/812M [00:07<01:10, 10.9MB/s]\n",
      "  9%|9         | 77.0M/812M [00:07<01:07, 11.4MB/s]\n",
      " 10%|9         | 81.0M/812M [00:07<00:57, 13.4MB/s]\n",
      " 10%|#         | 83.0M/812M [00:08<01:51, 6.87MB/s]\n",
      " 10%|#         | 85.0M/812M [00:08<01:37, 7.85MB/s]\n",
      " 11%|#         | 87.0M/812M [00:08<01:26, 8.79MB/s]\n",
      " 11%|#         | 89.0M/812M [00:09<01:19, 9.59MB/s]\n",
      " 11%|#1        | 91.0M/812M [00:09<02:11, 5.77MB/s]\n",
      " 11%|#1        | 93.0M/812M [00:10<01:51, 6.79MB/s]\n",
      " 12%|#1        | 95.0M/812M [00:10<01:37, 7.68MB/s]\n",
      " 12%|#1        | 97.0M/812M [00:10<01:49, 6.83MB/s]\n",
      " 12%|#2        | 98.0M/812M [00:10<02:13, 5.61MB/s]\n",
      " 12%|#2        | 99.0M/812M [00:11<02:08, 5.81MB/s]\n",
      " 12%|#2        | 100M/812M [00:11<01:58, 6.31MB/s] \n",
      " 12%|#2        | 101M/812M [00:11<01:46, 6.97MB/s]\n",
      " 13%|#2        | 103M/812M [00:11<01:24, 8.79MB/s]\n",
      " 13%|#2        | 105M/812M [00:11<01:09, 10.7MB/s]\n",
      " 13%|#3        | 107M/812M [00:11<00:57, 12.9MB/s]\n",
      " 14%|#3        | 110M/812M [00:11<00:44, 16.6MB/s]\n",
      " 14%|#3        | 113M/812M [00:11<00:36, 20.1MB/s]\n",
      " 14%|#4        | 116M/812M [00:12<00:54, 13.3MB/s]\n",
      " 15%|#4        | 120M/812M [00:12<00:39, 18.3MB/s]\n",
      " 15%|#5        | 123M/812M [00:13<01:35, 7.55MB/s]\n",
      " 15%|#5        | 125M/812M [00:13<01:21, 8.80MB/s]\n",
      " 16%|#5        | 127M/812M [00:13<01:12, 9.86MB/s]\n",
      " 16%|#5        | 129M/812M [00:13<01:24, 8.44MB/s]\n",
      " 16%|#6        | 131M/812M [00:14<01:50, 6.44MB/s]\n",
      " 16%|#6        | 133M/812M [00:14<01:32, 7.73MB/s]\n",
      " 17%|#6        | 135M/812M [00:14<01:18, 9.05MB/s]\n",
      " 17%|#6        | 137M/812M [00:14<01:06, 10.6MB/s]\n",
      " 17%|#7        | 139M/812M [00:15<01:07, 10.4MB/s]\n",
      " 17%|#7        | 141M/812M [00:15<01:01, 11.5MB/s]\n",
      " 18%|#7        | 143M/812M [00:15<00:56, 12.5MB/s]\n",
      " 18%|#7        | 145M/812M [00:16<02:26, 4.76MB/s]\n",
      " 18%|#8        | 147M/812M [00:16<01:55, 6.02MB/s]\n",
      " 18%|#8        | 149M/812M [00:16<01:33, 7.41MB/s]\n",
      " 19%|#8        | 151M/812M [00:16<01:18, 8.83MB/s]\n",
      " 19%|#8        | 153M/812M [00:17<02:31, 4.56MB/s]\n",
      " 19%|#9        | 155M/812M [00:17<01:59, 5.76MB/s]\n",
      " 19%|#9        | 157M/812M [00:18<01:36, 7.12MB/s]\n",
      " 20%|#9        | 159M/812M [00:18<01:22, 8.33MB/s]\n",
      " 20%|#9        | 161M/812M [00:18<01:28, 7.68MB/s]\n",
      " 20%|##        | 163M/812M [00:18<01:37, 6.98MB/s]\n",
      " 20%|##        | 164M/812M [00:19<02:06, 5.37MB/s]\n",
      " 20%|##        | 165M/812M [00:19<01:58, 5.74MB/s]\n",
      " 20%|##        | 166M/812M [00:19<01:47, 6.29MB/s]\n",
      " 21%|##        | 167M/812M [00:19<02:21, 4.79MB/s]\n",
      " 21%|##        | 168M/812M [00:20<02:31, 4.44MB/s]\n",
      " 21%|##        | 169M/812M [00:20<02:13, 5.04MB/s]\n",
      " 21%|##        | 170M/812M [00:20<01:54, 5.86MB/s]\n",
      " 21%|##1       | 172M/812M [00:20<01:26, 7.77MB/s]\n",
      " 21%|##1       | 174M/812M [00:20<01:05, 10.2MB/s]\n",
      " 22%|##1       | 177M/812M [00:20<00:48, 13.7MB/s]\n",
      " 22%|##2       | 180M/812M [00:20<00:37, 17.6MB/s]\n",
      " 23%|##2       | 184M/812M [00:21<00:30, 21.8MB/s]\n",
      " 23%|##3       | 187M/812M [00:21<00:56, 11.5MB/s]\n",
      " 23%|##3       | 189M/812M [00:21<01:06, 9.84MB/s]\n",
      " 24%|##3       | 193M/812M [00:22<00:49, 13.0MB/s]\n",
      " 24%|##4       | 195M/812M [00:22<01:06, 9.80MB/s]\n",
      " 24%|##4       | 197M/812M [00:22<01:18, 8.22MB/s]\n",
      " 25%|##4       | 201M/812M [00:23<00:52, 12.2MB/s]\n",
      " 25%|##4       | 203M/812M [00:23<01:11, 8.91MB/s]\n",
      " 25%|##5       | 207M/812M [00:23<00:49, 12.9MB/s]\n",
      " 26%|##5       | 210M/812M [00:24<01:10, 8.99MB/s]\n",
      " 26%|##6       | 214M/812M [00:24<00:50, 12.5MB/s]\n",
      " 27%|##6       | 217M/812M [00:25<01:14, 8.35MB/s]\n",
      " 27%|##7       | 220M/812M [00:25<00:59, 10.4MB/s]\n",
      " 27%|##7       | 223M/812M [00:25<00:48, 12.7MB/s]\n",
      " 28%|##7       | 226M/812M [00:25<00:54, 11.3MB/s]\n",
      " 28%|##8       | 228M/812M [00:25<01:08, 9.00MB/s]\n",
      " 28%|##8       | 230M/812M [00:26<00:59, 10.2MB/s]\n",
      " 29%|##8       | 232M/812M [00:26<00:54, 11.2MB/s]\n",
      " 29%|##8       | 234M/812M [00:26<00:49, 12.2MB/s]\n",
      " 29%|##9       | 236M/812M [00:26<00:47, 12.8MB/s]\n",
      " 29%|##9       | 238M/812M [00:26<00:44, 13.5MB/s]\n",
      " 30%|##9       | 240M/812M [00:26<00:42, 14.2MB/s]\n",
      " 30%|##9       | 242M/812M [00:26<00:40, 14.9MB/s]\n",
      " 30%|###       | 244M/812M [00:27<00:39, 15.2MB/s]\n",
      " 30%|###       | 246M/812M [00:27<00:38, 15.4MB/s]\n",
      " 31%|###       | 248M/812M [00:27<00:37, 15.6MB/s]\n",
      " 31%|###       | 250M/812M [00:27<00:38, 15.3MB/s]\n",
      " 31%|###1      | 252M/812M [00:27<00:38, 15.2MB/s]\n",
      " 31%|###1      | 254M/812M [00:27<00:37, 15.7MB/s]\n",
      " 32%|###1      | 257M/812M [00:28<00:46, 12.6MB/s]\n",
      " 32%|###2      | 260M/812M [00:28<00:36, 15.7MB/s]\n",
      " 32%|###2      | 263M/812M [00:28<00:30, 18.8MB/s]\n",
      " 33%|###2      | 266M/812M [00:28<00:36, 15.9MB/s]\n",
      " 33%|###2      | 268M/812M [00:28<00:35, 16.1MB/s]\n",
      " 33%|###3      | 270M/812M [00:28<00:34, 16.6MB/s]\n",
      " 33%|###3      | 272M/812M [00:28<00:34, 16.4MB/s]\n",
      " 34%|###3      | 275M/812M [00:28<00:29, 19.2MB/s]\n",
      " 34%|###4      | 278M/812M [00:29<00:25, 22.1MB/s]\n",
      " 35%|###4      | 281M/812M [00:29<00:32, 17.1MB/s]\n",
      " 35%|###5      | 285M/812M [00:29<00:24, 22.1MB/s]\n",
      " 36%|###5      | 289M/812M [00:29<00:33, 16.2MB/s]\n",
      " 36%|###6      | 293M/812M [00:29<00:26, 20.4MB/s]\n",
      " 37%|###6      | 297M/812M [00:30<00:35, 15.3MB/s]\n",
      " 37%|###7      | 301M/812M [00:30<00:27, 19.1MB/s]\n",
      " 38%|###7      | 305M/812M [00:30<00:34, 15.5MB/s]\n",
      " 38%|###8      | 309M/812M [00:30<00:28, 18.8MB/s]\n",
      " 39%|###8      | 313M/812M [00:31<00:33, 15.7MB/s]\n",
      " 39%|###8      | 316M/812M [00:31<00:31, 16.4MB/s]\n",
      " 39%|###9      | 319M/812M [00:31<00:29, 17.3MB/s]\n",
      " 40%|###9      | 322M/812M [00:31<00:29, 17.4MB/s]\n",
      " 40%|####      | 326M/812M [00:31<00:23, 21.5MB/s]\n",
      " 41%|####      | 329M/812M [00:32<00:31, 16.0MB/s]\n",
      " 41%|####1     | 333M/812M [00:32<00:24, 20.2MB/s]\n",
      " 41%|####1     | 337M/812M [00:32<00:35, 14.1MB/s]\n",
      " 42%|####1     | 341M/812M [00:32<00:27, 17.8MB/s]\n",
      " 43%|####2     | 346M/812M [00:33<00:21, 22.8MB/s]\n",
      " 43%|####3     | 350M/812M [00:33<00:18, 26.3MB/s]\n",
      " 44%|####3     | 354M/812M [00:33<00:18, 25.9MB/s]\n",
      " 44%|####4     | 358M/812M [00:33<00:16, 28.2MB/s]\n",
      " 45%|####4     | 362M/812M [00:33<00:15, 31.3MB/s]\n",
      " 45%|####5     | 366M/812M [00:33<00:13, 33.8MB/s]\n",
      " 46%|####5     | 370M/812M [00:33<00:15, 29.5MB/s]\n",
      " 46%|####6     | 374M/812M [00:33<00:14, 31.2MB/s]\n",
      " 47%|####6     | 378M/812M [00:34<00:13, 32.5MB/s]\n",
      " 47%|####7     | 382M/812M [00:34<00:14, 31.2MB/s]\n",
      " 48%|####7     | 386M/812M [00:34<00:14, 29.9MB/s]\n",
      " 48%|####7     | 389M/812M [00:34<00:17, 26.1MB/s]\n",
      " 48%|####8     | 392M/812M [00:34<00:18, 23.9MB/s]\n",
      " 49%|####8     | 395M/812M [00:34<00:19, 22.9MB/s]\n",
      " 49%|####9     | 398M/812M [00:34<00:18, 23.9MB/s]\n",
      " 49%|####9     | 401M/812M [00:35<00:17, 24.8MB/s]\n",
      " 50%|####9     | 404M/812M [00:35<00:16, 26.4MB/s]\n",
      " 50%|#####     | 408M/812M [00:35<00:13, 30.3MB/s]\n",
      " 51%|#####     | 412M/812M [00:35<00:13, 32.1MB/s]\n",
      " 51%|#####1    | 416M/812M [00:35<00:11, 34.7MB/s]\n",
      " 52%|#####1    | 420M/812M [00:35<00:11, 36.7MB/s]\n",
      " 52%|#####2    | 424M/812M [00:35<00:10, 38.1MB/s]\n",
      " 53%|#####2    | 428M/812M [00:35<00:14, 27.2MB/s]\n",
      " 53%|#####3    | 432M/812M [00:36<00:13, 30.4MB/s]\n",
      " 54%|#####3    | 436M/812M [00:36<00:20, 19.6MB/s]\n",
      " 54%|#####4    | 440M/812M [00:36<00:16, 23.0MB/s]\n",
      " 55%|#####4    | 443M/812M [00:36<00:23, 16.5MB/s]\n",
      " 55%|#####5    | 447M/812M [00:37<00:19, 20.1MB/s]\n",
      " 56%|#####5    | 451M/812M [00:37<00:16, 23.2MB/s]\n",
      " 56%|#####6    | 455M/812M [00:37<00:14, 25.9MB/s]\n",
      " 57%|#####6    | 459M/812M [00:37<00:17, 21.0MB/s]\n",
      " 57%|#####6    | 462M/812M [00:37<00:16, 22.0MB/s]\n",
      " 57%|#####7    | 465M/812M [00:38<00:37, 9.58MB/s]\n",
      " 58%|#####7    | 469M/812M [00:38<00:28, 12.6MB/s]\n",
      " 58%|#####8    | 473M/812M [00:38<00:21, 16.2MB/s]\n",
      " 59%|#####8    | 478M/812M [00:38<00:16, 21.5MB/s]\n",
      " 59%|#####9    | 482M/812M [00:39<00:17, 20.3MB/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|#####9    | 486M/812M [00:39<00:14, 24.0MB/s]\n",
      " 60%|######    | 490M/812M [00:39<00:12, 27.5MB/s]\n",
      " 61%|######    | 494M/812M [00:39<00:11, 29.6MB/s]\n",
      " 61%|######1   | 498M/812M [00:39<00:12, 27.1MB/s]\n",
      " 62%|######1   | 502M/812M [00:39<00:10, 30.3MB/s]\n",
      " 62%|######2   | 506M/812M [00:39<00:10, 31.9MB/s]\n",
      " 63%|######2   | 510M/812M [00:39<00:09, 34.3MB/s]\n",
      " 63%|######3   | 514M/812M [00:40<00:18, 17.1MB/s]\n",
      " 64%|######3   | 517M/812M [00:40<00:18, 16.7MB/s]\n",
      " 64%|######4   | 520M/812M [00:40<00:17, 17.2MB/s]\n",
      " 64%|######4   | 523M/812M [00:40<00:15, 19.6MB/s]\n",
      " 65%|######4   | 527M/812M [00:41<00:12, 23.2MB/s]\n",
      " 65%|######5   | 531M/812M [00:41<00:11, 26.2MB/s]\n",
      " 66%|######5   | 535M/812M [00:41<00:09, 29.8MB/s]\n",
      " 66%|######6   | 539M/812M [00:41<00:13, 22.0MB/s]\n",
      " 67%|######6   | 543M/812M [00:41<00:10, 25.7MB/s]\n",
      " 67%|######7   | 547M/812M [00:42<00:14, 18.6MB/s]\n",
      " 68%|######7   | 551M/812M [00:42<00:12, 22.4MB/s]\n",
      " 68%|######8   | 554M/812M [00:42<00:18, 14.9MB/s]\n",
      " 69%|######8   | 558M/812M [00:42<00:14, 18.7MB/s]\n",
      " 69%|######9   | 562M/812M [00:42<00:11, 22.2MB/s]\n",
      " 70%|######9   | 566M/812M [00:42<00:10, 25.7MB/s]\n",
      " 70%|#######   | 570M/812M [00:43<00:10, 23.4MB/s]\n",
      " 71%|#######   | 574M/812M [00:43<00:09, 25.9MB/s]\n",
      " 71%|#######1  | 578M/812M [00:43<00:08, 29.3MB/s]\n",
      " 72%|#######1  | 582M/812M [00:43<00:08, 30.0MB/s]\n",
      " 72%|#######2  | 586M/812M [00:43<00:08, 29.5MB/s]\n",
      " 73%|#######2  | 590M/812M [00:43<00:08, 25.9MB/s]\n",
      " 73%|#######3  | 593M/812M [00:43<00:09, 23.3MB/s]\n",
      " 73%|#######3  | 596M/812M [00:44<00:10, 21.8MB/s]\n",
      " 74%|#######3  | 599M/812M [00:44<00:09, 22.4MB/s]\n",
      " 74%|#######4  | 603M/812M [00:44<00:08, 25.3MB/s]\n",
      " 75%|#######4  | 607M/812M [00:44<00:07, 28.1MB/s]\n",
      " 75%|#######5  | 610M/812M [00:45<00:15, 13.4MB/s]\n",
      " 76%|#######5  | 614M/812M [00:45<00:12, 16.9MB/s]\n",
      " 76%|#######5  | 617M/812M [00:45<00:15, 13.5MB/s]\n",
      " 76%|#######6  | 620M/812M [00:45<00:13, 14.6MB/s]\n",
      " 77%|#######6  | 622M/812M [00:45<00:13, 14.9MB/s]\n",
      " 77%|#######6  | 624M/812M [00:46<00:13, 14.9MB/s]\n",
      " 77%|#######7  | 626M/812M [00:46<00:13, 14.8MB/s]\n",
      " 77%|#######7  | 628M/812M [00:46<00:13, 14.1MB/s]\n",
      " 78%|#######7  | 630M/812M [00:46<00:15, 11.9MB/s]\n",
      " 78%|#######7  | 632M/812M [00:47<00:31, 5.99MB/s]\n",
      " 78%|#######7  | 633M/812M [00:47<00:31, 6.02MB/s]\n",
      " 78%|#######8  | 634M/812M [00:47<00:28, 6.55MB/s]\n",
      " 78%|#######8  | 636M/812M [00:47<00:23, 7.87MB/s]\n",
      " 79%|#######8  | 638M/812M [00:47<00:18, 9.95MB/s]\n",
      " 79%|#######8  | 641M/812M [00:48<00:13, 13.5MB/s]\n",
      " 79%|#######9  | 644M/812M [00:48<00:10, 17.2MB/s]\n",
      " 80%|#######9  | 648M/812M [00:48<00:07, 22.5MB/s]\n",
      " 80%|########  | 652M/812M [00:48<00:06, 26.3MB/s]\n",
      " 81%|########  | 657M/812M [00:48<00:07, 22.0MB/s]\n",
      " 81%|########1 | 661M/812M [00:48<00:06, 25.9MB/s]\n",
      " 82%|########1 | 665M/812M [00:49<00:07, 20.4MB/s]\n",
      " 82%|########2 | 669M/812M [00:49<00:06, 23.6MB/s]\n",
      " 83%|########2 | 673M/812M [00:49<00:09, 15.2MB/s]\n",
      " 83%|########3 | 677M/812M [00:49<00:07, 18.4MB/s]\n",
      " 84%|########3 | 681M/812M [00:49<00:07, 19.1MB/s]\n",
      " 84%|########4 | 684M/812M [00:50<00:06, 21.1MB/s]\n",
      " 85%|########4 | 687M/812M [00:50<00:05, 22.4MB/s]\n",
      " 85%|########4 | 690M/812M [00:50<00:07, 17.5MB/s]\n",
      " 85%|########5 | 693M/812M [00:50<00:06, 18.7MB/s]\n",
      " 86%|########5 | 696M/812M [00:50<00:05, 21.2MB/s]\n",
      " 86%|########6 | 699M/812M [00:50<00:05, 23.4MB/s]\n",
      " 87%|########6 | 703M/812M [00:50<00:04, 27.7MB/s]\n",
      " 87%|########6 | 706M/812M [00:51<00:04, 23.6MB/s]\n",
      " 87%|########7 | 710M/812M [00:51<00:03, 27.8MB/s]\n",
      " 88%|########7 | 714M/812M [00:51<00:04, 21.7MB/s]\n",
      " 88%|########8 | 718M/812M [00:51<00:03, 25.7MB/s]\n",
      " 89%|########8 | 721M/812M [00:51<00:04, 23.4MB/s]\n",
      " 89%|########9 | 726M/812M [00:51<00:03, 28.6MB/s]\n",
      " 90%|########9 | 730M/812M [00:52<00:04, 19.2MB/s]\n",
      " 90%|######### | 733M/812M [00:52<00:04, 19.0MB/s]\n",
      " 91%|######### | 736M/812M [00:52<00:03, 20.1MB/s]\n",
      " 91%|######### | 739M/812M [00:52<00:04, 17.8MB/s]\n",
      " 91%|#########1| 743M/812M [00:52<00:03, 22.1MB/s]\n",
      " 92%|#########1| 747M/812M [00:52<00:02, 26.1MB/s]\n",
      " 92%|#########2| 751M/812M [00:53<00:02, 29.7MB/s]\n",
      " 93%|#########2| 755M/812M [00:53<00:01, 32.6MB/s]\n",
      " 93%|#########3| 759M/812M [00:53<00:01, 35.0MB/s]\n",
      " 94%|#########3| 763M/812M [00:53<00:01, 32.2MB/s]\n",
      " 94%|#########4| 767M/812M [00:53<00:01, 34.6MB/s]\n",
      " 95%|#########4| 771M/812M [00:53<00:01, 28.3MB/s]\n",
      " 96%|#########5| 776M/812M [00:53<00:01, 32.8MB/s]\n",
      " 96%|#########6| 780M/812M [00:54<00:01, 29.3MB/s]\n",
      " 97%|#########6| 785M/812M [00:54<00:00, 33.5MB/s]\n",
      " 97%|#########7| 790M/812M [00:54<00:00, 37.0MB/s]\n",
      " 98%|#########7| 795M/812M [00:54<00:00, 38.8MB/s]\n",
      " 99%|#########8| 800M/812M [00:54<00:00, 41.0MB/s]\n",
      " 99%|#########9| 805M/812M [00:54<00:00, 42.8MB/s]\n",
      "100%|#########9| 810M/812M [00:54<00:00, 41.9MB/s]\n",
      "100%|##########| 812M/812M [00:54<00:00, 15.5MB/s]\n"
     ]
    }
   ],
   "source": [
    "try: \n",
    "    !kaggle competitions download -c dogs-vs-cats\n",
    "except: \n",
    "    !pip install kaggle\n",
    "    !kaggle competitions download -c dogs-vs-cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "except:\n",
    "    with zipfile.ZipFile('dogs-vs-cats.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "    with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 강아지-고양이 데이터셋 분류 저장 및 적재"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 files belonging to 2 classes.\n",
      "Found 1000 files belonging to 2 classes.\n",
      "Found 2000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# 이미지셋 분류 저장 경로 지정\n",
    "import os, shutil, pathlib\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "original_dir = pathlib.Path(\"train\")\n",
    "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
    "\n",
    "# 이미지셋 분류 저장\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "\n",
    "# 데이터셋 적재\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**모델 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 데이터 증식 층"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 미니 Xception 모델 구성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# 입력층\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "\n",
    "# 하나의 Conv2D 은닉층\n",
    "x = layers.Conv2D(filters=32, kernel_size=5, use_bias=False)(x)\n",
    "\n",
    "# SeparableConv2D, BatchNormalization, MaxPooling2D 층으로 구성된 블록 쌓기\n",
    "# 잔차 연결 활용\n",
    "for size in [32, 64, 128, 256, 512]:   # 필터 수\n",
    "    residual = x\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "    x = layers.SeparableConv2D(size, 3, padding=\"same\", use_bias=False)(x)\n",
    "\n",
    "    x = layers.MaxPooling2D(3, strides=2, padding=\"same\")(x)\n",
    "\n",
    "    # 잔차 연결\n",
    "    residual = layers.Conv2D(\n",
    "        size, 1, strides=2, padding=\"same\", use_bias=False)(residual)\n",
    "    x = layers.add([x, residual])\n",
    "\n",
    "# 마지막 은닉층은 GlobalAveragePooling2D과 Dropout\n",
    "x = layers.GlobalAveragePooling2D()(x)    # flatten 역할 수행(채널 별 평균값으로 구성)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 출력층\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# 모델 지정\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 180, 180, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "sequential (Sequential)         (None, 180, 180, 3)  0           input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "rescaling_1 (Rescaling)         (None, 180, 180, 3)  0           sequential[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 176, 176, 32) 2400        rescaling_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 176, 176, 32) 128         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 176, 176, 32) 0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d (SeparableConv (None, 176, 176, 32) 1312        activation[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 176, 176, 32) 128         separable_conv2d[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 176, 176, 32) 0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_1 (SeparableCo (None, 176, 176, 32) 1312        activation_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 88, 88, 32)   0           separable_conv2d_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 88, 88, 32)   1024        conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_17 (Add)                    (None, 88, 88, 32)   0           max_pooling2d_3[0][0]            \n",
      "                                                                 conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 88, 88, 32)   128         add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 88, 88, 32)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_2 (SeparableCo (None, 88, 88, 64)   2336        activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 88, 88, 64)   256         separable_conv2d_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_3 (Activation)       (None, 88, 88, 64)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_3 (SeparableCo (None, 88, 88, 64)   4672        activation_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 44, 44, 64)   0           separable_conv2d_3[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 44, 44, 64)   2048        add_17[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_18 (Add)                    (None, 44, 44, 64)   0           max_pooling2d_4[0][0]            \n",
      "                                                                 conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 44, 44, 64)   256         add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_4 (Activation)       (None, 44, 44, 64)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_4 (SeparableCo (None, 44, 44, 128)  8768        activation_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 44, 44, 128)  512         separable_conv2d_4[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_5 (Activation)       (None, 44, 44, 128)  0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_5 (SeparableCo (None, 44, 44, 128)  17536       activation_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2D)  (None, 22, 22, 128)  0           separable_conv2d_5[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 22, 22, 128)  8192        add_18[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_19 (Add)                    (None, 22, 22, 128)  0           max_pooling2d_5[0][0]            \n",
      "                                                                 conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 22, 22, 128)  512         add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_6 (Activation)       (None, 22, 22, 128)  0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_6 (SeparableCo (None, 22, 22, 256)  33920       activation_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 22, 22, 256)  1024        separable_conv2d_6[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_7 (Activation)       (None, 22, 22, 256)  0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_7 (SeparableCo (None, 22, 22, 256)  67840       activation_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2D)  (None, 11, 11, 256)  0           separable_conv2d_7[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 11, 11, 256)  32768       add_19[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_20 (Add)                    (None, 11, 11, 256)  0           max_pooling2d_6[0][0]            \n",
      "                                                                 conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 11, 11, 256)  1024        add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "activation_8 (Activation)       (None, 11, 11, 256)  0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_8 (SeparableCo (None, 11, 11, 512)  133376      activation_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 11, 11, 512)  2048        separable_conv2d_8[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "activation_9 (Activation)       (None, 11, 11, 512)  0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "separable_conv2d_9 (SeparableCo (None, 11, 11, 512)  266752      activation_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2D)  (None, 6, 6, 512)    0           separable_conv2d_9[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 6, 6, 512)    131072      add_20[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "add_21 (Add)                    (None, 6, 6, 512)    0           max_pooling2d_7[0][0]            \n",
      "                                                                 conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_1 (Glo (None, 512)          0           add_21[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 512)          0           global_average_pooling2d_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            513         dropout[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 721,857\n",
      "Trainable params: 718,849\n",
      "Non-trainable params: 3,008\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모델 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "63/63 [==============================] - 32s 310ms/step - loss: 0.7129 - accuracy: 0.5650 - val_loss: 0.6934 - val_accuracy: 0.5000\n",
      "Epoch 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gslee\\anaconda3\\lib\\site-packages\\keras\\utils\\generic_utils.py:494: CustomMaskWarning: Custom mask layers require a config and must override get_config. When loading, the custom mask layer must be passed to the custom_objects argument.\n",
      "  warnings.warn('Custom mask layers require a config and must override '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 19s 297ms/step - loss: 0.6554 - accuracy: 0.6060 - val_loss: 0.6946 - val_accuracy: 0.5000\n",
      "Epoch 3/100\n",
      "63/63 [==============================] - 19s 304ms/step - loss: 0.6460 - accuracy: 0.6355 - val_loss: 0.6951 - val_accuracy: 0.5000\n",
      "Epoch 4/100\n",
      "63/63 [==============================] - 19s 304ms/step - loss: 0.6223 - accuracy: 0.6575 - val_loss: 0.7169 - val_accuracy: 0.5000\n",
      "Epoch 5/100\n",
      "63/63 [==============================] - 19s 307ms/step - loss: 0.6103 - accuracy: 0.6760 - val_loss: 0.7466 - val_accuracy: 0.5000\n",
      "Epoch 6/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.5881 - accuracy: 0.6965 - val_loss: 0.7633 - val_accuracy: 0.5000\n",
      "Epoch 7/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.5711 - accuracy: 0.7045 - val_loss: 0.8972 - val_accuracy: 0.5000\n",
      "Epoch 8/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.5621 - accuracy: 0.7125 - val_loss: 0.7046 - val_accuracy: 0.5520\n",
      "Epoch 9/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.5559 - accuracy: 0.7155 - val_loss: 1.0341 - val_accuracy: 0.5340\n",
      "Epoch 10/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.5153 - accuracy: 0.7515 - val_loss: 0.5501 - val_accuracy: 0.7080\n",
      "Epoch 11/100\n",
      "63/63 [==============================] - 19s 304ms/step - loss: 0.5170 - accuracy: 0.7450 - val_loss: 0.6464 - val_accuracy: 0.6460\n",
      "Epoch 12/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.5260 - accuracy: 0.7520 - val_loss: 0.7382 - val_accuracy: 0.6200\n",
      "Epoch 13/100\n",
      "63/63 [==============================] - 19s 303ms/step - loss: 0.4875 - accuracy: 0.7715 - val_loss: 0.8880 - val_accuracy: 0.5800\n",
      "Epoch 14/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.4555 - accuracy: 0.7880 - val_loss: 0.9598 - val_accuracy: 0.6070\n",
      "Epoch 15/100\n",
      "63/63 [==============================] - 20s 310ms/step - loss: 0.4575 - accuracy: 0.7800 - val_loss: 0.5213 - val_accuracy: 0.7610\n",
      "Epoch 16/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.4592 - accuracy: 0.7900 - val_loss: 0.6380 - val_accuracy: 0.6850\n",
      "Epoch 17/100\n",
      "63/63 [==============================] - 19s 303ms/step - loss: 0.4333 - accuracy: 0.8015 - val_loss: 0.7936 - val_accuracy: 0.6900\n",
      "Epoch 18/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.4172 - accuracy: 0.8145 - val_loss: 0.5306 - val_accuracy: 0.7480\n",
      "Epoch 19/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.4157 - accuracy: 0.8100 - val_loss: 0.4632 - val_accuracy: 0.7750\n",
      "Epoch 20/100\n",
      "63/63 [==============================] - 19s 302ms/step - loss: 0.4120 - accuracy: 0.8035 - val_loss: 0.7216 - val_accuracy: 0.7040\n",
      "Epoch 21/100\n",
      "63/63 [==============================] - 19s 303ms/step - loss: 0.3788 - accuracy: 0.8330 - val_loss: 0.7511 - val_accuracy: 0.7210\n",
      "Epoch 22/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.3869 - accuracy: 0.8350 - val_loss: 0.6418 - val_accuracy: 0.7690\n",
      "Epoch 23/100\n",
      "63/63 [==============================] - 19s 306ms/step - loss: 0.3816 - accuracy: 0.8275 - val_loss: 0.6153 - val_accuracy: 0.7330\n",
      "Epoch 24/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.3535 - accuracy: 0.8505 - val_loss: 0.7222 - val_accuracy: 0.7490\n",
      "Epoch 25/100\n",
      "63/63 [==============================] - 19s 308ms/step - loss: 0.3350 - accuracy: 0.8500 - val_loss: 0.8011 - val_accuracy: 0.7400\n",
      "Epoch 26/100\n",
      "63/63 [==============================] - 19s 303ms/step - loss: 0.3388 - accuracy: 0.8575 - val_loss: 0.6957 - val_accuracy: 0.6610\n",
      "Epoch 27/100\n",
      "63/63 [==============================] - 20s 309ms/step - loss: 0.3401 - accuracy: 0.8440 - val_loss: 0.6324 - val_accuracy: 0.7670\n",
      "Epoch 28/100\n",
      "63/63 [==============================] - 20s 311ms/step - loss: 0.3171 - accuracy: 0.8735 - val_loss: 0.5255 - val_accuracy: 0.8070\n",
      "Epoch 29/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.3008 - accuracy: 0.8600 - val_loss: 0.5955 - val_accuracy: 0.7930\n",
      "Epoch 30/100\n",
      "63/63 [==============================] - 19s 304ms/step - loss: 0.3283 - accuracy: 0.8535 - val_loss: 0.4400 - val_accuracy: 0.8030\n",
      "Epoch 31/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.2950 - accuracy: 0.8755 - val_loss: 1.2714 - val_accuracy: 0.6500\n",
      "Epoch 32/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.2868 - accuracy: 0.8780 - val_loss: 1.4175 - val_accuracy: 0.6620\n",
      "Epoch 33/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.2983 - accuracy: 0.8740 - val_loss: 0.4471 - val_accuracy: 0.8210\n",
      "Epoch 34/100\n",
      "63/63 [==============================] - 19s 306ms/step - loss: 0.2807 - accuracy: 0.8770 - val_loss: 0.9578 - val_accuracy: 0.7620\n",
      "Epoch 35/100\n",
      "63/63 [==============================] - 19s 309ms/step - loss: 0.2694 - accuracy: 0.8850 - val_loss: 0.4311 - val_accuracy: 0.8500\n",
      "Epoch 36/100\n",
      "63/63 [==============================] - 19s 307ms/step - loss: 0.2759 - accuracy: 0.8830 - val_loss: 1.7197 - val_accuracy: 0.6460\n",
      "Epoch 37/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.2675 - accuracy: 0.8855 - val_loss: 1.2854 - val_accuracy: 0.6600\n",
      "Epoch 38/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.2604 - accuracy: 0.8960 - val_loss: 1.4749 - val_accuracy: 0.6000\n",
      "Epoch 39/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.2517 - accuracy: 0.8890 - val_loss: 0.5740 - val_accuracy: 0.7980\n",
      "Epoch 40/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.2540 - accuracy: 0.8985 - val_loss: 0.4637 - val_accuracy: 0.8220\n",
      "Epoch 41/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.2475 - accuracy: 0.8930 - val_loss: 0.5131 - val_accuracy: 0.8330\n",
      "Epoch 42/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.2567 - accuracy: 0.8870 - val_loss: 0.4671 - val_accuracy: 0.7900\n",
      "Epoch 43/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.2445 - accuracy: 0.9025 - val_loss: 0.4743 - val_accuracy: 0.8270\n",
      "Epoch 44/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.2091 - accuracy: 0.9165 - val_loss: 0.5560 - val_accuracy: 0.7940\n",
      "Epoch 45/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.2424 - accuracy: 0.8995 - val_loss: 0.4657 - val_accuracy: 0.8050\n",
      "Epoch 46/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.2272 - accuracy: 0.9070 - val_loss: 0.3562 - val_accuracy: 0.8420\n",
      "Epoch 47/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.2073 - accuracy: 0.9095 - val_loss: 0.5141 - val_accuracy: 0.8060\n",
      "Epoch 48/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.2014 - accuracy: 0.9155 - val_loss: 0.3395 - val_accuracy: 0.8540\n",
      "Epoch 49/100\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.2110 - accuracy: 0.9155 - val_loss: 0.4573 - val_accuracy: 0.8480\n",
      "Epoch 50/100\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.2056 - accuracy: 0.9175 - val_loss: 0.4650 - val_accuracy: 0.8490\n",
      "Epoch 51/100\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.2066 - accuracy: 0.9110 - val_loss: 0.6371 - val_accuracy: 0.7700\n",
      "Epoch 52/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1969 - accuracy: 0.9230 - val_loss: 0.5021 - val_accuracy: 0.8220\n",
      "Epoch 53/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.2047 - accuracy: 0.9150 - val_loss: 0.5595 - val_accuracy: 0.7890\n",
      "Epoch 54/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.2010 - accuracy: 0.9175 - val_loss: 0.3788 - val_accuracy: 0.8630\n",
      "Epoch 55/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1921 - accuracy: 0.9210 - val_loss: 0.3993 - val_accuracy: 0.8410\n",
      "Epoch 56/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1635 - accuracy: 0.9320 - val_loss: 0.4425 - val_accuracy: 0.8500\n",
      "Epoch 57/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1878 - accuracy: 0.9235 - val_loss: 0.7252 - val_accuracy: 0.8080\n",
      "Epoch 58/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1762 - accuracy: 0.9260 - val_loss: 0.4475 - val_accuracy: 0.8640\n",
      "Epoch 59/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.1796 - accuracy: 0.9195 - val_loss: 0.3719 - val_accuracy: 0.8630\n",
      "Epoch 60/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1767 - accuracy: 0.9245 - val_loss: 0.3220 - val_accuracy: 0.8700\n",
      "Epoch 61/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1755 - accuracy: 0.9295 - val_loss: 0.4743 - val_accuracy: 0.8470\n",
      "Epoch 62/100\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.1664 - accuracy: 0.9280 - val_loss: 0.4888 - val_accuracy: 0.8340\n",
      "Epoch 63/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1621 - accuracy: 0.9370 - val_loss: 0.5812 - val_accuracy: 0.7950\n",
      "Epoch 64/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1628 - accuracy: 0.9360 - val_loss: 1.2086 - val_accuracy: 0.7210\n",
      "Epoch 65/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1589 - accuracy: 0.9350 - val_loss: 0.5746 - val_accuracy: 0.8360\n",
      "Epoch 66/100\n",
      "63/63 [==============================] - 19s 293ms/step - loss: 0.1534 - accuracy: 0.9395 - val_loss: 0.4577 - val_accuracy: 0.8480\n",
      "Epoch 67/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1404 - accuracy: 0.9495 - val_loss: 1.6608 - val_accuracy: 0.6820\n",
      "Epoch 68/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1406 - accuracy: 0.9435 - val_loss: 0.4108 - val_accuracy: 0.8510\n",
      "Epoch 69/100\n",
      "63/63 [==============================] - 19s 303ms/step - loss: 0.1431 - accuracy: 0.9430 - val_loss: 0.5181 - val_accuracy: 0.8550\n",
      "Epoch 70/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.1616 - accuracy: 0.9340 - val_loss: 0.3000 - val_accuracy: 0.8740\n",
      "Epoch 71/100\n",
      "63/63 [==============================] - 19s 294ms/step - loss: 0.1462 - accuracy: 0.9375 - val_loss: 0.5359 - val_accuracy: 0.8040\n",
      "Epoch 72/100\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.1479 - accuracy: 0.9455 - val_loss: 0.4031 - val_accuracy: 0.8640\n",
      "Epoch 73/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1389 - accuracy: 0.9485 - val_loss: 1.3163 - val_accuracy: 0.7320\n",
      "Epoch 74/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1297 - accuracy: 0.9505 - val_loss: 0.4890 - val_accuracy: 0.8590\n",
      "Epoch 75/100\n",
      "63/63 [==============================] - 19s 302ms/step - loss: 0.1232 - accuracy: 0.9485 - val_loss: 1.8173 - val_accuracy: 0.6710\n",
      "Epoch 76/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1328 - accuracy: 0.9445 - val_loss: 0.3301 - val_accuracy: 0.8860\n",
      "Epoch 77/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.1249 - accuracy: 0.9490 - val_loss: 0.3632 - val_accuracy: 0.8900\n",
      "Epoch 78/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.1354 - accuracy: 0.9425 - val_loss: 0.3637 - val_accuracy: 0.8500\n",
      "Epoch 79/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1280 - accuracy: 0.9460 - val_loss: 0.8541 - val_accuracy: 0.7390\n",
      "Epoch 80/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1475 - accuracy: 0.9440 - val_loss: 0.3577 - val_accuracy: 0.8730\n",
      "Epoch 81/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1424 - accuracy: 0.9405 - val_loss: 0.3227 - val_accuracy: 0.8740\n",
      "Epoch 82/100\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.1201 - accuracy: 0.9520 - val_loss: 0.4930 - val_accuracy: 0.8210\n",
      "Epoch 83/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1286 - accuracy: 0.9450 - val_loss: 0.4637 - val_accuracy: 0.8210\n",
      "Epoch 84/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.1152 - accuracy: 0.9520 - val_loss: 0.4887 - val_accuracy: 0.8560\n",
      "Epoch 85/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1039 - accuracy: 0.9570 - val_loss: 0.3485 - val_accuracy: 0.8960\n",
      "Epoch 86/100\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.1274 - accuracy: 0.9505 - val_loss: 0.5519 - val_accuracy: 0.8370\n",
      "Epoch 87/100\n",
      "63/63 [==============================] - 19s 302ms/step - loss: 0.1034 - accuracy: 0.9610 - val_loss: 0.6313 - val_accuracy: 0.8520\n",
      "Epoch 88/100\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.1348 - accuracy: 0.9510 - val_loss: 0.3381 - val_accuracy: 0.8860\n",
      "Epoch 89/100\n",
      "63/63 [==============================] - 19s 299ms/step - loss: 0.1049 - accuracy: 0.9625 - val_loss: 0.5969 - val_accuracy: 0.8400\n",
      "Epoch 90/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1200 - accuracy: 0.9560 - val_loss: 1.1107 - val_accuracy: 0.7810\n",
      "Epoch 91/100\n",
      "63/63 [==============================] - 19s 300ms/step - loss: 0.1064 - accuracy: 0.9580 - val_loss: 0.4043 - val_accuracy: 0.8650\n",
      "Epoch 92/100\n",
      "63/63 [==============================] - 19s 295ms/step - loss: 0.1271 - accuracy: 0.9475 - val_loss: 0.4210 - val_accuracy: 0.8800\n",
      "Epoch 93/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1072 - accuracy: 0.9640 - val_loss: 3.1291 - val_accuracy: 0.6350\n",
      "Epoch 94/100\n",
      "63/63 [==============================] - 19s 298ms/step - loss: 0.1159 - accuracy: 0.9560 - val_loss: 1.0338 - val_accuracy: 0.8030\n",
      "Epoch 95/100\n",
      "63/63 [==============================] - 19s 306ms/step - loss: 0.0948 - accuracy: 0.9630 - val_loss: 0.3869 - val_accuracy: 0.8850\n",
      "Epoch 96/100\n",
      "63/63 [==============================] - 19s 307ms/step - loss: 0.1107 - accuracy: 0.9595 - val_loss: 0.7990 - val_accuracy: 0.8250\n",
      "Epoch 97/100\n",
      "63/63 [==============================] - 19s 297ms/step - loss: 0.1261 - accuracy: 0.9535 - val_loss: 0.4372 - val_accuracy: 0.8690\n",
      "Epoch 98/100\n",
      "63/63 [==============================] - 19s 296ms/step - loss: 0.0960 - accuracy: 0.9625 - val_loss: 0.9833 - val_accuracy: 0.7720\n",
      "Epoch 99/100\n",
      "63/63 [==============================] - 19s 302ms/step - loss: 0.1064 - accuracy: 0.9560 - val_loss: 0.5434 - val_accuracy: 0.8610\n",
      "Epoch 100/100\n",
      "63/63 [==============================] - 19s 301ms/step - loss: 0.0891 - accuracy: 0.9625 - val_loss: 0.3944 - val_accuracy: 0.8860\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"mini_xception.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "    ]\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과대적합은 50번 정도의 에포크 실행 후에 발생한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* 훈련 성능 평가\n",
    "\n",
    "직접 구현한 모델이지만 테스트셋에 대한 정확도가 90% 정도 나온다. \n",
    "8장에서 직접 구현해서 훈련시켰을 때의 성능인 83% 정도보다 훨씬 높게 나온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63/63 [==============================] - 2s 19ms/step - loss: 0.3141 - accuracy: 0.8785\n",
      "Test accuracy: 0.878\n"
     ]
    }
   ],
   "source": [
    "test_model = keras.models.load_model(\"mini_xception.keras\")\n",
    "\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**성능 높이기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "보다 성능을 높이려면 하이퍼파라미터 미세조정 및 앙상블 학습을 활용해야 한다(13장 참조).\n",
    "\n",
    "- 하이퍼파라미터 미세조정: [케라스튜너(KerasTuner)](https://www.tensorflow.org/tutorials/keras/keras_tuner) 활용\n",
    "- 앙상블 학습: [그레이디언트 부스팅(gradient boosting)](https://codingalzi.github.io/handson-ml2/notebooks/handson-ml2-07.html) 활용"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dlp09_part02_modern_convnet_architecture_patterns",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
