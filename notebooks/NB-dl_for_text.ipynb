{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5ZZS6o0Z8Oj"
   },
   "source": [
    "# 11장 자연어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGXhJe5XZ8Ou"
   },
   "source": [
    "##  IMDB 영화 후기 데이터셋 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfwyzgmXZ8Ou"
   },
   "source": [
    "IMDB 데이터셋을 직접 다운로드하여 벡터화하는 과정을 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaO7ilwSZ8Ou"
   },
   "source": [
    "준비 과정 1: 데이터셋 다운로드 압축 풀기\n",
    "\n",
    "압축을 풀면 아래 구조의 디렉토리가 생성된다.\n",
    "\n",
    "```\n",
    "aclImdb/\n",
    "...train/\n",
    "......pos/\n",
    "......neg/\n",
    "...test/\n",
    "......pos/\n",
    "......neg/\n",
    "```\n",
    "\n",
    "`train`의 `pos`와 `neg` 서브디렉토리에 각각 12,500개의 긍정과 부정 후기가\n",
    "포함되어 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eScCXFlWZ8Ou",
    "outputId": "e64fb37f-d5a6-49ac-f8b9-0567bb3ce0b2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  3997k      0  0:00:20  0:00:20 --:--:-- 5328k\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMRKwRM9Z8Ou"
   },
   "source": [
    "`aclImdb/train/unsup` 서브디렉토리는 필요 없기에 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "2xsKw4vPZ8Ou"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !rm -r aclImdb/train/unsup\n",
    "else:\n",
    "    import shutil\n",
    "    unsup_path = './aclImdb/train/unsup'\n",
    "    shutil.rmtree(unsup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rZ3a9ETZ8Ou"
   },
   "source": [
    "긍정 후기 하나의 내용을 살펴보자.\n",
    "모델 구성 이전에 훈련 데이터셋을 살펴 보고\n",
    "모델에 대한 직관을 갖는 과정이 항상 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VK5FRFX0Z8Ou",
    "outputId": "1304a433-6e6f-4ad0-f0e7-80b49147acb4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !cat aclImdb/train/pos/4077_10.txt\n",
    "else:\n",
    "    with open('aclImdb/train/pos/4077_10.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfeoV0YCZ8Ov"
   },
   "source": [
    "준비 과정 2: 검증셋 준비\n",
    "\n",
    "훈련셋의 20%를 검증셋으로 떼어낸다.\n",
    "이를 위해 `aclImdb/val` 디렉토리를 생성한 후에\n",
    "긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "xX30tWVBZ8Ov"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)            # val 디렉토리 생성\n",
    "    files = os.listdir(train_dir / category)\n",
    "\n",
    "    random.Random(1337).shuffle(files)         # 훈련셋 무작위 섞기\n",
    "\n",
    "    num_val_samples = int(0.2 * len(files))    # 20% 지정 후 검증셋으로 옮기기\n",
    "    val_files = files[-num_val_samples:]\n",
    "\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fljBC3puZ8Ov"
   },
   "source": [
    "준비 과정 3: 텐서 데이터셋 준비\n",
    "\n",
    "`text_dataset_from_directory()` 함수를 이용하여\n",
    "훈련셋, 검증셋, 테스트셋을 준비한다.\n",
    "자료형은 모두 `Dataset`이며, 배치 크기는 32를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e41g6BtPZ8Ov",
    "outputId": "8fa59267-238d-44c7-87f4-33df0e3e15b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    "    )\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    "    )\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLi_rBmlZ8Ov"
   },
   "source": [
    "각 데이터셋은 배치로 구분되며\n",
    "입력은 `tf.string` 텐서이고, 타깃은 `int32` 텐서이다.\n",
    "크기는 모두 32이며 지정된 배치 크기이다.\n",
    "예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G05FSfO1Z8Ov",
    "outputId": "8ca36337-3ce3-4a51-d35b-ce16eafbc25b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'Marlon Brando and Frank Sinatra HATED each other while doing this film and in the years following it.Brando asked Sinatra for some singing lessons,but Sinatra (who was already upset over the fact that he was cast as Nathan Detroit,when he wanted the part of Skye Masterson)wanted to be the star singer in the production and didn\\'t want any one to be better than he was.He told Brando to get lost,so to get back at him Marlon kept screwing up the scene in the diner where Sinatra was eating cheesecake so that he would have to keep eating,eating and eating.In the later years,Sinatra gave Brando the nickname \"Mumbles\" or \"Mr.Mumbles\" because of the way he talks.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "\n",
    "    # 예제: 첫째 배치의 첫째 후기\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aM3uJTQVlJSy"
   },
   "source": [
    "### 11.3.3 시퀀스 활용법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXZ34Qg7lJSy"
   },
   "source": [
    "**정수 벡터 데이터셋 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zoVifu3DlJSy"
   },
   "source": [
    "훈련셋의 모든 후기 문장을 정수들의 벡터로 변환한다.\n",
    "단, 후기 문장이 최대 600개의 단어만 포함하도록 한다.\n",
    "또한 사용되는 어휘는 빈도 기준 최대 2만 개로 제한한다.\n",
    "\n",
    "- `max_length = 600`\n",
    "- `max_tokens = 20000`\n",
    "- `output_sequence_length=max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "orh58qTYlJSy"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "# 어휘 색인 생성 대상 훈련셋 후기 텍스트 데이터셋\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T94MVnSHlJSy"
   },
   "source": [
    "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
    "`output_sequence_length=600`으로 지정하였기에 모든 문장은 단어를 최대 600개에서\n",
    "잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.\n",
    "물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다.\n",
    "또한 벡터에 사용된 정수는 2만보다 작은 값이며,\n",
    "이는 빈도가 가장 높은 2만개의 단어만을 대상(`max_tokens=20000`)으로 했기 때문이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7B-SbA7rlJSy",
    "outputId": "a951c537-5db8-4bfe-eda4-346b2e4145b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 600)\n",
      "inputs.dtype: <dtype: 'int64'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(\n",
      "[   11    19    14     2   240    19   122    91    21     2  1173    10\n",
      "    39  7749    51    71    11    19     2  2770  2765   357     2    19\n",
      "     7  1159     2  2445  2765     4   125    32    10    14  1555     6\n",
      "  4563     2    86    12    91    11     1    81    22   422   617     5\n",
      "   123   121   146    11    19     2    62    50   171    14    52     2\n",
      "    19  1027    11    19     7  2010 10817  8957  1491   469 10817  9616\n",
      "     4     1 10817  1049   577    81    22   422   123    61   146    11\n",
      "    19    23    78  2466     1   176     6   118   131    11    19  1911\n",
      "    74     2  1112  2190     2  2770  2190     2   548   576    10   377\n",
      "     7 14685     2    86   774     6  2558   525    58     2  1128  1354\n",
      "   103    48    32    12     4    69   846    58     6     2  1640    10\n",
      "   377    48    14     3   648 14784  4561   626   798    31     3   248\n",
      "    57  2714     3 13834    31     9    36  1291 14784   242   832  2338\n",
      "    23    69   156   610     9     2   113    14   514    99     1  1491\n",
      "     7   156     3    50    19     4    11    41    66     6   139     4\n",
      "  2406     9     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in int_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCrjK1-zUwFj"
   },
   "source": [
    "**트랜스포머 구현**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iH2T-AQOUwFj"
   },
   "source": [
    "위 그림에서 설명된 트랜스포머 인코더를 층(layer)으로 구현하면 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "X6g33CzrUwFk"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "class TransformerEncoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, inputs, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_output = self.attention(\n",
    "            inputs, inputs, attention_mask=mask)\n",
    "        proj_input = self.layernorm_1(inputs + attention_output)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input + proj_output)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtXomfGDUwFk"
   },
   "source": [
    "**트랜스포머 인코더 활용 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPSowlcGUwFk"
   },
   "source": [
    "훈련 데이터셋이 입력되면 먼저 단어 임베딩을 이용하여\n",
    "단어들 사이의 연관성을 찾는다.\n",
    "이후 트랜스포머 인코더로 셀프 어텐션을 적용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hxbIVhi3UwFk",
    "outputId": "77b51533-c428-4b80-b704-24227fc076cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding (Embedding)       (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " transformer_encoder (Trans  (None, None, 256)         543776    \n",
      " formerEncoder)                                                  \n",
      "                                                                 \n",
      " global_max_pooling1d (Glob  (None, 256)               0         \n",
      " alMaxPooling1D)                                                 \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 256)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5664033 (21.61 MB)\n",
      "Trainable params: 5664033 (21.61 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "x = layers.Embedding(vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "# 길이가 600인 1차원 어레이로 변환\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rJ82rMIUwFl"
   },
   "source": [
    "훈련 과정은 특별한 게 없다.\n",
    "테스트셋에 대한 정확도가 87.5% 정도로 바이그램 모델보다 좀 더 낮다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R2BDEYHaUwFl",
    "outputId": "a81a5913-fdf8-44f9-f2ce-d4f983479c51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.3207 - accuracy: 0.8619 - val_loss: 0.3410 - val_accuracy: 0.8494\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 48s 77ms/step - loss: 0.2878 - accuracy: 0.8774 - val_loss: 0.3082 - val_accuracy: 0.8710\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 48s 78ms/step - loss: 0.2578 - accuracy: 0.8935 - val_loss: 0.3053 - val_accuracy: 0.8676\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 44s 70ms/step - loss: 0.2257 - accuracy: 0.9112 - val_loss: 0.3263 - val_accuracy: 0.8644\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 44s 70ms/step - loss: 0.1978 - accuracy: 0.9232 - val_loss: 0.3199 - val_accuracy: 0.8714\n",
      "782/782 [==============================] - 21s 26ms/step - loss: 0.3357 - accuracy: 0.8511\n",
      "Test acc: 0.851\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"transformer_encoder\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=5, callbacks=callbacks)\n",
    "# model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\n",
    "    \"transformer_encoder\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder})\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8tEOxNV_UwFl"
   },
   "source": [
    "**단어 위치 인코딩**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uHQszXy-UwFl"
   },
   "source": [
    "다음 `PositionalEmbedding` 층 클래스는 두 개의 임베딩 클래스를 사용한다.\n",
    "하나는 보통의 단어 임베딩이며,\n",
    "다른 하나는 단어의 위치 정보를 임베딩한다.\n",
    "각 임베딩의 출력값을 합친 값을 트랜스포머에게 전달하는 역할을 수행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRiJ-vBXUwFm"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cG2Db4SRUwFm"
   },
   "source": [
    "**단어위치인식 트랜스포머 아키텍처**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fq24k0wlUwFm"
   },
   "source": [
    "아래 코드는 `PositionalEmbedding` 층을 활용하여 트랜스포머 인코더가\n",
    "단어위치를 활용할 수 있도록 한다.\n",
    "최종 모델의 테스트셋에 대한 정확도가 88.3%까지 향상됨을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jYcxHlk7UwFm",
    "outputId": "ba70a092-839a-4307-ec8a-a28610059829"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " positional_embedding (Posi  (None, None, 256)         5273600   \n",
      " tionalEmbedding)                                                \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tra  (None, None, 256)         543776    \n",
      " nsformerEncoder)                                                \n",
      "                                                                 \n",
      " global_max_pooling1d_1 (Gl  (None, 256)               0         \n",
      " obalMaxPooling1D)                                               \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5817633 (22.19 MB)\n",
      "Trainable params: 5817633 (22.19 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/5\n",
      "625/625 [==============================] - 63s 96ms/step - loss: 0.5286 - accuracy: 0.7521 - val_loss: 0.3209 - val_accuracy: 0.8666\n",
      "Epoch 2/5\n",
      "625/625 [==============================] - 53s 84ms/step - loss: 0.3014 - accuracy: 0.8733 - val_loss: 0.2812 - val_accuracy: 0.8826\n",
      "Epoch 3/5\n",
      "625/625 [==============================] - 49s 78ms/step - loss: 0.2344 - accuracy: 0.9080 - val_loss: 0.5238 - val_accuracy: 0.8086\n",
      "Epoch 4/5\n",
      "625/625 [==============================] - 48s 76ms/step - loss: 0.1981 - accuracy: 0.9237 - val_loss: 0.2857 - val_accuracy: 0.8866\n",
      "Epoch 5/5\n",
      "625/625 [==============================] - 46s 74ms/step - loss: 0.1655 - accuracy: 0.9373 - val_loss: 0.3083 - val_accuracy: 0.8846\n",
      "782/782 [==============================] - 24s 30ms/step - loss: 1.0643 - accuracy: 0.5014\n",
      "Test acc: 0.501\n"
     ]
    }
   ],
   "source": [
    "vocab_size = 20000\n",
    "sequence_length = 600\n",
    "embed_dim = 256\n",
    "num_heads = 2\n",
    "dense_dim = 32\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(inputs)\n",
    "x = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "x = layers.GlobalMaxPooling1D()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"full_transformer_encoder\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "# model.fit(int_train_ds, validation_data=int_val_ds, epochs=20, callbacks=callbacks)\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=5, callbacks=callbacks)\n",
    "model = keras.models.load_model(\n",
    "    \"full_transformer_encoder\",\n",
    "    custom_objects={\"TransformerEncoder\": TransformerEncoder,\n",
    "                    \"PositionalEmbedding\": PositionalEmbedding})\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XsHTQVECT_5r"
   },
   "source": [
    "## 시퀀스-투-시퀀스 학습: 기계 번역"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8kRw8L0jT_5r"
   },
   "source": [
    "### 영어-스페인어 기계 번역 데이터셋 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A4zqvFdKT_5r",
    "outputId": "1e97f4d7-d743-45d0-d29f-d94872e59484"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-12-12 16:24:17--  http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
      "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.206.219, 142.250.207.123, 142.250.206.251, ...\n",
      "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.206.219|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 2638744 (2.5M) [application/zip]\n",
      "Saving to: ‘spa-eng.zip’\n",
      "\n",
      "spa-eng.zip         100%[===================>]   2.52M  10.0MB/s    in 0.3s    \n",
      "\n",
      "2023-12-12 16:24:18 (10.0 MB/s) - ‘spa-eng.zip’ saved [2638744/2638744]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
    "!unzip -q spa-eng.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "adbnHDocT_5r"
   },
   "outputs": [],
   "source": [
    "text_file = \"spa-eng/spa.txt\"\n",
    "with open(text_file) as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "text_pairs = []\n",
    "for line in lines:\n",
    "    english, spanish = line.split(\"\\t\")\n",
    "    spanish = \"[start] \" + spanish + \" [end]\"\n",
    "    text_pairs.append((english, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BN_IurD9T_5s",
    "outputId": "9f1afe71-546f-4b11-ec09-f2eb089174ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('I gave her just what she needed.', '[start] Le di justo lo que necesitaba. [end]')\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "print(random.choice(text_pairs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kK2mwr96T_5s"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(text_pairs)\n",
    "num_val_samples = int(0.15 * len(text_pairs))\n",
    "num_train_samples = len(text_pairs) - 2 * num_val_samples\n",
    "train_pairs = text_pairs[:num_train_samples]\n",
    "val_pairs = text_pairs[num_train_samples:num_train_samples + num_val_samples]\n",
    "test_pairs = text_pairs[num_train_samples + num_val_samples:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AuqUCPshT_5s"
   },
   "source": [
    "**영어/스페인어 텍스트 벡터화**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8CWdFJIT_5t"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import string\n",
    "import re\n",
    "\n",
    "strip_chars = string.punctuation + \"¿\"\n",
    "strip_chars = strip_chars.replace(\"[\", \"\")\n",
    "strip_chars = strip_chars.replace(\"]\", \"\")\n",
    "\n",
    "def custom_standardization(input_string):\n",
    "    lowercase = tf.strings.lower(input_string)\n",
    "    return tf.strings.regex_replace(\n",
    "        lowercase, f\"[{re.escape(strip_chars)}]\", \"\")\n",
    "\n",
    "vocab_size = 15000\n",
    "sequence_length = 20\n",
    "\n",
    "source_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length,\n",
    ")\n",
    "target_vectorization = layers.TextVectorization(\n",
    "    max_tokens=vocab_size,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=sequence_length + 1,\n",
    "    standardize=custom_standardization,\n",
    ")\n",
    "train_english_texts = [pair[0] for pair in train_pairs]\n",
    "train_spanish_texts = [pair[1] for pair in train_pairs]\n",
    "source_vectorization.adapt(train_english_texts)\n",
    "target_vectorization.adapt(train_spanish_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B7HeM8qPT_5t"
   },
   "source": [
    "**훈련셋과 검증셋 준비**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XEgEfPpqT_5t"
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def format_dataset(eng, spa):\n",
    "    eng = source_vectorization(eng)\n",
    "    spa = target_vectorization(spa)\n",
    "    return ({\n",
    "        \"english\": eng,\n",
    "        \"spanish\": spa[:, :-1],\n",
    "    }, spa[:, 1:])\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    eng_texts, spa_texts = zip(*pairs)\n",
    "    eng_texts = list(eng_texts)\n",
    "    spa_texts = list(spa_texts)\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((eng_texts, spa_texts))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(format_dataset)\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "train_ds = make_dataset(train_pairs)\n",
    "val_ds = make_dataset(val_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tvL9A1MQT_5t",
    "outputId": "1db6f2f6-6ba9-406d-80f6-adc0ae681426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs['english'].shape: (64, 20)\n",
      "inputs['spanish'].shape: (64, 20)\n",
      "targets.shape: (64, 20)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds.take(1):\n",
    "    print(f\"inputs['english'].shape: {inputs['english'].shape}\")\n",
    "    print(f\"inputs['spanish'].shape: {inputs['spanish'].shape}\")\n",
    "    print(f\"targets.shape: {targets.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0JE4Oo1FT_5x"
   },
   "source": [
    "### 기계 번역용도의 트랜스포머"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HY_E2huQT_5x"
   },
   "source": [
    "**트랜스포머 디코더**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5I9kRgqdT_5x"
   },
   "outputs": [],
   "source": [
    "class TransformerDecoder(layers.Layer):\n",
    "    def __init__(self, embed_dim, dense_dim, num_heads, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation=\"relu\"),\n",
    "             layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"embed_dim\": self.embed_dim,\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_causal_attention_mask(self, inputs):\n",
    "        input_shape = tf.shape(inputs)\n",
    "        batch_size, sequence_length = input_shape[0], input_shape[1]\n",
    "        i = tf.range(sequence_length)[:, tf.newaxis]\n",
    "        j = tf.range(sequence_length)\n",
    "        mask = tf.cast(i >= j, dtype=\"int32\")\n",
    "        mask = tf.reshape(mask, (1, input_shape[1], input_shape[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype=tf.int32)], axis=0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = self.get_causal_attention_mask(inputs)\n",
    "        if mask is not None:\n",
    "            padding_mask = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            padding_mask = tf.minimum(padding_mask, causal_mask)\n",
    "        attention_output_1 = self.attention_1(\n",
    "            query=inputs,\n",
    "            value=inputs,\n",
    "            key=inputs,\n",
    "            attention_mask=causal_mask)\n",
    "        attention_output_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "        attention_output_2 = self.attention_2(\n",
    "            query=attention_output_1,\n",
    "            value=encoder_outputs,\n",
    "            key=encoder_outputs,\n",
    "            attention_mask=padding_mask,\n",
    "        )\n",
    "        attention_output_2 = self.layernorm_2(\n",
    "            attention_output_1 + attention_output_2)\n",
    "        proj_output = self.dense_proj(attention_output_2)\n",
    "        return self.layernorm_3(attention_output_2 + proj_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YFa7AenVT_5y"
   },
   "source": [
    "**단어 위치 임베딩 층**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3ooMUyFT_5y"
   },
   "outputs": [],
   "source": [
    "class PositionalEmbedding(layers.Layer):\n",
    "    def __init__(self, sequence_length, input_dim, output_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim=input_dim, output_dim=output_dim)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim=sequence_length, output_dim=output_dim)\n",
    "        self.sequence_length = sequence_length\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "    def call(self, inputs):\n",
    "        length = tf.shape(inputs)[-1]\n",
    "        positions = tf.range(start=0, limit=length, delta=1)\n",
    "        embedded_tokens = self.token_embeddings(inputs)\n",
    "        embedded_positions = self.position_embeddings(positions)\n",
    "        return embedded_tokens + embedded_positions\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(PositionalEmbedding, self).get_config()\n",
    "        config.update({\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length,\n",
    "            \"input_dim\": self.input_dim,\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Yw8lS_QT_5y"
   },
   "source": [
    "**트랜스포머 모델 지정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W5B1RqwgT_5y"
   },
   "outputs": [],
   "source": [
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "num_heads = 8\n",
    "\n",
    "encoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"english\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(encoder_inputs)\n",
    "encoder_outputs = TransformerEncoder(embed_dim, dense_dim, num_heads)(x)\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEmbedding(sequence_length, vocab_size, embed_dim)(decoder_inputs)\n",
    "x = TransformerDecoder(embed_dim, dense_dim, num_heads)(x, encoder_outputs)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "decoder_outputs = layers.Dense(vocab_size, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I53wCz6AT_5y"
   },
   "source": [
    "**시퀀스-투-시퀀스 모델 훈련**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZdbYo16QT_5y",
    "outputId": "494e1652-1a0f-440d-ec67-d77a613ed071"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1302/1302 [==============================] - 104s 76ms/step - loss: 3.7903 - accuracy: 0.4404 - val_loss: 2.8882 - val_accuracy: 0.5304\n",
      "Epoch 2/3\n",
      "1302/1302 [==============================] - 90s 69ms/step - loss: 2.8508 - accuracy: 0.5496 - val_loss: 2.4917 - val_accuracy: 0.5919\n",
      "Epoch 3/3\n",
      "1302/1302 [==============================] - 101s 77ms/step - loss: 2.5524 - accuracy: 0.5940 - val_loss: 2.3856 - val_accuracy: 0.6111\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7caadc6bad40>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "transformer.fit(train_ds, epochs=3, validation_data=val_ds)\n",
    "# transformer.fit(train_ds, epochs=30, validation_data=val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V_2veTqxT_5y"
   },
   "source": [
    "**텍스트 번역**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gtl4ic-tT_5y",
    "outputId": "161655a1-d1dd-408c-cf46-683f3e719223"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Tom isn't a good person.\n",
      "[start] tom no es una persona [end]\n",
      "-\n",
      "Automobile sales suffered a setback at the end of the financial year.\n",
      "[start] el examen de [UNK] a la luz al final de la año [end]\n",
      "-\n",
      "I don't need an explanation.\n",
      "[start] no necesito una explicación [end]\n",
      "-\n",
      "Tom is going to college now.\n",
      "[start] tom está a ir a la universidad ahora [end]\n",
      "-\n",
      "The dish is too sweet for Tom.\n",
      "[start] la carta es demasiado bajo para tom [end]\n",
      "-\n",
      "I don't think you know anything.\n",
      "[start] no sé que tú sabes nada [end]\n",
      "-\n",
      "How quickly does the bird fly?\n",
      "[start] cómo se puede correr el pájaro [end]\n",
      "-\n",
      "Do you know what Tom's secret is?\n",
      "[start] sabes lo que es el secreto de tom [end]\n",
      "-\n",
      "I'm going to cry.\n",
      "[start] voy a ir a llorar [end]\n",
      "-\n",
      "The reason she killed herself is unknown.\n",
      "[start] la razón se [UNK] a la razón [end]\n",
      "-\n",
      "If I were you, I'd go home right away.\n",
      "[start] si yo fuera tú yo fuera a casa mismo [end]\n",
      "-\n",
      "Go to work.\n",
      "[start] ve al trabajo [end]\n",
      "-\n",
      "I'll send you the link.\n",
      "[start] te [UNK] la luz [end]\n",
      "-\n",
      "No matter where you go, I'll follow you.\n",
      "[start] no te importa dónde te voy [end]\n",
      "-\n",
      "He came home three hours later.\n",
      "[start] Él vino a casa tres horas más tarde [end]\n",
      "-\n",
      "Tom explained the purpose of the project to Mary.\n",
      "[start] tom le explicó la cuenta de la proyecto a mary [end]\n",
      "-\n",
      "I can't live without her.\n",
      "[start] no puedo vivir sin ella [end]\n",
      "-\n",
      "From time to time she stopped and looked round.\n",
      "[start] de tiempo para que ella se dejó de tiempo [end]\n",
      "-\n",
      "I do not know any of them.\n",
      "[start] no sé que nadie [end]\n",
      "-\n",
      "Tom thinks he's being shadowed by a private detective.\n",
      "[start] tom cree que se está siendo un [UNK] de [UNK] [end]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "spa_vocab = target_vectorization.get_vocabulary()\n",
    "spa_index_lookup = dict(zip(range(len(spa_vocab)), spa_vocab))\n",
    "max_decoded_sentence_length = 20\n",
    "\n",
    "def decode_sequence(input_sentence):\n",
    "    tokenized_input_sentence = source_vectorization([input_sentence])\n",
    "    decoded_sentence = \"[start]\"\n",
    "    for i in range(max_decoded_sentence_length):\n",
    "        tokenized_target_sentence = target_vectorization(\n",
    "            [decoded_sentence])[:, :-1]\n",
    "        predictions = transformer(\n",
    "            [tokenized_input_sentence, tokenized_target_sentence])\n",
    "        sampled_token_index = np.argmax(predictions[0, i, :])\n",
    "        sampled_token = spa_index_lookup[sampled_token_index]\n",
    "        decoded_sentence += \" \" + sampled_token\n",
    "        if sampled_token == \"[end]\":\n",
    "            break\n",
    "    return decoded_sentence\n",
    "\n",
    "test_eng_texts = [pair[0] for pair in test_pairs]\n",
    "for _ in range(20):\n",
    "    input_sentence = random.choice(test_eng_texts)\n",
    "    print(\"-\")\n",
    "    print(input_sentence)\n",
    "    print(decode_sequence(input_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xFJXavRueOZC"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c86b3592b6800d985c04531f2c445f0fa6967131b8dd6395a925f7622e55602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
