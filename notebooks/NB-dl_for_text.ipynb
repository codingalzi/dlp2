{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k5ZZS6o0Z8Oj"
      },
      "source": [
        "# 11장 자연어처리 1부"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8OD2TTxZ8Om"
      },
      "source": [
        "## 11.1 자연어처리 소개"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B48vvr6KZ8Oo"
      },
      "source": [
        "## 11.2 텍스트 벡터화"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLeiiilMZ8Ou"
      },
      "source": [
        "## 11.3 단어 모음 표현법: 집합과 시퀀스"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6uzRLQDcZ8Ou"
      },
      "source": [
        "여기서는 IMDB 영화 후기 데이터를 이용하여 두 모델 방식의 \n",
        "활용법과 차이점을 소개한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGXhJe5XZ8Ou"
      },
      "source": [
        "### 11.3.1 IMDB 영화 후기 데이터 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rfwyzgmXZ8Ou"
      },
      "source": [
        "이전과는 달리 여기서는 IMDB 데이터셋을 직접 다운로드하여 전처리하는 \n",
        "과정을 살펴본다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaO7ilwSZ8Ou"
      },
      "source": [
        "준비 과정 1: 데이터셋 다운로드 압축 풀기\n",
        "\n",
        "압축을 풀면 아래 구조의 디렉토리가 생성된다.\n",
        "\n",
        "```\n",
        "aclImdb/\n",
        "...train/\n",
        "......pos/\n",
        "......neg/\n",
        "...test/\n",
        "......pos/\n",
        "......neg/\n",
        "```\n",
        "\n",
        "`train`의 `pos`와 `neg` 서브디렉토리에 각각 12,500개의 긍정과 부정 후기가\n",
        "포함되어 있다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eScCXFlWZ8Ou",
        "outputId": "5f58ef19-483b-4e43-aa25-1d120a86137d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 80.2M  100 80.2M    0     0  9303k      0  0:00:08  0:00:08 --:--:-- 16.4M\n"
          ]
        }
      ],
      "source": [
        "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
        "!tar -xf aclImdb_v1.tar.gz"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XMRKwRM9Z8Ou"
      },
      "source": [
        "`aclImdb/train/unsup` 서브디렉토리는 필요 없기에 삭제한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xsKw4vPZ8Ou"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !rm -r aclImdb/train/unsup\n",
        "else: \n",
        "    import shutil\n",
        "    unsup_path = './aclImdb/train/unsup'\n",
        "    shutil.rmtree(unsup_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9rZ3a9ETZ8Ou"
      },
      "source": [
        "긍정 후기 하나의 내용을 살펴보자.\n",
        "모델 구성 이전에 훈련 데이터셋을 살펴 보고\n",
        "모델에 대한 직관을 갖는 과정이 항상 필요하다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VK5FRFX0Z8Ou",
        "outputId": "4ce5b131-e7c3-4890-a285-76ef3448374b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !cat aclImdb/train/pos/4077_10.txt\n",
        "else:\n",
        "    with open('aclImdb/train/pos/4077_10.txt', 'r') as f:\n",
        "        text = f.read()\n",
        "        print(text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfeoV0YCZ8Ov"
      },
      "source": [
        "준비 과정 2: 검증셋 준비\n",
        "\n",
        "훈련셋의 20%를 검증셋으로 떼어낸다.\n",
        "이를 위해 `aclImdb/val` 디렉토리를 생성한 후에\n",
        "긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xX30tWVBZ8Ov"
      },
      "outputs": [],
      "source": [
        "import os, pathlib, shutil, random\n",
        "\n",
        "base_dir = pathlib.Path(\"aclImdb\")\n",
        "val_dir = base_dir / \"val\"\n",
        "train_dir = base_dir / \"train\"\n",
        "\n",
        "for category in (\"neg\", \"pos\"):\n",
        "    os.makedirs(val_dir / category)            # val 디렉토리 생성\n",
        "    files = os.listdir(train_dir / category)\n",
        "    \n",
        "    random.Random(1337).shuffle(files)         # 훈련셋 무작위 섞기\n",
        "    \n",
        "    num_val_samples = int(0.2 * len(files))    # 20% 지정 후 검증셋으로 옮기기\n",
        "    val_files = files[-num_val_samples:]\n",
        "    \n",
        "    for fname in val_files:\n",
        "        shutil.move(train_dir / category / fname,\n",
        "                    val_dir / category / fname)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fljBC3puZ8Ov"
      },
      "source": [
        "준비 과정 3: 텐서 데이터셋 준비\n",
        "\n",
        "`text_dataset_from_directory()` 함수를 이용하여 \n",
        "훈련셋, 검증셋, 테스트셋을 준비한다. \n",
        "자료형은 모두 `Dataset`이며, 배치 크기는 32를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41g6BtPZ8Ov",
        "outputId": "822834ca-acb3-4741-f661-a39dd3cba48f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 20000 files belonging to 2 classes.\n",
            "Found 5000 files belonging to 2 classes.\n",
            "Found 25000 files belonging to 2 classes.\n"
          ]
        }
      ],
      "source": [
        "from tensorflow import keras\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "train_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/train\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "val_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/val\", batch_size=batch_size\n",
        "    )\n",
        "\n",
        "test_ds = keras.utils.text_dataset_from_directory(\n",
        "    \"aclImdb/test\", batch_size=batch_size\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLi_rBmlZ8Ov"
      },
      "source": [
        "각 데이터셋은 배치로 구분되며\n",
        "입력은 `tf.string` 텐서이고, 타깃은 `int32` 텐서이다.\n",
        "크기는 모두 32이며 지정된 배치 크기이다.\n",
        "예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G05FSfO1Z8Ov",
        "outputId": "f3b02983-d278-4def-c809-8adc79199a31"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32,)\n",
            "inputs.dtype: <dtype: 'string'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(b'This movie is awful. At the end of it you will realize that several hours have been stolen from your life that you can\\'t get back. The \"twist\" ending is very contrived. The character development leading up to this ending is not consistent with their final actions at the conclusion. Ninety minutes of preparation-- with the premise that the Rob Lowe character will die on Christmas Eve-- is explained away in literally ninety seconds of \"No we were just tricking you.\" Then the Rob Lowe character is not even upset about it! \"I will forgive you if you can forgive me,\" is as upset as he gets. If someone took weeks to convince me I was about to die and then said \"No, sorry , just fooling you\" I would raise some serious hell. I don\\'t feel bad about giving away the spoiler because I might be able to save some of you out there from watching. Please save yourself and DON\\'T WATCH THIS MOVIE.', shape=(), dtype=string)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    \n",
        "    # 예제: 첫째 배치의 첫째 후기\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    \n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAVLNfkAZ8Ov"
      },
      "source": [
        "### 11.3.2 단어주머니 기법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ke4S9dNAZ8Ov"
      },
      "source": [
        "**방식 1: 유니그램 멀티-핫 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv53srVkZ8Ov"
      },
      "source": [
        "`TextVectorization` 클래스의 `output_mode=\"multi_hot\"` 옵션을 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VqPH5YjlZ8Ov"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.layers import TextVectorization\n",
        "\n",
        "text_vectorization = TextVectorization(\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")\n",
        "\n",
        "# 어휘색인 생성\n",
        "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
        "text_vectorization.adapt(text_only_train_ds)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "It6Yxh3BZ8Ow"
      },
      "source": [
        "생성된 어휘색인을 이용하여 훈련셋, 검증셋, 테스트셋 모두 벡터화한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kjDRTte3Z8Ow"
      },
      "outputs": [],
      "source": [
        "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yvWbCZP_Z8Ow"
      },
      "source": [
        "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
        "`max_tokens=20000`으로 지정하였기에 모든 문장은 길이가 2만인 벡터로 변환되었다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LgkB1GBiZ8Ow",
        "outputId": "5d5b75e8-4ff8-449f-91f7-4a3dedb9f98d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 20000)\n",
            "inputs.dtype: <dtype: 'float32'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor([0. 1. 0. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
            "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in binary_1gram_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5MpMNs0Z8Ow"
      },
      "source": [
        "*밀집 모델 활용*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ylif_1jYZ8Ow"
      },
      "source": [
        "단어주머니 모델로 여기서는 밀집 모델을 사용한다. \n",
        "`get_model()` 함수가 컴파일 된 단순한 밀집 모델을 반환한다.\n",
        "모델의 출력값은 긍정일 확률이며, \n",
        "최상위 층의 활성화 함수로 `sigmoid`를 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H0F71SW_Z8Ow"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "def get_model(max_tokens=20000, hidden_dim=16):\n",
        "    inputs = keras.Input(shape=(max_tokens,))\n",
        "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # 긍정일 확률 계산\n",
        "    \n",
        "    model = keras.Model(inputs, outputs)\n",
        "    \n",
        "    model.compile(optimizer=\"rmsprop\",\n",
        "                  loss=\"binary_crossentropy\",\n",
        "                  metrics=[\"accuracy\"])\n",
        "    \n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A72xJj81Z8Ow",
        "outputId": "748a877d-2e7d-45a6-8b21-4256cafbdeb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense (Dense)               (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 16)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e797rZZfZ8Ow"
      },
      "source": [
        "밀집 모델 훈련과정은 특별한 게 없다.\n",
        "훈련 후 테스트셋에 대한 정확도가 89% 보다 조금 낮게 나온다.\n",
        "최고 성능의 모델이 테스트셋에 대해 95% 정도 정확도를 내는 것보다는 낮지만\n",
        "무작위로 찍는 모델보다는 훨씬 좋은 모델이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u_HTlND7Z8Ow",
        "outputId": "fc0f1267-fb94-4453-b8fd-d65983190662"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 12s 14ms/step - loss: 0.3855 - accuracy: 0.8395 - val_loss: 0.2990 - val_accuracy: 0.8818\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2617 - accuracy: 0.9026 - val_loss: 0.2998 - val_accuracy: 0.8862\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2392 - accuracy: 0.9194 - val_loss: 0.3207 - val_accuracy: 0.8878\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2222 - accuracy: 0.9243 - val_loss: 0.3413 - val_accuracy: 0.8812\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2144 - accuracy: 0.9310 - val_loss: 0.3596 - val_accuracy: 0.8832\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2108 - accuracy: 0.9308 - val_loss: 0.3708 - val_accuracy: 0.8794\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2069 - accuracy: 0.9348 - val_loss: 0.3932 - val_accuracy: 0.8802\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2150 - accuracy: 0.9346 - val_loss: 0.4013 - val_accuracy: 0.8790\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2085 - accuracy: 0.9357 - val_loss: 0.4086 - val_accuracy: 0.8814\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2126 - accuracy: 0.9354 - val_loss: 0.4201 - val_accuracy: 0.8768\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2932 - accuracy: 0.8871\n",
            "Test acc: 0.887\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_1gram_train_ds.cache(),\n",
        "          validation_data=binary_1gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_1gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fSR-OuWBZ8Ow"
      },
      "source": [
        "**방식 2: 바이그램 멀티-핫 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-6ZgkkZZ8Ox"
      },
      "source": [
        "`TextVectorization` 클래스의 `ngrams=N` 옵션을 이용하면\n",
        "N-그램들로 이루어진 어휘색인을 생성할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RI1QftoAZ8Ox"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"multi_hot\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGNSDESNZ8Ox"
      },
      "source": [
        "어휘색인 생성과 훈련셋, 검증셋, 테스트셋의 벡터화 과정은 동일하다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IwK6WQY4Z8Ox"
      },
      "outputs": [],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gmDQyWhEZ8Ox"
      },
      "source": [
        "훈련 후 테스트셋에 대한 정확도가 90%를 조금 웃돌 정도로 많이 향상되었다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o6Flk8dIZ8Ox",
        "outputId": "d41d4a9d-b6bf-4e70-d6d9-18d41ef71966"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 0.3810 - accuracy: 0.8419 - val_loss: 0.2822 - val_accuracy: 0.8888\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2380 - accuracy: 0.9148 - val_loss: 0.2808 - val_accuracy: 0.8938\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2041 - accuracy: 0.9336 - val_loss: 0.3045 - val_accuracy: 0.8910\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1912 - accuracy: 0.9403 - val_loss: 0.3187 - val_accuracy: 0.8910\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1773 - accuracy: 0.9460 - val_loss: 0.3301 - val_accuracy: 0.8902\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1764 - accuracy: 0.9488 - val_loss: 0.3489 - val_accuracy: 0.8846\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1753 - accuracy: 0.9495 - val_loss: 0.3643 - val_accuracy: 0.8858\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1736 - accuracy: 0.9515 - val_loss: 0.3626 - val_accuracy: 0.8890\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1689 - accuracy: 0.9539 - val_loss: 0.3791 - val_accuracy: 0.8848\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.1663 - accuracy: 0.9529 - val_loss: 0.3768 - val_accuracy: 0.8840\n",
            "782/782 [==============================] - 7s 9ms/step - loss: 0.2695 - accuracy: 0.9004\n",
            "Test acc: 0.900\n"
          ]
        }
      ],
      "source": [
        "model = get_model()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(binary_2gram_train_ds.cache(),\n",
        "          validation_data=binary_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"binary_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "31J9DJI6Z8Ox"
      },
      "source": [
        "**방식 3: 바이그램 TF-IDF 인코딩**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRL6wVHKZ8Ox"
      },
      "source": [
        "`output_mode=\"tf_idf\"` 옵션을 사용하면 TF-IDF 인코딩을 지원한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo-6bcsmZ8Ox"
      },
      "outputs": [],
      "source": [
        "text_vectorization = TextVectorization(\n",
        "    ngrams=2,\n",
        "    max_tokens=20000,\n",
        "    output_mode=\"tf_idf\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5BGCzr0Z8Oy"
      },
      "source": [
        "훈련 후 테스트셋에 대한 정확도가 다시 89% 아래로 내려간다.\n",
        "여기서는 별 도움이 되지 않았지만 많은 텍스트 분류 모델에서는 1% 정도의 성능 향상을 가져온다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgz5cvX9Z8Oy",
        "outputId": "dd6d7e39-1ff5-483d-bcfb-3c08bcdc3ec8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_3 (InputLayer)        [(None, 20000)]           0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 16)                320016    \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 16)                0         \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 17        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 320,033\n",
            "Trainable params: 320,033\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 9s 13ms/step - loss: 0.4890 - accuracy: 0.8054 - val_loss: 0.2923 - val_accuracy: 0.8826\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.3095 - accuracy: 0.8806 - val_loss: 0.2969 - val_accuracy: 0.8856\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2727 - accuracy: 0.8976 - val_loss: 0.3090 - val_accuracy: 0.8874\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2512 - accuracy: 0.9071 - val_loss: 0.3295 - val_accuracy: 0.8806\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2450 - accuracy: 0.9080 - val_loss: 0.3520 - val_accuracy: 0.8604\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2362 - accuracy: 0.9121 - val_loss: 0.3289 - val_accuracy: 0.8682\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2309 - accuracy: 0.9103 - val_loss: 0.3388 - val_accuracy: 0.8672\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2187 - accuracy: 0.9155 - val_loss: 0.3481 - val_accuracy: 0.8720\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2167 - accuracy: 0.9161 - val_loss: 0.3577 - val_accuracy: 0.8632\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 3s 4ms/step - loss: 0.2152 - accuracy: 0.9164 - val_loss: 0.3569 - val_accuracy: 0.8678\n",
            "782/782 [==============================] - 8s 9ms/step - loss: 0.2886 - accuracy: 0.8914\n",
            "Test acc: 0.891\n"
          ]
        }
      ],
      "source": [
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "\n",
        "model = get_model()\n",
        "model.summary()\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "model.fit(tfidf_2gram_train_ds.cache(),\n",
        "          validation_data=tfidf_2gram_val_ds.cache(),\n",
        "          epochs=10,\n",
        "          callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
        "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM3uJTQVlJSy"
      },
      "source": [
        "### 11.3.3 시퀀스 활용법"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXZ34Qg7lJSy"
      },
      "source": [
        "*정수 벡터 데이터셋 준비*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoVifu3DlJSy"
      },
      "source": [
        "훈련셋의 모든 후기 문장을 정수들의 벡터로 변환한다.\n",
        "단, 후기 문장이 최대 600개의 단어만 포함하도록 한다. \n",
        "또한 사용되는 어휘는 빈도 기준 최대 2만개로 제한한다. \n",
        "\n",
        "- `max_length = 600`\n",
        "- `max_tokens = 20000`\n",
        "- `output_sequence_length=max_length`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "orh58qTYlJSy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers\n",
        "\n",
        "max_length = 600\n",
        "max_tokens = 20000\n",
        "\n",
        "text_vectorization = layers.TextVectorization(\n",
        "    max_tokens=max_tokens,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=max_length,\n",
        ")\n",
        "\n",
        "text_vectorization.adapt(text_only_train_ds)\n",
        "\n",
        "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
        "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T94MVnSHlJSy"
      },
      "source": [
        "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
        "`output_sequence_length=600`으로 지정하였기에 모든 문장은 단어를 최대 600개에서\n",
        "잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.\n",
        "물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다. \n",
        "또한 벡터에 사용된 정수는 2만보다 작은 값이며, \n",
        "이는 빈도가 가장 높은 2만개의 단어만을 대상(`max_tokens=20000`)으로 했기 때문이다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7B-SbA7rlJSy",
        "outputId": "f467ce8f-e595-4007-bfd2-1b94f2c2112e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs.shape: (32, 600)\n",
            "inputs.dtype: <dtype: 'int64'>\n",
            "targets.shape: (32,)\n",
            "targets.dtype: <dtype: 'int32'>\n",
            "inputs[0]: tf.Tensor(\n",
            "[   11     7     4  8614    18    38     9   139   138   197   640    12\n",
            "    30    22   167     6  3035     2    86  3146   664    19    12   291\n",
            "    11    14  2400  2996    13    55   322   429    11    19   172     4\n",
            "   337    35   116   230   172     4  1107     2   196  1562    14    12\n",
            "    10   399     9   100     9    14   478    46  1368   162    31    47\n",
            "   509    56     2  7585   645    66   733     5   239  1428     1    17\n",
            "     2    86    18     3    56    47   645    12    23    66     6    28\n",
            "   920     6   376    19   197   107 14487    39     8  8227    83    23\n",
            "   103   235     1    16  6307    13     4   309   869    21     2  7585\n",
            "   645    10    14   776     6   158    12   593     5     2   645    67\n",
            "    41  3488  5321     8   188    48    67   208    57     1    31    32\n",
            "     2  1990    67   154   239  1265    35   154    66     4     1     3\n",
            "    67   208     8    50  1244   450    39    55   322     6   103    12\n",
            "   217    53     6   493    72   167     6     2  3925     3    11    18\n",
            "     7   479     8   144     1    13  8499    49   330     2   223    14\n",
            "  5673    22   730    15  1428    15     8     2    86    42   327    18\n",
            "    19   943     5   250    16     2   322    57  2027  1932   383    62\n",
            "    14     4 13077    16    70     4   110   215    19   157   100   609\n",
            "     2  1013     5     1   500    55   322  3987    22   242     4  3852\n",
            "   690    14  2207    16    12  2227    13    32     8    32   450   129\n",
            "    11     7     4    84    18    16   322     5    98   588    29   172\n",
            "  1319  2224     6   381    99   104    10   328    22     6    28  2012\n",
            "  2677    19   193    66     6  1810    58     3   460   127     2   247\n",
            "   301     4   163    93    12    67   324     1    72   848    19   321\n",
            "  2224     6   544     2   698   301    11    29   450   129  1245   183\n",
            "   574   149    23   225   158    12    23   341     9   100     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0\n",
            "     0     0     0     0     0     0     0     0     0     0     0     0], shape=(600,), dtype=int64)\n",
            "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
          ]
        }
      ],
      "source": [
        "for inputs, targets in int_train_ds:\n",
        "    print(\"inputs.shape:\", inputs.shape)\n",
        "    print(\"inputs.dtype:\", inputs.dtype)\n",
        "    print(\"targets.shape:\", targets.shape)\n",
        "    print(\"targets.dtype:\", targets.dtype)\n",
        "    print(\"inputs[0]:\", inputs[0])\n",
        "    print(\"targets[0]:\", targets[0])\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ejQHih-lJS0"
      },
      "source": [
        "**시퀀스 생성법 1: 원-핫 단어 벡터 활용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "아래 코드는 `tf.one_hot()` 함수를 전처리로 활용하는 순환 신경망 모델을 정의한다. 이렇게 하면 정수 벡터를 바로 모델의 입력값으로 사용할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y9ydEYjxlJS0",
        "outputId": "bb85d226-23ae-48e8-bbb2-773802372ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_4\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_5 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " tf.one_hot (TFOpLambda)     (None, None, 20000)       0         \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 64)               5128448   \n",
            " l)                                                              \n",
            "                                                                 \n",
            " dropout_3 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_6 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,128,513\n",
            "Trainable params: 5,128,513\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# 원-핫 인코딩\n",
        "embedded = tf.one_hot(inputs, depth=max_tokens)  # (600, 20000) 모양의 출력값 생성\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Bqet0GflJS0"
      },
      "source": [
        "모델 훈련이 매우 느리다. \n",
        "이유는 입력 데이터가 너무 많은 특성을 갖기 때문이다. \n",
        "입력 데이터 하나의 모양과 특성 수는 다음과 같다.\n",
        "\n",
        "- 모양: `(600, 20000)`\n",
        "- 특성 수: `600 * 20,000 = 12,000,000`\n",
        "\n",
        "양방향 LSTM은 엄청난 양의 반복을 실행하기에 당연히 훈련 시간이 길어진다.\n",
        "게다가 훈련된 모델의 성능이 별로 좋지 않다.\n",
        "테스테셋에 대한 정확도가 87% 정도에 불과해서\n",
        "바이그램 모델보다 성능이 낮다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPqEPwkLlJS1"
      },
      "source": [
        "**주의사항**: 모델 훈련과정을 한 번 보기만 하려면 `epochs=1`로 설정하는 것을 권장한다.\n",
        "책에서는 원래 `epochs=10`을 사용하였는데 컴퓨터 성능에 따라 몇 시간이 소요될 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dTTTH31ylJS1",
        "outputId": "e95c6b1e-94ae-447b-a21c-abe1b8ff0f87"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "625/625 [==============================] - 178s 275ms/step - loss: 0.5245 - accuracy: 0.7528 - val_loss: 0.4744 - val_accuracy: 0.8194\n",
            "782/782 [==============================] - 105s 133ms/step - loss: 0.4757 - accuracy: 0.8214\n",
            "Test acc: 0.821\n"
          ]
        }
      ],
      "source": [
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7YOlU0yElJS1"
      },
      "source": [
        "**시퀀스 생성법 2: 단어 임베딩 활용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vYiogA5TlJS1"
      },
      "source": [
        "아래 코드는 단어 임베딩을 모델 구성에 직접 활용하는 것을 보여준다.\n",
        "여전히 양방향 LSTM 층을 사용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cY_Vv2chlJS1",
        "outputId": "6c1f32af-93d3-4e2f-9980-affc9149c24e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_5\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_6 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_4 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_7 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 37s 54ms/step - loss: 0.4664 - accuracy: 0.7949 - val_loss: 0.4624 - val_accuracy: 0.7898\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.3018 - accuracy: 0.8873 - val_loss: 0.3227 - val_accuracy: 0.8774\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 33s 53ms/step - loss: 0.2372 - accuracy: 0.9125 - val_loss: 0.3195 - val_accuracy: 0.8814\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 32s 51ms/step - loss: 0.2022 - accuracy: 0.9277 - val_loss: 0.3314 - val_accuracy: 0.8838\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 33s 52ms/step - loss: 0.1699 - accuracy: 0.9444 - val_loss: 0.3550 - val_accuracy: 0.8760\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 32s 50ms/step - loss: 0.1408 - accuracy: 0.9524 - val_loss: 0.3673 - val_accuracy: 0.8768\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 33s 54ms/step - loss: 0.1184 - accuracy: 0.9607 - val_loss: 0.4460 - val_accuracy: 0.8808\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.1001 - accuracy: 0.9684 - val_loss: 0.4177 - val_accuracy: 0.8784\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0843 - accuracy: 0.9736 - val_loss: 0.4034 - val_accuracy: 0.8666\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0745 - accuracy: 0.9768 - val_loss: 0.6037 - val_accuracy: 0.8688\n",
            "782/782 [==============================] - 24s 30ms/step - loss: 0.3611 - accuracy: 0.8650\n",
            "Test acc: 0.865\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# 단어 임베딩\n",
        "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xt8yQOrDlJS1"
      },
      "source": [
        "훈련은 원-핫 인코딩 방식보다 훨씬 빠르게 이루어지며 성능은 87% 정도로 비슷하다. \n",
        "바이그램 모델보다 성능이 여전히 떨어지는 이유 중에 하나는 후기에 사용된 단어의 수를 600개로 제한하였기 때문이다. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MUNZKa1JlJS2"
      },
      "source": [
        "*패딩과 마스킹*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6T2XbN_LlJS2"
      },
      "source": [
        "아래 코드는 마스킹을 활용하는 방식을 보여준다.\n",
        "\n",
        "- `mask_zero=True` 옵션: 마스킹 옵션 켜기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsAh7IpUlJS2",
        "outputId": "c42ef514-0884-482c-d01a-e0053bb716a9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_7 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 64)               73984     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_5 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,194,049\n",
            "Trainable params: 5,194,049\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 48s 64ms/step - loss: 0.3945 - accuracy: 0.8210 - val_loss: 0.3775 - val_accuracy: 0.8426\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 38s 60ms/step - loss: 0.2302 - accuracy: 0.9123 - val_loss: 0.2707 - val_accuracy: 0.8820\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.1731 - accuracy: 0.9348 - val_loss: 0.2907 - val_accuracy: 0.8866\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.1306 - accuracy: 0.9542 - val_loss: 0.3093 - val_accuracy: 0.8830\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 36s 57ms/step - loss: 0.0993 - accuracy: 0.9656 - val_loss: 0.3547 - val_accuracy: 0.8852\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.0740 - accuracy: 0.9737 - val_loss: 0.3823 - val_accuracy: 0.8720\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 37s 59ms/step - loss: 0.0532 - accuracy: 0.9817 - val_loss: 0.4408 - val_accuracy: 0.8786\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0397 - accuracy: 0.9876 - val_loss: 0.4608 - val_accuracy: 0.8734\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 36s 58ms/step - loss: 0.0287 - accuracy: 0.9914 - val_loss: 0.5242 - val_accuracy: 0.8652\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.0209 - accuracy: 0.9926 - val_loss: 0.6575 - val_accuracy: 0.8686\n",
            "782/782 [==============================] - 26s 30ms/step - loss: 0.2918 - accuracy: 0.8766\n",
            "Test acc: 0.877\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# 마스킹 활용 단어 임베딩\n",
        "embedded = layers.Embedding(\n",
        "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrSKzVyklJS2"
      },
      "source": [
        "모델 성능이 88% 정도로 살짝 향상된다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARMW4aZzlJS2"
      },
      "source": [
        "**시퀀스 생성법 3: GloVe 단어 임베딩 활용**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ay48poX2lJS2"
      },
      "source": [
        "- GloVe 단어 임베딩 다운로드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xC-wYwztlJS2",
        "outputId": "b422bab7-6d47-4776-e8ce-dd42a7b546a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2022-11-22 14:06:53--  http://nlp.stanford.edu/data/glove.6B.zip\n",
            "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
            "--2022-11-22 14:06:53--  https://nlp.stanford.edu/data/glove.6B.zip\n",
            "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
            "--2022-11-22 14:06:54--  https://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
            "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
            "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 862182613 (822M) [application/zip]\n",
            "Saving to: ‘glove.6B.zip’\n",
            "\n",
            "glove.6B.zip        100%[===================>] 822.24M  4.34MB/s    in 3m 12s  \n",
            "\n",
            "2022-11-22 14:10:07 (4.28 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "    !unzip -q glove.6B.zip\n",
        "else: \n",
        "    try: \n",
        "        import wget, zipfile\n",
        "    except ModuleNotFoundError: \n",
        "        !pip install wget\n",
        "        \n",
        "    import wget, zipfile\n",
        "    wget.download('http://nlp.stanford.edu/data/glove.6B.zip')\n",
        "    with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
        "        zip_ref.extractall('./')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KA3AjJN_lJS2"
      },
      "source": [
        "- GloVe 워드 임베딩 파일 파싱"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dMCuC4cslJS2",
        "outputId": "407df77d-ef18-4547-e551-154299ac6203"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 400000 word vectors.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "path_to_glove_file = \"glove.6B.100d.txt\"\n",
        "\n",
        "embeddings_index = {}\n",
        "\n",
        "with open(path_to_glove_file) as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(f\"Found {len(embeddings_index)} word vectors.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9YWmdEllJS2"
      },
      "source": [
        "- GloVe 단어 임베딩 행렬 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qH5olPZNlJS2"
      },
      "outputs": [],
      "source": [
        "embedding_dim = 100\n",
        "\n",
        "vocabulary = text_vectorization.get_vocabulary()\n",
        "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
        "\n",
        "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
        "for word, i in word_index.items():\n",
        "    if i < max_tokens:\n",
        "        embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGYQV_4jlJS3"
      },
      "source": [
        "- 임베딩 층 준비"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oPgZRjjalJS3"
      },
      "outputs": [],
      "source": [
        "embedding_layer = layers.Embedding(\n",
        "    max_tokens,\n",
        "    embedding_dim,\n",
        "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
        "    trainable=False,\n",
        "    mask_zero=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZbCEnXJlJS3"
      },
      "source": [
        "- GloVe 임베딩 활용 모델 구성 및 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-XryyZ10lJS3",
        "outputId": "23355aba-7574-4ac6-9229-4e0417f2e7e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"model_7\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_8 (InputLayer)        [(None, None)]            0         \n",
            "                                                                 \n",
            " embedding_2 (Embedding)     (None, None, 100)         2000000   \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 64)               34048     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dropout_6 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 1)                 65        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 2,034,113\n",
            "Trainable params: 34,113\n",
            "Non-trainable params: 2,000,000\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n",
            "625/625 [==============================] - 45s 61ms/step - loss: 0.5720 - accuracy: 0.6967 - val_loss: 0.4727 - val_accuracy: 0.7714\n",
            "Epoch 2/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.4500 - accuracy: 0.7962 - val_loss: 0.4004 - val_accuracy: 0.8246\n",
            "Epoch 3/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.3940 - accuracy: 0.8264 - val_loss: 0.3646 - val_accuracy: 0.8364\n",
            "Epoch 4/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.3614 - accuracy: 0.8476 - val_loss: 0.4692 - val_accuracy: 0.7940\n",
            "Epoch 5/10\n",
            "625/625 [==============================] - 35s 57ms/step - loss: 0.3334 - accuracy: 0.8590 - val_loss: 0.3286 - val_accuracy: 0.8518\n",
            "Epoch 6/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.3148 - accuracy: 0.8699 - val_loss: 0.3218 - val_accuracy: 0.8552\n",
            "Epoch 7/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.2947 - accuracy: 0.8805 - val_loss: 0.3205 - val_accuracy: 0.8600\n",
            "Epoch 8/10\n",
            "625/625 [==============================] - 34s 55ms/step - loss: 0.2803 - accuracy: 0.8881 - val_loss: 0.3178 - val_accuracy: 0.8574\n",
            "Epoch 9/10\n",
            "625/625 [==============================] - 34s 54ms/step - loss: 0.2636 - accuracy: 0.8941 - val_loss: 0.3140 - val_accuracy: 0.8606\n",
            "Epoch 10/10\n",
            "625/625 [==============================] - 35s 56ms/step - loss: 0.2543 - accuracy: 0.8977 - val_loss: 0.2957 - val_accuracy: 0.8716\n",
            "782/782 [==============================] - 22s 25ms/step - loss: 0.2899 - accuracy: 0.8804\n",
            "Test acc: 0.880\n"
          ]
        }
      ],
      "source": [
        "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
        "\n",
        "# GloVe 단어 임베딩 활용\n",
        "embedded = embedding_layer(inputs)\n",
        "\n",
        "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "\n",
        "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
        "\n",
        "model = keras.Model(inputs, outputs)\n",
        "\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"binary_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "callbacks = [\n",
        "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
        "                                    save_best_only=True)\n",
        "]\n",
        "\n",
        "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
        "\n",
        "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
        "\n",
        "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.12 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "ad0f3c8a064f687cbf898a0868fd45ba1c7e928ac8a0404f7c241d812ddc1e76"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
