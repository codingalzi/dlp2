{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k5ZZS6o0Z8Oj"
   },
   "source": [
    "# 11장 자연어처리 1부"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x8OD2TTxZ8Om"
   },
   "source": [
    "## 11.1 자연어처리 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B48vvr6KZ8Oo"
   },
   "source": [
    "## 11.2 텍스트 벡터화"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLeiiilMZ8Ou"
   },
   "source": [
    "## 11.3 단어 모음 표현법: 집합과 시퀀스"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uzRLQDcZ8Ou"
   },
   "source": [
    "여기서는 IMDB 영화 리뷰 데이터를 이용하여 두 모델 방식의 \n",
    "활용법과 차이점을 소개한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BGXhJe5XZ8Ou"
   },
   "source": [
    "### 11.3.1 IMDB 영화 리뷰 데이터 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rfwyzgmXZ8Ou"
   },
   "source": [
    "이전과는 달리 여기서는 IMDB 데이터셋을 직접 다운로드하여 전처리하는 \n",
    "과정을 살펴본다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KaO7ilwSZ8Ou"
   },
   "source": [
    "준비 과정 1: 데이터셋 다운로드 압축 풀기\n",
    "\n",
    "압축을 풀면 아래 구조의 디렉토리가 생성된다.\n",
    "\n",
    "```\n",
    "aclImdb/\n",
    "...train/\n",
    "......pos/\n",
    "......neg/\n",
    "...test/\n",
    "......pos/\n",
    "......neg/\n",
    "```\n",
    "\n",
    "`train`의 `pos`와 `neg` 서브디렉토리에 각각 12,500개의 긍정과 부정 리뷰가\n",
    "포함되어 있다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eScCXFlWZ8Ou",
    "outputId": "c012c11c-3fa8-4e0e-8ae0-abe428e09101"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 80.2M  100 80.2M    0     0  26.1M      0  0:00:03  0:00:03 --:--:-- 26.1M\n"
     ]
    }
   ],
   "source": [
    "!curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -xf aclImdb_v1.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XMRKwRM9Z8Ou"
   },
   "source": [
    "`aclImdb/train/unsup` 서브디렉토리는 필요 없기에 삭제한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "2xsKw4vPZ8Ou"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !rm -r aclImdb/train/unsup\n",
    "else: \n",
    "    import shutil\n",
    "    unsup_path = './aclImdb/train/unsup'\n",
    "    shutil.rmtree(unsup_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9rZ3a9ETZ8Ou"
   },
   "source": [
    "긍정 리뷰 하나의 내용을 살펴보자.\n",
    "모델 구성 이전에 훈련 데이터셋을 살펴 보고\n",
    "모델에 대한 직관을 갖는 과정이 항상 필요하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VK5FRFX0Z8Ou",
    "outputId": "346927ca-d81f-4c01-8074-87a92a7bd90b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I first saw this back in the early 90s on UK TV, i did like it then but i missed the chance to tape it, many years passed but the film always stuck with me and i lost hope of seeing it TV again, the main thing that stuck with me was the end, the hole castle part really touched me, its easy to watch, has a great story, great music, the list goes on and on, its OK me saying how good it is but everyone will take there own best bits away with them once they have seen it, yes the animation is top notch and beautiful to watch, it does show its age in a very few parts but that has now become part of it beauty, i am so glad it has came out on DVD as it is one of my top 10 films of all time. Buy it or rent it just see it, best viewing is at night alone with drink and food in reach so you don't have to stop the film.<br /><br />Enjoy"
     ]
    }
   ],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !cat aclImdb/train/pos/4077_10.txt\n",
    "else:\n",
    "    with open('aclImdb/train/pos/4077_10.txt', 'r') as f:\n",
    "        text = f.read()\n",
    "        print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IfeoV0YCZ8Ov"
   },
   "source": [
    "준비 과정 2: 검증셋 준비\n",
    "\n",
    "훈련셋의 20%를 검증셋으로 떼어낸다.\n",
    "이를 위해 `aclImdb/val` 디렉토리를 생성한 후에\n",
    "긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "xX30tWVBZ8Ov"
   },
   "outputs": [],
   "source": [
    "import os, pathlib, shutil, random\n",
    "\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category)            # val 디렉토리 생성\n",
    "    files = os.listdir(train_dir / category)\n",
    "    \n",
    "    random.Random(1337).shuffle(files)         # 훈련셋 무작위 섞기\n",
    "    \n",
    "    num_val_samples = int(0.2 * len(files))    # 20% 지정 후 검증셋으로 옮기기\n",
    "    val_files = files[-num_val_samples:]\n",
    "    \n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fljBC3puZ8Ov"
   },
   "source": [
    "준비 과정 3: 텐서 데이터셋 준비\n",
    "\n",
    "`text_dataset_from_directory()` 함수를 이용하여 \n",
    "훈련셋, 검증셋, 테스트셋을 준비한다. \n",
    "자료형은 모두 `Dataset`이며, 배치 크기는 32를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "e41g6BtPZ8Ov",
    "outputId": "b40fd231-c8d5-4ac9-86df-a291b3b40e64"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 20000 files belonging to 2 classes.\n",
      "Found 5000 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    "    )\n",
    "\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    "    )\n",
    "\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qLi_rBmlZ8Ov"
   },
   "source": [
    "각 데이터셋은 배치로 구분되며\n",
    "입력은 `tf.string` 텐서이고, 타깃은 `int32` 텐서이다.\n",
    "크기는 모두 32이며 지정된 배치 크기이다.\n",
    "예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G05FSfO1Z8Ov",
    "outputId": "590ae91b-24f2-45a7-864a-ab9e8615e6e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32,)\n",
      "inputs.dtype: <dtype: 'string'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor(b'The film begins with a bunch of kids in reform school and focuses on a kid named \\'Gabe\\', who has apparently worked hard to earn his parole. Gabe and his sister move to a new neighborhood to make a fresh start and soon Gabe meets up with the Dead End Kids. The Kids in this film are little punks, but they are much less antisocial than they\\'d been in other previous films and down deep, they are well-meaning punks. However, in this neighborhood there are also some criminals who are perpetrating insurance fraud through arson and see Gabe as a convenient scapegoat--after all, he\\'d been to reform school and no one would believe he was innocent once he was framed. So, when Gabe is about ready to be sent back to \"The Big House\", it\\'s up to the rest of the gang to save him and expose the real crooks.<br /><br />The \"Dead End Kids\" appeared in several Warner Brothers films in the late 1930s and the films were generally very good (particularly ANGELS WITH DIRTY FACES). However, after the boys\\' contracts expired, they went on to Monogram Studios and the films, to put it charitably, were very weak and formulaic--with Huntz Hall and Leo Gorcey being pretty much the whole show and the group being renamed \"The Bowery Boys\". Because ANGELS WASH THEIR FACES had the excellent writing and production values AND Hall and Gorcey were not constantly mugging for the camera, it\\'s a pretty good film--and almost earns a score of 7 (it\\'s REAL close). In fact, while this isn\\'t a great film aesthetically, it\\'s sure a lot of fun to watch, so I will give it a 7! Sure, it was a tad hokey-particularly towards the end when the kids take the law into their own hands and Reagan ignores the Bill of Rights--but it was also quite entertaining. The Dead End Kids are doing their best performances and Ronald Reagan and Ann Sheridan provided excellent support. Sure, this part of the film was illogical and impossible but somehow it was still funny and rather charming--so if you can suspend disbelief, it works well.', shape=(), dtype=string)\n",
      "targets[0]: tf.Tensor(1, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    \n",
    "    # 예제: 첫째 배치의 첫째 리뷰\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    \n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAVLNfkAZ8Ov"
   },
   "source": [
    "### 11.3.2 단어주머니 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z17LE7S-Z8Ov"
   },
   "source": [
    "단어주머니에 채울 토큰으로 어떤 N-그램을 사용할지 먼저 지정해야 한다. \n",
    "\n",
    "- 유니그램(unigrams): 하나의 단어가 한의 토큰\n",
    "- N-그램(N-grams): 최대 N 개의 연속 단어로 이루어진 토큰"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ke4S9dNAZ8Ov"
   },
   "source": [
    "**방식 1: 유니그램 바이너리 인코딩**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rv53srVkZ8Ov"
   },
   "source": [
    "예를 들어 \"the cat sat on the mat\" 문장을 유니그램으로 처리하면 다음 \n",
    "단어주머니가 생성된다.\n",
    "집합으로 처리되기에 단어들의 순서는 완전히 무시된다.\n",
    "\n",
    "```\n",
    "{\"cat\", \"mat\", \"on\", \"sat\", \"the\"}\n",
    "```\n",
    "\n",
    "이제 모든 문장은 어휘색인에 포함된 단어들의 수만큼 긴 1차원 이진 텐서(binary tensor)로\n",
    "처리된다. \n",
    "즉, 멀티-핫(multi-hot) 인코딩 방식을 사용해서 텐서로 변환된다.\n",
    "[4장](https://codingalzi.github.io/dlp/notebooks/dlp04_getting_started_with_neural_networks.html)과 \n",
    "[5장](https://codingalzi.github.io/dlp/notebooks/dlp05_fundamentals_of_ml.html)에서 \n",
    "문장을 인코딩 방식과 동일하다. \n",
    "\n",
    "`TextVectorization` 클래스의 `output_mode=\"multi_hot\"` 옵션을 이용하면\n",
    "방금 설명한 내용을 그대로 처리해준다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "VqPH5YjlZ8Ov"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import TextVectorization\n",
    "\n",
    "text_vectorization = TextVectorization(\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")\n",
    "\n",
    "# 어휘색인 생성\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)\n",
    "text_vectorization.adapt(text_only_train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "It6Yxh3BZ8Ow"
   },
   "source": [
    "생성된 어휘색인을 이용하여 훈련셋, 검증셋, 테스트셋 모두 벡터화한다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "kjDRTte3Z8Ow"
   },
   "outputs": [],
   "source": [
    "binary_1gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_1gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvWbCZP_Z8Ow"
   },
   "source": [
    "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
    "`max_tokens=20000`으로 지정하였기에 모든 문장은 길이가 2만인 벡터로 변환되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LgkB1GBiZ8Ow",
    "outputId": "138995c1-7172-48ac-b7bd-268e87cbe66e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs.shape: (32, 20000)\n",
      "inputs.dtype: <dtype: 'float32'>\n",
      "targets.shape: (32,)\n",
      "targets.dtype: <dtype: 'int32'>\n",
      "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
      "targets[0]: tf.Tensor(0, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "for inputs, targets in binary_1gram_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W5MpMNs0Z8Ow"
   },
   "source": [
    "**밀집 모델 지정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ylif_1jYZ8Ow"
   },
   "source": [
    "단어주머니 모델로 여기서는 밀집 모델을 사용한다. \n",
    "`get_model()` 함수가 컴파일 된 단순한 밀집 모델을 반환한다.\n",
    "모델의 출력값은 긍정일 확률이며, \n",
    "최상위 층의 활성화 함수로 `sigmoid`를 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "H0F71SW_Z8Ow"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def get_model(max_tokens=20000, hidden_dim=16):\n",
    "    inputs = keras.Input(shape=(max_tokens,))\n",
    "    x = layers.Dense(hidden_dim, activation=\"relu\")(inputs)\n",
    "    x = layers.Dropout(0.5)(x)\n",
    "    outputs = layers.Dense(1, activation=\"sigmoid\")(x)  # 긍정일 확률 계산\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    \n",
    "    model.compile(optimizer=\"rmsprop\",\n",
    "                  loss=\"binary_crossentropy\",\n",
    "                  metrics=[\"accuracy\"])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "A72xJj81Z8Ow",
    "outputId": "d6024f1f-c272-417b-df2e-d2350fcd08bf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PvGyigWsZ8Ow"
   },
   "source": [
    "**모델 훈련**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e797rZZfZ8Ow"
   },
   "source": [
    "밀집 모델 훈련과정은 특별한 게 없다.\n",
    "훈련 후 테스트셋에 대한 정확도가 89% 보다 조금 낮게 나온다.\n",
    "최고 성능의 모델이 테스트셋에 대해 95% 정도 정확도를 내는 것보다는 낮지만\n",
    "무작위로 찍는 모델보다는 훨씬 좋은 모델이다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u_HTlND7Z8Ow",
    "outputId": "48a72a1c-4695-48b8-d63a-6ee780f47b96"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 10s 16ms/step - loss: 0.4074 - accuracy: 0.8277 - val_loss: 0.2792 - val_accuracy: 0.8908\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2746 - accuracy: 0.8981 - val_loss: 0.2774 - val_accuracy: 0.8964\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2471 - accuracy: 0.9115 - val_loss: 0.2872 - val_accuracy: 0.8976\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 3s 5ms/step - loss: 0.2246 - accuracy: 0.9244 - val_loss: 0.3187 - val_accuracy: 0.8936\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2156 - accuracy: 0.9313 - val_loss: 0.3164 - val_accuracy: 0.8960\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 3s 6ms/step - loss: 0.2108 - accuracy: 0.9341 - val_loss: 0.3355 - val_accuracy: 0.8934\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2052 - accuracy: 0.9366 - val_loss: 0.3354 - val_accuracy: 0.8944\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2017 - accuracy: 0.9365 - val_loss: 0.3582 - val_accuracy: 0.8940\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2013 - accuracy: 0.9394 - val_loss: 0.3497 - val_accuracy: 0.8938\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2043 - accuracy: 0.9394 - val_loss: 0.3631 - val_accuracy: 0.8940\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2861 - accuracy: 0.8885\n",
      "Test acc: 0.888\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_1gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_1gram_train_ds.cache(),\n",
    "          validation_data=binary_1gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"binary_1gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_1gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fSR-OuWBZ8Ow"
   },
   "source": [
    "**방식 2: 바이그램 바이너리 인코딩**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-6ZgkkZZ8Ox"
   },
   "source": [
    "바이그램(2-grams)을 유니그램 대신 이용해보자. \n",
    "예를 들어 \"the cat sat on the mat\" 문장을 바이그램으로 처리하면 다음 \n",
    "단어주머니가 생성된다.\n",
    "\n",
    "```\n",
    "{\"the\", \"the cat\", \"cat\", \"cat sat\", \"sat\",\n",
    " \"sat on\", \"on\", \"on the\", \"the mat\", \"mat\"}\n",
    "```\n",
    "\n",
    "`TextVectorization` 클래스의 `ngrams=N` 옵션을 이용하면\n",
    "N-그램들로 이루어진 어휘색인을 생성할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "RI1QftoAZ8Ox"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"multi_hot\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGNSDESNZ8Ox"
   },
   "source": [
    "어휘색인 생성과 훈련셋, 검증셋, 테스트셋의 벡터화 과정은 동일하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "IwK6WQY4Z8Ox"
   },
   "outputs": [],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "binary_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "binary_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gmDQyWhEZ8Ox"
   },
   "source": [
    "훈련 후 테스트셋에 대한 정확도가 90%를 조금 웃돌 정도로 많이 향상되었다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "o6Flk8dIZ8Ox",
    "outputId": "c9d1a291-150a-4130-d616-268015a5354f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "625/625 [==============================] - 12s 18ms/step - loss: 0.3857 - accuracy: 0.8347 - val_loss: 0.2791 - val_accuracy: 0.9000\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2592 - accuracy: 0.9082 - val_loss: 0.2947 - val_accuracy: 0.8988\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2277 - accuracy: 0.9241 - val_loss: 0.3060 - val_accuracy: 0.8978\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2074 - accuracy: 0.9333 - val_loss: 0.3417 - val_accuracy: 0.8994\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2070 - accuracy: 0.9365 - val_loss: 0.3538 - val_accuracy: 0.8968\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1997 - accuracy: 0.9395 - val_loss: 0.3908 - val_accuracy: 0.8946\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1940 - accuracy: 0.9421 - val_loss: 0.3715 - val_accuracy: 0.8940\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1902 - accuracy: 0.9427 - val_loss: 0.4054 - val_accuracy: 0.8930\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1952 - accuracy: 0.9432 - val_loss: 0.3848 - val_accuracy: 0.8880\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.1949 - accuracy: 0.9441 - val_loss: 0.4011 - val_accuracy: 0.8912\n",
      "782/782 [==============================] - 9s 11ms/step - loss: 0.2788 - accuracy: 0.8953\n",
      "Test acc: 0.895\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"binary_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(binary_2gram_train_ds.cache(),\n",
    "          validation_data=binary_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"binary_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(binary_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31J9DJI6Z8Ox"
   },
   "source": [
    "**방식 3: 바이그램 TF-IDF 인코딩**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UozpwugMZ8Ox"
   },
   "source": [
    "N-그램을 벡터화할 때 사용 빈도를 함께 저장하는 방식을 사용할 수 있다.\n",
    "단어의 사용 빈도가 아무래도 문장 평가에 중요한 역할을 수행할 것이기 때문이다.\n",
    "아래 코드에서처럼 `output_mode=\"count\"` 옵션을 사용하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "NjdliURqZ8Ox"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"count\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SRW9qHQwZ8Ox"
   },
   "source": [
    "그런데 이렇게 하면 \"the\", \"a\", \"is\", \"are\" 등의 사용 빈도는 매우 높은 반면에\n",
    "\"Chollet\" 등의 단어는 빈도가 거의 0에 가깝게 나온다.\n",
    "또한 생성된 벡터의 대부분은 0으로 채워질 것이다. \n",
    "`max_tokens=20000`을 사용한 반면에 하나의 문장엔 많아야 몇 십개 정도의 단어만 사용되었기 때문이다. \n",
    "\n",
    "```python\n",
    "inputs[0]: tf.Tensor([1. 1. 1. ... 0. 0. 0.], shape=(20000,), dtype=float32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y3CeRLImZ8Ox"
   },
   "source": [
    "이 점을 고려해서 사용 빈도를 정규화한다. \n",
    "평균을 원점으로 만들지는 않고 TF-IDF 값으로 나누기만 실행한다.\n",
    "이유는 평균을 옮기면 벡터의 대부분의 값이 0이 아니게 되어\n",
    "훈련에 보다 많은 계산이 요구되기 때문이다. \n",
    "\n",
    "**TF-IDF**의 의미는 다음과 같다.\n",
    "\n",
    "- `TF`(Term Frequency)\n",
    "    - 하나의 문장에서 사용되는 단어의 빈도\n",
    "    - 높을 수록 중요\n",
    "    - 예를 들어, 하나의 리뷰에 \"terrible\" 이 많이 사용되었다면\n",
    "        해당 리뷰는 부정일 가능성 높음.\n",
    "- `IDF`(Inverse Document Frequency)\n",
    "    - 데이터셋 전체 문장에서 사용된 단어의 빈도\n",
    "    - 낮을 수록 중요. \n",
    "    - \"the\", \"a\", \"is\" 등의 `IDF` 값은 높지만 별로 중요하지 않음.\n",
    "- `TF-IDF = TF / IDF`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DRL6wVHKZ8Ox"
   },
   "source": [
    "`output_mode=\"tf_idf\"` 옵션을 사용하면 TF-IDF 인코딩을 지원한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "fo-6bcsmZ8Ox"
   },
   "outputs": [],
   "source": [
    "text_vectorization = TextVectorization(\n",
    "    ngrams=2,\n",
    "    max_tokens=20000,\n",
    "    output_mode=\"tf_idf\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A5BGCzr0Z8Oy"
   },
   "source": [
    "훈련 후 테스트셋에 대한 정확도가 다시 89% 아래로 내려간다.\n",
    "여기서는 별 도움이 되지 않았지만 많은 텍스트 분류 모델에서는 1% 정도의 성능 향상을 가져온다.\n",
    "\n",
    "**주의사항**: 아래 코드는 현재(Tensorflow 2.6과 2.7) GPU를 사용하지 않는 경우에만 작동한다. \n",
    "이유는 아직 모른다([여기 참조](https://github.com/fchollet/deep-learning-with-python-notebooks/issues/190))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zgz5cvX9Z8Oy",
    "outputId": "5feb8d61-4efd-4c97-af2a-bda88966fad2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_3 (InputLayer)        [(None, 20000)]           0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 16)                320016    \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 16)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 17        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 320,033\n",
      "Trainable params: 320,033\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "625/625 [==============================] - 11s 17ms/step - loss: 0.5232 - accuracy: 0.7588 - val_loss: 0.3197 - val_accuracy: 0.8806\n",
      "Epoch 2/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3534 - accuracy: 0.8442 - val_loss: 0.2946 - val_accuracy: 0.8954\n",
      "Epoch 3/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3231 - accuracy: 0.8609 - val_loss: 0.3086 - val_accuracy: 0.8864\n",
      "Epoch 4/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.3053 - accuracy: 0.8734 - val_loss: 0.3087 - val_accuracy: 0.8814\n",
      "Epoch 5/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2781 - accuracy: 0.8845 - val_loss: 0.3225 - val_accuracy: 0.8878\n",
      "Epoch 6/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2703 - accuracy: 0.8870 - val_loss: 0.3472 - val_accuracy: 0.8702\n",
      "Epoch 7/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2695 - accuracy: 0.8883 - val_loss: 0.3357 - val_accuracy: 0.8682\n",
      "Epoch 8/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2650 - accuracy: 0.8931 - val_loss: 0.3343 - val_accuracy: 0.8664\n",
      "Epoch 9/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2606 - accuracy: 0.8901 - val_loss: 0.3546 - val_accuracy: 0.8580\n",
      "Epoch 10/10\n",
      "625/625 [==============================] - 4s 6ms/step - loss: 0.2575 - accuracy: 0.8924 - val_loss: 0.3318 - val_accuracy: 0.8760\n",
      "782/782 [==============================] - 8s 10ms/step - loss: 0.2998 - accuracy: 0.8927\n",
      "Test acc: 0.893\n"
     ]
    }
   ],
   "source": [
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "tfidf_2gram_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "tfidf_2gram_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "\n",
    "model = get_model()\n",
    "model.summary()\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"tfidf_2gram.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(tfidf_2gram_train_ds.cache(),\n",
    "          validation_data=tfidf_2gram_val_ds.cache(),\n",
    "          epochs=10,\n",
    "          callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"tfidf_2gram.keras\")\n",
    "print(f\"Test acc: {model.evaluate(tfidf_2gram_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i0qpJU1kZ8Oy"
   },
   "source": [
    "**부록: 문자열 벡터화 전처리를 함께 처리하는 모델 내보내기**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTGUYv3WZ8Oy"
   },
   "source": [
    "훈련된 모델을 실전에 배치하려면 텍스트 벡터화도 모델과 함께 내보내야 한다.\n",
    "이를 위해 `TextVectorization` 층의 결과를 재활용만 하면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "GKVJPepUZ8Oy"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(1,), dtype=\"string\")\n",
    "# 텍스트 벡터화 추가\n",
    "processed_inputs = text_vectorization(inputs)\n",
    "# 훈련된 모델에 적용\n",
    "outputs = model(processed_inputs)\n",
    "\n",
    "# 최종 모델\n",
    "inference_model = keras.Model(inputs, outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIw88cllZ8Oy"
   },
   "source": [
    "`inference_model`은 일반 텍스트 문장을 직접 인자로 받을 수 있다.\n",
    "예를 들어 \"That was an excellent movie, I loved it.\"라는 리뷰는\n",
    "긍정일 확률이 매우 높다고 예측된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32njxCoxZ8Oy",
    "outputId": "5ca607d5-330a-41ef-9fa9-85670da3ae22"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92.10 percent positive\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "raw_text_data = tf.convert_to_tensor([\n",
    "    [\"That was an excellent movie, I loved it.\"],\n",
    "])\n",
    "\n",
    "predictions = inference_model(raw_text_data)\n",
    "print(f\"{float(predictions[0] * 100):.2f} percent positive\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### 11.3.3 시퀀스 모델 기법"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 살펴본 대로 바이그램(bigrams) 등을 이용하여 단어들 사이의 순서 정보를 함께 활용하면 기본적으로 훈련된 모델의 성능이 향상된다.\n",
    "하지만 N-그램 등은 일종의 수동으로 진행하는 일종의 특성공학(feature engineering)이며,\n",
    "딥러닝은 그런 특성공학을 가능하면 진행하지 않는 방향으로 발전해왔다.\n",
    "여기서는 단언들의 순서를 그대로 함께 전달만 하고 나머지 특성은 모델 스스로 찾아내도록 하는 시퀀스 모델의 활용법을 살펴본다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**정수 시퀀스 데이터셋 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련셋의 모든 리뷰 문장을 정수들의 벡터로 변환한다.\n",
    "단, 리뷰 문장이 최대 600개의 단어만 포함하도록 한다. \n",
    "또한 사용되는 어휘는 빈도 기준 최대 2만개로 제한한다. \n",
    "\n",
    "- `max_length = 600`\n",
    "- `max_tokens = 20000`\n",
    "- `output_sequence_length=max_length`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_val_ds = val_ds.map(lambda x, y: (text_vectorization(x), y))\n",
    "int_test_ds = test_ds.map(lambda x, y: (text_vectorization(x), y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.\n",
    "`output_sequence_length=600`으로 지정하였기에 모든 문장은 단어를 최대 600개에서\n",
    "잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.\n",
    "물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다. \n",
    "또한 벡터에 사용된 정수는 2만보다 작은 값이며, \n",
    "이는 빈도가 가장 높은 2만개의 단어만을 대상(`max_tokens=20000`)으로 했기 때문이다.\n",
    "\n",
    "리뷰 문장의 길이를 600개의 단어로 제한한 이유는 리뷰가 평균적으로 233개의 단어를 사용하기 때문이다.\n",
    "그리고 600 단어 이상을 사용하는 리뷰는 전체의 5% 정도에 불과하다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inputs, targets in int_train_ds:\n",
    "    print(\"inputs.shape:\", inputs.shape)\n",
    "    print(\"inputs.dtype:\", inputs.dtype)\n",
    "    print(\"targets.shape:\", targets.shape)\n",
    "    print(\"targets.dtype:\", targets.dtype)\n",
    "    print(\"inputs[0]:\", inputs[0])\n",
    "    print(\"targets[0]:\", targets[0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**벡터 원-핫 인코딩**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래에서 소개하는 시퀀스 모델은 정수들의 벡터를 원-핫 인코딩된 벡터들의 시퀀스로 변환해서 사용한다.\n",
    "예를 들어 `[2, 1, 4, 0, 0]` 벡터를 원-핫 인코딩하면 아래 결과를 얻는다.\n",
    "단, 벡터에 사용된 정수는 0에서 4까지라고 가정한다. \n",
    "\n",
    "```python\n",
    "[[0, 0, 1, 0, 0],\n",
    " [0, 1, 0, 0, 0],\n",
    " [0, 0, 0, 0, 1],\n",
    " [1, 0, 0, 0, 0],\n",
    " [1, 0, 0, 0, 0]]\n",
    "```\n",
    "\n",
    "`tf.one_hot()` 함수가 원-핫 인코딩을 실행한다. \n",
    "에를 들어 위 결과는 아래 방식으로 얻어진다.\n",
    "\n",
    "```python\n",
    "tf.one_hot(indices=[2, 1, 4, 0, 0], depth=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**시퀀스 모델 예제 1**\n",
    "\n",
    "- 원-핫 인코딩 활용: 입력값을 바로 원-핫 인코딩함.\n",
    "- 양방향 LSTM 모델 활용\n",
    "    - 1차원 합성곱 신경망도 경우에 따라 유사한 성능을 발휘하지만 거의 사용되지 않음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 원-핫 인코딩\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)  # (600, 20000) 모양의 출력값 생성\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**모델 훈련**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련이 매우 느리다. \n",
    "이유는 입력 데이터가 너무 많은 특성을 갖기 때문이다. \n",
    "입력 데이터 하나의 모양과 특성 수는 다음과 같다.\n",
    "\n",
    "- 모양: `(600, 20000)`\n",
    "- 특성 수: `600 * 20,000 = 12,000,000`\n",
    "\n",
    "양방향 LSTM은 엄청난 양의 반복을 실행하기에 당연히 훈련 시간이 길어진다.\n",
    "게다가 훈련된 모델의 성능이 별로 좋지 않다.\n",
    "테스테셋에 대한 정확도가 87% 정도에 불과해서\n",
    "바이그램 모델보다 성능이 낮다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주의사항**: 모델 훈련과정을 한 번 보기만 하려면 `epochs=1`로 설정하는 것을 권장한다.\n",
    "책에서는 원래 `epochs=10`을 사용하였는데 컴퓨터 성능에 따라 몇 시간이 소요될 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=1, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**시퀀스 모델 예제 2: 단어 임베딩 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞서 보았듯이 원-핫 인코딩은 별로 적절하지 않다. \n",
    "원-핫 인코딩은 단어들의 순서는 잘 반영하지만 단어들 사이의 관계는 전혀 반형하지 못한다.\n",
    "\n",
    "- \"movie\"와 \"film\", \"비디오\"와 \"동영상\", \"강아지\"와 \"개\" 등이 사실상 동일하다는 사실\n",
    "- \"왕\"(남자)과 \"여왕\"(여자), \"boy\"와 \"girl\" 등의 성별 관계\n",
    "- \"king\"의 복수는 \"kings\" 등 문법 관계\n",
    "- \"고양이\"와 \"호랑이\"는 고양이과, \"개\"와 \"늑대\"는 개과, \"고양이\"와 \"개\"는 애완동물, \"늑대\"와 \"호랑이\"는 야생동물 등의 관계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 **단어 임베딩**(word embedding)은 단어들 사이의 관계를 모델 스스로 학습과정에서 찾도록 유도한다.\n",
    "단어 임베딩을 활용하는 방법은 일반적으로 다음 두 가지이다.\n",
    "\n",
    "- 모델 훈련과 동시에 단어 임베딩 학습도 진행하는 방식\n",
    "    - 자연어 종류와 모델 훈련 목적에 따라 기본적으로 서로 다른 단어 사이의 관계가 학습되어야 함.\n",
    "    - 예를 들어, 영화 리뷰 분석과 재판 판결문을 분석할 때 사용되는 단어 임베딩은 많이 다름.\n",
    "- 기존에 잘 훈련된 워드 임베딩 활용 방식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**케라스의 `Embedding` 층 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "케라스의 `Embedding` 층은 일종의 사전처럼 작동한다. \n",
    "하나의 문장에 해당하는 정수들의 벡터가 입력값으로 들어오면 단어들간에 존재하는 연관성을 (어떤식으로라도) 담은 \n",
    "부동소수점들의 벡터로 이루어진 시퀀스를 반환한다.\n",
    "아래 그림은 원-핫 인코딩 방식과 단어 임베딩 방식의 차이점을 보여준다. \n",
    "\n",
    "- 원-핫 인코딩: 특성 수가 너무 많음\n",
    "- 단어 임베딩: 단어들 사이의 연관성을 256개, 512개, 1024개 정도 수준에서 찾음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-02.png\" style=\"width:45%;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python(Manning MEAP)](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예를 들어, 600 단어로 이루어진 문장을 단어 임베딩할 때 무엇인지 모르지만 단어들 사이의 연관성을 256개 찾으라 하면\n",
    "`(600, 256)` 모양의 텐서(단어 벡터)를 생성한다. \n",
    "즉, 600개의 단어 각각이 총 2만개의 어휘 색인에 포함된 단어들과의 연관성을 256개 찾는다.\n",
    "\n",
    "방금 설명한 것을 아래 코드가 실행한다. \n",
    "\n",
    "```python\n",
    "layers.Embedding(input_dim=20000, output_dim=256)\n",
    "```\n",
    "\n",
    "아래 코드는 단어 임베딩을 모델 구성에 직접 활용하는 것을 보여준다.\n",
    "여전히 양방향 LSTM 층을 사용한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 단어 임베딩\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "훈련은 원-핫 인코딩 방식보다 훨씬 빠르게 이루어지며 성능은 87% 정도로 비슷하다. \n",
    "바이그램 모델보다 성능이 여전히 떨어지는 이유 중에 하나는 리뷰에 사용된 단어의 수를 600개로 제한하였기 때문이다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**패딩(padding)과 마스킹(masking)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "반면에 리뷰 문장의 길이가 600이 되지 않는 경우 나머지는 **패딩**(padding)에 의해 0으로 채워진다.\n",
    "하지만 이렇게 의미 없이 추가된 0이 훈련에 좋지 않은 영향을 미친다.\n",
    "따라서 모델이 패딩을 위해 차가된 0이 있다는 사실을 인식하도록 도와주는 **마스킹**(masking)\n",
    "기능을 활용하면 좋다.\n",
    "\n",
    "아래 코드는 마스킹을 활용하는 방식을 보여준다.\n",
    "\n",
    "- `mask_zero=True` 옵션: 마스킹 옵션 켜기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# 마스킹 활용 단어 임베딩\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 성능이 88% 정도로 살짝 향상된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**훈련된 단어 임베딩 활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 신경망에서 이미지넷 등의 대용량 데이터셋을 활용하여 잘 훈련된 모델을 재활용하였던 것처럼\n",
    "잘 구성된 대용량의 어휘 색인을 활용할 수 있다.\n",
    "여기서는 수 백만 개의 단어를 활용하여 생성된 2014년에 스탠포드 대학교의 연구자들이 생성한\n",
    "[GloVe(Gloval Vectors for Word Representation)](https://nlp.stanford.edu/projects/glove/) 단어 임베딩을 활용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "- GloVe 단어 임베딩 다운로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "    !unzip -q glove.6B.zip\n",
    "else: \n",
    "    try: \n",
    "        import wget, zipfile\n",
    "    except ModuleNotFoundError: \n",
    "        !pip install wget\n",
    "        \n",
    "    import wget, zipfile\n",
    "    wget.download('http://nlp.stanford.edu/data/glove.6B.zip')\n",
    "    with zipfile.ZipFile('glove.6B.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "- GloVe 워드 임베딩 파일 파싱"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "\n",
    "with open(path_to_glove_file) as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "- GloVe 단어 임베딩 행렬 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 임베딩 층 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GloVe 임베딩 활용 모델 구성 및 훈련"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "\n",
    "# GloVe 단어 임베딩 활용\n",
    "embedded = embedding_layer(inputs)\n",
    "\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "\n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "dlp11_part01_introduction",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad0f3c8a064f687cbf898a0868fd45ba1c7e928ac8a0404f7c241d812ddc1e76"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
