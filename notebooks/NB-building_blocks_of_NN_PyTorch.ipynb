{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "986bcd44",
   "metadata": {},
   "source": [
    "# Deep Learning with Python - PyTorch Version\n",
    "\n",
    "This notebook demonstrates the mathematical building blocks of neural networks using **PyTorch** instead of Keras/TensorFlow. It performs the same MNIST classification task as the original Keras version.\n",
    "\n",
    "**Key Differences from Keras Version:**\n",
    "- Uses PyTorch tensors and datasets\n",
    "- Manual training loop implementation\n",
    "- No high-level Keras abstractions\n",
    "- Direct gradient computation and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef11808d",
   "metadata": {},
   "source": [
    "## PyTorch 버전 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e23533d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.5.1\n",
      "TorchVision version: 0.20.1\n",
      "CUDA available: True\n"
     ]
    }
   ],
   "source": [
    "# PyTorch 버전 확인\n",
    "try:\n",
    "    import torch\n",
    "    import torchvision\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"TorchVision version: {torchvision.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "except ImportError:\n",
    "    print(\"❌ PyTorch가 설치되지 않았습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d56190",
   "metadata": {},
   "source": [
    "## Load and Explore the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cff881ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "dbf9e725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train images shape: (60000, 28, 28)\n",
      "Train labels shape: (60000,)\n",
      "Test images shape: (10000, 28, 28)\n",
      "Test labels shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "# MNIST 데이터셋 로드 (PyTorch 방식)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # transforms.Normalize((0.5,), (0.5,))  # [0,1] -> [-1,1] 정규화\n",
    "])\n",
    "\n",
    "# 훈련 및 테스트 데이터셋 로드\n",
    "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 데이터를 numpy 배열로 변환 (원본 Keras 코드와 동일한 형태로)\n",
    "train_images = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "test_images = test_dataset.data.numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "print(f\"Train images shape: {train_images.shape}\")\n",
    "print(f\"Train labels shape: {train_labels.shape}\")\n",
    "print(f\"Test images shape: {test_images.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2621ed05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training labels: [5 0 4 1 9 2 1 3 1 4 3 5 3 6 1 7 2 8 6 9]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Training labels: {train_labels[:20]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "28c12596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test labels: [7 2 1 0 4 1 4 9 5 9 0 6 9 0 1 5 9 7 3 4]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Test labels: {test_labels[:20]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f8403",
   "metadata": {},
   "source": [
    "## Build a Simple Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7c5e49ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch 신경망 모델 정의 (Keras Sequential과 동일한 구조)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        # 첫 번째 은닉층: 784 -> 512, ReLU 활성화\n",
    "        self.fc1 = nn.Linear(28 * 28, 512)\n",
    "        # 출력층: 512 -> 10, Softmax는 loss 함수에서 처리\n",
    "        self.fc2 = nn.Linear(512, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # 입력을 평면화 (28x28 -> 784)\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        # 첫 번째 층 + ReLU\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # 출력층 (softmax는 loss에서 처리)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d34712d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 구조:\n",
      "SimpleNN(\n",
      "  (fc1): Linear(in_features=784, out_features=512, bias=True)\n",
      "  (fc2): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# 모델 인스턴스 생성\n",
    "model = SimpleNN()\n",
    "print(\"모델 구조:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30c45795",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 모델 파라미터 상세 분석:\n",
      "==================================================\n",
      "fc1.weight      | Shape: torch.Size([512, 784]) | Parameters: 401,408\n",
      "fc1.bias        | Shape: torch.Size([512]) | Parameters:     512\n",
      "fc2.weight      | Shape: torch.Size([10, 512]) | Parameters:   5,120\n",
      "fc2.bias        | Shape: torch.Size([10]) | Parameters:      10\n",
      "==================================================\n",
      "총 파라미터 수: 407,050\n",
      "\n",
      "🔍 수동 계산 검증:\n",
      "fc1 가중치: 401,408 (784 × 512)\n",
      "fc1 편향:   512\n",
      "fc2 가중치: 5,120 (512 × 10)\n",
      "fc2 편향:   10\n",
      "총합:       407,050\n"
     ]
    }
   ],
   "source": [
    "# 파라미터 수 계산 (자세한 분석)\n",
    "print(\"📊 모델 파라미터 상세 분석:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "total_params = 0\n",
    "for name, param in model.named_parameters():\n",
    "    param_count = param.numel()\n",
    "    total_params += param_count\n",
    "    print(f\"{name:15} | Shape: {str(param.shape):15} | Parameters: {param_count:>7,}\")\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(f\"총 파라미터 수: {total_params:,}\")\n",
    "\n",
    "# 수동 계산으로 검증\n",
    "print(f\"\\n🔍 수동 계산 검증:\")\n",
    "fc1_weights = 28 * 28 * 512  # 784 * 512\n",
    "fc1_bias = 512\n",
    "fc2_weights = 512 * 10\n",
    "fc2_bias = 10\n",
    "\n",
    "print(f\"fc1 가중치: {fc1_weights:,} (784 × 512)\")\n",
    "print(f\"fc1 편향:   {fc1_bias:,}\")\n",
    "print(f\"fc2 가중치: {fc2_weights:,} (512 × 10)\")\n",
    "print(f\"fc2 편향:   {fc2_bias:,}\")\n",
    "print(f\"총합:       {fc1_weights + fc1_bias + fc2_weights + fc2_bias:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce208c4",
   "metadata": {},
   "source": [
    "## Compile the Model (PyTorch Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "698a3a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n",
      "Loss function: CrossEntropyLoss()\n",
      "Optimizer: Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# PyTorch에서 모델 구성 (Keras compile과 유사)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# 손실 함수: CrossEntropyLoss (sparse_categorical_crossentropy와 동일)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 옵티마이저: Adam\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Loss function: {criterion}\")\n",
    "print(f\"Optimizer: {optimizer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4be270",
   "metadata": {},
   "source": [
    "## Preprocess the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "be501892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "전처리된 훈련 데이터 형태: (60000, 784)\n",
      "전처리된 테스트 데이터 형태: (10000, 784)\n",
      "배치 크기: 128\n",
      "훈련 배치 수: 469\n",
      "테스트 배치 수: 79\n"
     ]
    }
   ],
   "source": [
    "# 데이터 전처리 (Keras 버전과 동일)\n",
    "# 이미지를 평면화하고 정규화\n",
    "train_images_processed = train_images.reshape((60000, 28 * 28)).astype(np.float32) / 255.0\n",
    "test_images_processed = test_images.reshape((10000, 28 * 28)).astype(np.float32) / 255.0\n",
    "\n",
    "# NumPy 배열을 PyTorch 텐서로 변환\n",
    "train_images_tensor = torch.FloatTensor(train_images_processed)\n",
    "train_labels_tensor = torch.LongTensor(train_labels)\n",
    "test_images_tensor = torch.FloatTensor(test_images_processed)\n",
    "test_labels_tensor = torch.LongTensor(test_labels)\n",
    "\n",
    "# 데이터셋과 데이터로더 생성\n",
    "train_dataset = TensorDataset(train_images_tensor, train_labels_tensor)\n",
    "test_dataset = TensorDataset(test_images_tensor, test_labels_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "print(f\"전처리된 훈련 데이터 형태: {train_images_processed.shape}\")\n",
    "print(f\"전처리된 테스트 데이터 형태: {test_images_processed.shape}\")\n",
    "print(f\"배치 크기: 128\")\n",
    "print(f\"훈련 배치 수: {len(train_loader)}\")\n",
    "print(f\"테스트 배치 수: {len(test_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a938083",
   "metadata": {},
   "source": [
    "## Train the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d939ebce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "신경망 훈련 시작...\n",
      "Epoch 1/5, Batch 0/469, Loss: 2.3080, Accuracy: 11.72%\n",
      "Epoch 1/5, Batch 0/469, Loss: 2.3080, Accuracy: 11.72%\n",
      "Epoch 1/5, Batch 100/469, Loss: 0.3546, Accuracy: 83.61%\n",
      "Epoch 1/5, Batch 100/469, Loss: 0.3546, Accuracy: 83.61%\n",
      "Epoch 1/5, Batch 200/469, Loss: 0.1627, Accuracy: 87.55%\n",
      "Epoch 1/5, Batch 200/469, Loss: 0.1627, Accuracy: 87.55%\n",
      "Epoch 1/5, Batch 300/469, Loss: 0.1602, Accuracy: 89.53%\n",
      "Epoch 1/5, Batch 300/469, Loss: 0.1602, Accuracy: 89.53%\n",
      "Epoch 1/5, Batch 400/469, Loss: 0.2466, Accuracy: 90.71%\n",
      "Epoch 1 완료 - Loss: 0.3193, Accuracy: 91.29%\n",
      "\n",
      "Epoch 2/5, Batch 0/469, Loss: 0.1815, Accuracy: 95.31%\n",
      "Epoch 1/5, Batch 400/469, Loss: 0.2466, Accuracy: 90.71%\n",
      "Epoch 1 완료 - Loss: 0.3193, Accuracy: 91.29%\n",
      "\n",
      "Epoch 2/5, Batch 0/469, Loss: 0.1815, Accuracy: 95.31%\n",
      "Epoch 2/5, Batch 100/469, Loss: 0.2646, Accuracy: 95.61%\n",
      "Epoch 2/5, Batch 100/469, Loss: 0.2646, Accuracy: 95.61%\n",
      "Epoch 2/5, Batch 200/469, Loss: 0.1179, Accuracy: 95.92%\n",
      "Epoch 2/5, Batch 200/469, Loss: 0.1179, Accuracy: 95.92%\n",
      "Epoch 2/5, Batch 300/469, Loss: 0.0447, Accuracy: 96.04%\n",
      "Epoch 2/5, Batch 300/469, Loss: 0.0447, Accuracy: 96.04%\n",
      "Epoch 2/5, Batch 400/469, Loss: 0.0228, Accuracy: 96.15%\n",
      "Epoch 2 완료 - Loss: 0.1303, Accuracy: 96.18%\n",
      "\n",
      "Epoch 3/5, Batch 0/469, Loss: 0.0853, Accuracy: 96.09%\n",
      "Epoch 2/5, Batch 400/469, Loss: 0.0228, Accuracy: 96.15%\n",
      "Epoch 2 완료 - Loss: 0.1303, Accuracy: 96.18%\n",
      "\n",
      "Epoch 3/5, Batch 0/469, Loss: 0.0853, Accuracy: 96.09%\n",
      "Epoch 3/5, Batch 100/469, Loss: 0.1615, Accuracy: 97.32%\n",
      "Epoch 3/5, Batch 100/469, Loss: 0.1615, Accuracy: 97.32%\n",
      "Epoch 3/5, Batch 200/469, Loss: 0.1314, Accuracy: 97.27%\n",
      "Epoch 3/5, Batch 200/469, Loss: 0.1314, Accuracy: 97.27%\n",
      "Epoch 3/5, Batch 300/469, Loss: 0.0650, Accuracy: 97.34%\n",
      "Epoch 3/5, Batch 300/469, Loss: 0.0650, Accuracy: 97.34%\n",
      "Epoch 3/5, Batch 400/469, Loss: 0.0647, Accuracy: 97.48%\n",
      "Epoch 3 완료 - Loss: 0.0854, Accuracy: 97.50%\n",
      "\n",
      "Epoch 4/5, Batch 0/469, Loss: 0.0522, Accuracy: 99.22%\n",
      "Epoch 3/5, Batch 400/469, Loss: 0.0647, Accuracy: 97.48%\n",
      "Epoch 3 완료 - Loss: 0.0854, Accuracy: 97.50%\n",
      "\n",
      "Epoch 4/5, Batch 0/469, Loss: 0.0522, Accuracy: 99.22%\n",
      "Epoch 4/5, Batch 100/469, Loss: 0.0439, Accuracy: 98.19%\n",
      "Epoch 4/5, Batch 100/469, Loss: 0.0439, Accuracy: 98.19%\n",
      "Epoch 4/5, Batch 200/469, Loss: 0.0461, Accuracy: 98.23%\n",
      "Epoch 4/5, Batch 200/469, Loss: 0.0461, Accuracy: 98.23%\n",
      "Epoch 4/5, Batch 300/469, Loss: 0.1186, Accuracy: 98.16%\n",
      "Epoch 4/5, Batch 300/469, Loss: 0.1186, Accuracy: 98.16%\n",
      "Epoch 4/5, Batch 400/469, Loss: 0.0497, Accuracy: 98.17%\n",
      "Epoch 4 완료 - Loss: 0.0621, Accuracy: 98.19%\n",
      "\n",
      "Epoch 5/5, Batch 0/469, Loss: 0.0271, Accuracy: 99.22%\n",
      "Epoch 4/5, Batch 400/469, Loss: 0.0497, Accuracy: 98.17%\n",
      "Epoch 4 완료 - Loss: 0.0621, Accuracy: 98.19%\n",
      "\n",
      "Epoch 5/5, Batch 0/469, Loss: 0.0271, Accuracy: 99.22%\n",
      "Epoch 5/5, Batch 100/469, Loss: 0.0162, Accuracy: 98.69%\n",
      "Epoch 5/5, Batch 100/469, Loss: 0.0162, Accuracy: 98.69%\n",
      "Epoch 5/5, Batch 200/469, Loss: 0.0872, Accuracy: 98.66%\n",
      "Epoch 5/5, Batch 200/469, Loss: 0.0872, Accuracy: 98.66%\n",
      "Epoch 5/5, Batch 300/469, Loss: 0.0304, Accuracy: 98.62%\n",
      "Epoch 5/5, Batch 300/469, Loss: 0.0304, Accuracy: 98.62%\n",
      "Epoch 5/5, Batch 400/469, Loss: 0.0656, Accuracy: 98.64%\n",
      "Epoch 5 완료 - Loss: 0.0465, Accuracy: 98.62%\n",
      "\n",
      "Epoch 5/5, Batch 400/469, Loss: 0.0656, Accuracy: 98.64%\n",
      "Epoch 5 완료 - Loss: 0.0465, Accuracy: 98.62%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 훈련 함수 정의\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=5):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        for batch_idx, (data, target) in enumerate(train_loader):\n",
    "            # 데이터를 디바이스로 이동\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            \n",
    "            # 그래디언트 초기화\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # 순전파\n",
    "            output = model(data)\n",
    "            \n",
    "            # 손실 계산\n",
    "            loss = criterion(output, target)\n",
    "            \n",
    "            # 역전파\n",
    "            loss.backward()\n",
    "            \n",
    "            # 가중치 업데이트\n",
    "            optimizer.step()\n",
    "            \n",
    "            # 통계 업데이트\n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(output.data, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "            \n",
    "            # 진행상황 출력 (매 100 배치마다)\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(f'Epoch {epoch+1}/{epochs}, Batch {batch_idx}/{len(train_loader)}, '\n",
    "                      f'Loss: {loss.item():.4f}, Accuracy: {100 * correct / total:.2f}%')\n",
    "        \n",
    "        # 에포크 종료 시 통계 출력\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f'Epoch {epoch+1} 완료 - Loss: {epoch_loss:.4f}, Accuracy: {epoch_acc:.2f}%\\n')\n",
    "\n",
    "# 모델 훈련 실행 (5 에포크)\n",
    "print(\"신경망 훈련 시작...\")\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5f0d25",
   "metadata": {},
   "source": [
    "## Make Predictions on Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0ba59ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 테스트 이미지의 예측 확률:\n",
      "[8.6196195e-08 1.4769905e-08 3.4010934e-06 8.9753339e-05 4.1802733e-10\n",
      " 2.1066103e-07 2.4664220e-12 9.9989688e-01 3.3224021e-06 6.2825375e-06]\n",
      "\n",
      "예측 확률의 합: 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 테스트 데이터에서 예측 수행 (Keras 버전과 동일한 동작)\n",
    "model.eval()  # 평가 모드로 설정\n",
    "\n",
    "with torch.no_grad():\n",
    "    # 첫 10개 테스트 이미지에 대한 예측\n",
    "    test_digits = test_images_tensor[0:10].to(device)\n",
    "    predictions = model(test_digits)\n",
    "    \n",
    "    # Softmax 적용하여 확률로 변환\n",
    "    predictions_prob = F.softmax(predictions, dim=1)\n",
    "    \n",
    "    print(\"첫 번째 테스트 이미지의 예측 확률:\")\n",
    "    print(predictions_prob[0].cpu().numpy())\n",
    "    print(f\"\\n예측 확률의 합: {predictions_prob[0].sum().item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b381ac",
   "metadata": {},
   "source": [
    "## Analyze Prediction Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1af8053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "예측된 클래스: 7\n",
      "클래스 7의 확률: 0.999897\n",
      "실제 레이블: 7\n",
      "예측 정확성: ✅ 맞음\n"
     ]
    }
   ],
   "source": [
    "# 예측 결과 분석 (Keras 버전과 동일)\n",
    "with torch.no_grad():\n",
    "    # 가장 높은 확률의 클래스 찾기 (argmax)\n",
    "    predicted_class = torch.argmax(predictions_prob[0]).item()\n",
    "    print(f\"예측된 클래스: {predicted_class}\")\n",
    "    \n",
    "    # 7번 클래스의 확률\n",
    "    prob_class_7 = predictions_prob[0][7].item()\n",
    "    print(f\"클래스 7의 확률: {prob_class_7:.6f}\")\n",
    "    \n",
    "    # 실제 레이블\n",
    "    actual_label = test_labels[0]\n",
    "    print(f\"실제 레이블: {actual_label}\")\n",
    "    \n",
    "    # 예측 정확도 확인\n",
    "    is_correct = predicted_class == actual_label\n",
    "    print(f\"예측 정확성: {'✅ 맞음' if is_correct else '❌ 틀림'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b19c202",
   "metadata": {},
   "source": [
    "## Evaluate Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "7b2d202c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "테스트 결과:\n",
      "테스트 손실 (test_loss): 0.0652\n",
      "테스트 정확도 (test_acc): 0.9798\n",
      "테스트 정확도 (백분율): 97.98%\n"
     ]
    }
   ],
   "source": [
    "# 전체 테스트 세트에 대한 모델 성능 평가\n",
    "def evaluate_model(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            # 손실 누적\n",
    "            test_loss += criterion(output, target).item()\n",
    "            \n",
    "            # 정확도 계산\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "    \n",
    "    # 평균 손실과 정확도 계산\n",
    "    avg_loss = test_loss / len(test_loader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    return avg_loss, accuracy\n",
    "\n",
    "# 모델 평가 실행\n",
    "test_loss, test_acc = evaluate_model(model, test_loader, criterion, device)\n",
    "\n",
    "print(f\"테스트 결과:\")\n",
    "print(f\"테스트 손실 (test_loss): {test_loss:.4f}\")\n",
    "print(f\"테스트 정확도 (test_acc): {test_acc:.4f}\")\n",
    "print(f\"테스트 정확도 (백분율): {test_acc * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19d18bb",
   "metadata": {},
   "source": [
    "## 📊 결과 비교\n",
    "\n",
    "### PyTorch vs Keras 구현 비교:\n",
    "\n",
    "| 항목 | Keras | PyTorch |\n",
    "|------|-------|----------|\n",
    "| **모델 정의** | `Sequential([layers.Dense(...)])` | `class SimpleNN(nn.Module)` |\n",
    "| **모델 컴파일** | `model.compile(optimizer, loss, metrics)` | 별도로 `criterion`, `optimizer` 정의 |\n",
    "| **훈련** | `model.fit(X, y, epochs, batch_size)` | 수동 훈련 루프 구현 |\n",
    "| **예측** | `model.predict(X)` | `model(X)` + `F.softmax()` |\n",
    "| **평가** | `model.evaluate(X, y)` | 수동 평가 함수 구현 |\n",
    "\n",
    "### 장단점:\n",
    "\n",
    "**Keras 장점:**\n",
    "- 간단하고 직관적인 API\n",
    "- 적은 코드로 빠른 프로토타이핑\n",
    "- 자동화된 훈련/평가 과정\n",
    "\n",
    "**PyTorch 장점:**\n",
    "- 더 세밀한 제어 가능\n",
    "- 훈련 과정의 모든 단계를 명시적으로 구현\n",
    "- 연구 및 실험에 더 적합\n",
    "- 동적 그래프 지원\n",
    "\n",
    "두 구현 모두 동일한 결과를 산출해야 하며, 약 97-98%의 테스트 정확도를 달성할 것으로 예상됩니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dlp3-tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
