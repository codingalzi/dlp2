{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "daf323e33b84"
   },
   "source": [
    "# 훈련 루프 상세"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**참고**\n",
    "\n",
    "[Writing a training loop from scratch](https://www.tensorflow.org/guide/keras/writing_a_training_loop_from_scratch)를 참고하였습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**소스코드**\n",
    "\n",
    "여기서 언급되는 코드를 [(구글 코랩) 훈련 루프](https://colab.research.google.com/github/codingalzi/dlp2/blob/master/notebooks/NB-training_loop_from_scratch.ipynb)에서 직접 실행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**슬라이드**\n",
    "\n",
    "본문 내용을 요약한 [슬라이드](https://github.com/codingalzi/dlp2/raw/master/slides/slides-training_loop_from_scratch.pdf)를 다운로드할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**주요 내용**\n",
    "\n",
    "신경망 모델의 `fit()` 메서드를 실행할 때 \n",
    "텐서플로우 내부에서 훈련 루프가 진행되는 과정을 상세히 살펴본다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 모델 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "설명을 위해 세 개의 `Dense` 층으로 구성된 순차 모델을 \n",
    "MNIST 데이터셋을 이용하여 훈련시킨다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "model = keras.Sequential([layers.Dense(256, activation=\"relu\"),\n",
    "                          layers.Dense(512, activation=\"relu\"),\n",
    "                          layers.Dense(10, activation=\"softmax\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 옵티마이저, 손실 함수, 평가지표 지정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련에 필요한 요소인 옵티마이저, 손실 함수, 평가지표를 지정하기 위해\n",
    "일반적으로 모델의 `compile()` 메서드를 다음과 같이 실행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "하지만 여기서는 모델의 `fit()` 메서드의 본체를 직접 구현하려 하기에\n",
    "`compile()` 메서드를 실행하는 대신 컴파일 과정에 요구되는 API를 직접 선언한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**옵티마이저 API 선언**\n",
    "\n",
    "아래코드는 모델 컴파일에 사용된 문자열 `'rmsprop'`에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.RMSprop(learning_rate=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**손실 함수 API 선언**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 0, 1, 2, 3 등 정수 형식의 타깃(레이블)을 예측하는 다중 클래스 분류 모델을 훈련시키는 경우\n",
    "보통 `SparseCategoricalCrossentropy` 클래스를 손실함수 API로 지정한다.\n",
    "아래코드는 모델 컴파일에 사용된 문자열 `'sparse_categorical_crossentropy'`에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**평가지표 API 선언**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 0, 1, 2, 3 등 정수 형식의 타깃(레이블)을 예측하는 다중 클래스 분류 모델을 훈련시키는 경우\n",
    "평가지표는 `SparseCategoricalAccuracy` 클래스를 이용한다.\n",
    "아래코드는 모델 컴파일에 사용된 문자열 `'accuracy'`에 해당한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc_metric = keras.metrics.SparseCategoricalAccuracy() # 훈련셋 대상 평가"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 데이터셋 준비"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련에 사용될 데이터를 준비한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**훈련셋과 테스트셋 지정**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "# 훈련셋과 테스트셋 가져오기\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train = np.reshape(x_train, (-1, 784))\n",
    "x_test = np.reshape(x_test, (-1, 784))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**배치 묶음 `Dataset` 객체 지정**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "훈련셋을 대상으로 배치 묶음으로 구성된 이터러블 객체인\n",
    "텐서플로우의 `tf.data.Dataset` 자료형 값을 선언한다.\n",
    "`tf.data.Dataset`는 머신러닝 모델 훈련에 사용되는 대용량 데이터의 효율적인 처리를\n",
    "지원하는 모음 자료형이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} 이터러블 객체\n",
    ":class: note\n",
    "\n",
    "이터러블<font size='2'>iterable</font> 객체는 `for` 반복문에 활용될 수 있는 값이다.\n",
    "보다 자세한 설명은 [이터러블, 이터레이터, 제너레이터](https://codingalzi.github.io/pybook/iterator_generator.html#sec-iterators)를 참고한다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "아래 이미지는 넘파이 어레이 훈련셋과 타깃셋을 조합하여 하나의 `tf.data.Dataset` 객체를 생성하는\n",
    "과정을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://github.com/codingalzi/dlp2/blob/master/jupyter-book/imgs/tf_dataset_1.png?raw=true\" style=\"width:550px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://igormintz.medium.com/introduction-to-tensorflows-data-dataset-api-83d49f300740\">Introduction to TensorFlow’s data.Dataset API</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`tf.data.Dataset`의 다양한 API(메서드)를 이용하면 적정한 배치를 모델 훈련에 제공하는 \n",
    "이터러블 객체를 생성할 수 있다.\n",
    "아래 이미지는 대용량 데이터를 다룰 때 유용한 `tf.data.Dataset`의 \n",
    "자식 클래스들을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://github.com/codingalzi/dlp2/blob/master/jupyter-book/imgs/tf_dataset_2.png?raw=true\" style=\"width:600px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://ekababisong.org/gcp-ml-seminar/tensorflow/\">TensorFlow</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "여기서는 배치 크기가 128인 묶음으로 구성된 `Dataset`객체를 \n",
    "생성하여 훈련 루프에 이용한다.\n",
    "아래 코드는 훈련용 `Dataset` 객체를 지정한다.\n",
    "\n",
    "- 훈련셋 어레이와 훈련셋 타깃 어레이로부터 `Dataset` 생성 후 배치로 묶은 `Dataset`으로 변환.\n",
    "- `shuffle()` 메서드를 이용하여 데이터 무작위 섞기 실행 후 배치 묶음 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "# 훈련용 Dataset 객체\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 훈련 루프"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "모델 훈련을 담당하는 훈련 루프는 다음과 같다.\n",
    "\n",
    "- 지정된 에포크 수만큼 에포크를 반복하는 `for` 반복문 실행\n",
    "- 각 에포크에 대해 배치 단위로 스텝을 진행하는 `for` 반복문 실행\n",
    "    - 각 배치에 대해 `GradientTape()` 영역 내에서 순전파 실행 후 손실값 계산\n",
    "    - 계산된 손실값을 대상으로 모델 가중치의 그래디언트 계산\n",
    "    - 옵티마이저를 사용하여 모델의 가중치 업데이트\n",
    "    - 해당 스텝의 평가지표 계산, 즉 훈련셋 평가지표 객체의 `update_state()` 메서드 호출\n",
    "- 평가지표를 확인하면서 에포크 마무리\n",
    "    - 매 스텝을 통해 업데이트된 평가지표의 최종 결과 확인, 즉, `result()` 메서드 호출\n",
    "    - 평가지표 초기화, 즉 `reset_state()` 메서드 호출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 2.1188\n",
      "  - 200번째 스텝 손실값: 1.9567\n",
      "  - 300번째 스텝 손실값: 0.9321\n",
      "  - 400번째 스텝 손실값: 0.6668\n",
      "  - 에포크 훈련 후 모델 정확도: 0.8797\n",
      "  - 에포크 훈련에 걸린 시간: 7.6723\n",
      "\n",
      "1 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.4685\n",
      "  - 200번째 스텝 손실값: 0.3514\n",
      "  - 300번째 스텝 손실값: 0.0946\n",
      "  - 400번째 스텝 손실값: 0.2356\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9433\n",
      "  - 에포크 훈련에 걸린 시간: 7.8152\n",
      "\n",
      "2 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.1057\n",
      "  - 200번째 스텝 손실값: 0.4352\n",
      "  - 300번째 스텝 손실값: 0.3588\n",
      "  - 400번째 스텝 손실값: 0.1679\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9583\n",
      "  - 에포크 훈련에 걸린 시간: 7.6305\n",
      "\n",
      "3 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.2093\n",
      "  - 200번째 스텝 손실값: 0.1426\n",
      "  - 300번째 스텝 손실값: 0.1657\n",
      "  - 400번째 스텝 손실값: 0.0974\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9656\n",
      "  - 에포크 훈련에 걸린 시간: 7.4949\n",
      "\n",
      "4 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.1324\n",
      "  - 200번째 스텝 손실값: 0.1083\n",
      "  - 300번째 스텝 손실값: 0.1788\n",
      "  - 400번째 스텝 손실값: 0.1302\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9717\n",
      "  - 에포크 훈련에 걸린 시간: 7.7237\n",
      "\n",
      "5 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.4711\n",
      "  - 200번째 스텝 손실값: 0.3974\n",
      "  - 300번째 스텝 손실값: 0.1506\n",
      "  - 400번째 스텝 손실값: 0.3549\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9743\n",
      "  - 에포크 훈련에 걸린 시간: 7.7204\n",
      "\n",
      "6 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.1762\n",
      "  - 200번째 스텝 손실값: 0.0775\n",
      "  - 300번째 스텝 손실값: 0.1324\n",
      "  - 400번째 스텝 손실값: 0.0979\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9772\n",
      "  - 에포크 훈련에 걸린 시간: 7.6753\n",
      "\n",
      "7 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.1643\n",
      "  - 200번째 스텝 손실값: 0.2421\n",
      "  - 300번째 스텝 손실값: 0.1368\n",
      "  - 400번째 스텝 손실값: 0.3365\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9789\n",
      "  - 에포크 훈련에 걸린 시간: 7.7692\n",
      "\n",
      "8 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.1190\n",
      "  - 200번째 스텝 손실값: 0.0344\n",
      "  - 300번째 스텝 손실값: 0.0426\n",
      "  - 400번째 스텝 손실값: 0.1442\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9804\n",
      "  - 에포크 훈련에 걸린 시간: 7.6562\n",
      "\n",
      "9 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.3695\n",
      "  - 200번째 스텝 손실값: 0.3615\n",
      "  - 300번째 스텝 손실값: 0.0614\n",
      "  - 400번째 스텝 손실값: 0.0680\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9831\n",
      "  - 에포크 훈련에 걸린 시간: 7.7118\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n{epoch} 번째 에포크 시작\")\n",
    "    # 에포크 시작시간\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 훈련 스텝: 배치 단위로 진행되는 훈련 루프\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        # 그레이디언트테이프: 손실함수 대상 그레이디언트 계산 준비\n",
    "        with tf.GradientTape() as tape:\n",
    "            logits = model(x_batch_train, training=True)\n",
    "            loss_value = loss_fn(y_batch_train, logits)\n",
    "        \n",
    "        # 그레이디언트 계산 후 가중치 업데이트\n",
    "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "\n",
    "        # 훈련세 대상 평가지표(정확도) 업데이트\n",
    "        train_acc_metric.update_state(y_batch_train, logits)\n",
    "\n",
    "        # 100번째 스텝마타 손실값 출력\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(f\"  - {step}번째 스텝 손실값: {loss_value:.4f}\")\n",
    "\n",
    "    ## 에포크 마무리 단계\n",
    "    \n",
    "    # 에포크의 정확도 출력\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"  - 에포크 훈련 후 모델 정확도: {train_acc:.4f}\")\n",
    "\n",
    "    # 평가지표 초기화: 에포크 단위로 정확도 계산을 새롭게 진행하기 위해.\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # 에포크 진행에 걸린시간 출력\n",
    "    print(f\"  - 에포크 훈련에 걸린 시간: {time.time() - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1c9a16c21790"
   },
   "source": [
    "## `@tf.function` 데코레이터"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "텐서플로우의 텐서를 다루는 함수에 `@tf.function` 데코레이터를 추가하면 모델 훈련 속도가 빨라질 수 있다.\n",
    "훈련 속도의 변화가 발생하는 이유는 여기서는 다루지 않는다.\n",
    "다만 전적으로 텐서플로우의 텐서 연산만 사용하는 함수에만 적용될 수 있음에 주의한다.\n",
    "\n",
    "아래 코드는 훈련 스텝을 담당하는 함수를 선언한 다음에\n",
    "`@tf.function` 데코레이터를 추가한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "fdacc2d48ade"
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    train_acc_metric.update_state(y, logits)\n",
    "    return loss_value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d552377968f1"
   },
   "source": [
    "아래 코드는 위 두 개의 함수를 이용하여 모델 훈련을 훨씬 빠르게 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.0693\n",
      "  - 200번째 스텝 손실값: 0.2337\n",
      "  - 300번째 스텝 손실값: 0.0785\n",
      "  - 400번째 스텝 손실값: 0.0332\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9845\n",
      "  - 에포크 훈련에 걸린 시간: 1.4537\n",
      "\n",
      "1 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.0816\n",
      "  - 200번째 스텝 손실값: 0.1980\n",
      "  - 300번째 스텝 손실값: 0.0417\n",
      "  - 400번째 스텝 손실값: 0.3837\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9844\n",
      "  - 에포크 훈련에 걸린 시간: 0.9751\n",
      "\n",
      "2 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.2355\n",
      "  - 200번째 스텝 손실값: 0.0504\n",
      "  - 300번째 스텝 손실값: 0.0007\n",
      "  - 400번째 스텝 손실값: 0.4392\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9865\n",
      "  - 에포크 훈련에 걸린 시간: 0.9317\n",
      "\n",
      "3 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.2081\n",
      "  - 200번째 스텝 손실값: 0.0340\n",
      "  - 300번째 스텝 손실값: 0.2235\n",
      "  - 400번째 스텝 손실값: 0.4308\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9873\n",
      "  - 에포크 훈련에 걸린 시간: 0.9103\n",
      "\n",
      "4 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.3127\n",
      "  - 200번째 스텝 손실값: 0.0000\n",
      "  - 300번째 스텝 손실값: 0.0349\n",
      "  - 400번째 스텝 손실값: 0.1000\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9873\n",
      "  - 에포크 훈련에 걸린 시간: 0.9877\n",
      "\n",
      "5 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.0252\n",
      "  - 200번째 스텝 손실값: 0.3112\n",
      "  - 300번째 스텝 손실값: 0.0605\n",
      "  - 400번째 스텝 손실값: 0.0480\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9888\n",
      "  - 에포크 훈련에 걸린 시간: 0.9603\n",
      "\n",
      "6 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.2532\n",
      "  - 200번째 스텝 손실값: 0.2600\n",
      "  - 300번째 스텝 손실값: 0.2482\n",
      "  - 400번째 스텝 손실값: 0.1113\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9900\n",
      "  - 에포크 훈련에 걸린 시간: 0.9107\n",
      "\n",
      "7 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.0262\n",
      "  - 200번째 스텝 손실값: 0.1634\n",
      "  - 300번째 스텝 손실값: 0.2633\n",
      "  - 400번째 스텝 손실값: 0.0062\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9900\n",
      "  - 에포크 훈련에 걸린 시간: 0.9000\n",
      "\n",
      "8 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.1859\n",
      "  - 200번째 스텝 손실값: 0.1314\n",
      "  - 300번째 스텝 손실값: 0.0584\n",
      "  - 400번째 스텝 손실값: 0.1928\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9907\n",
      "  - 에포크 훈련에 걸린 시간: 0.9393\n",
      "\n",
      "9 번째 에포크 시작\n",
      "  - 100번째 스텝 손실값: 0.0504\n",
      "  - 200번째 스텝 손실값: 0.4150\n",
      "  - 300번째 스텝 손실값: 0.1889\n",
      "  - 400번째 스텝 손실값: 0.0000\n",
      "  - 에포크 훈련 후 모델 정확도: 0.9905\n",
      "  - 에포크 훈련에 걸린 시간: 0.9409\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\n{epoch} 번째 에포크 시작\")\n",
    "    # 에포크 시작시간\n",
    "    start_time = time.time()\n",
    "\n",
    "    # 훈련 스텝: 배치 단위로 진행되는 훈련 루프\n",
    "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch_train, y_batch_train)\n",
    "\n",
    "        # 100번째 스텝마타 손실값 출력\n",
    "        if step % 100 == 0 and step > 0:\n",
    "            print(f\"  - {step}번째 스텝 손실값: {loss_value:.4f}\")\n",
    "\n",
    "    ## 에포크 마무리 단계\n",
    "    \n",
    "    # 에포크의 정확도 출력\n",
    "    train_acc = train_acc_metric.result()\n",
    "    print(f\"  - 에포크 훈련 후 모델 정확도: {train_acc:.4f}\")\n",
    "\n",
    "    # 평가지표 초기화: 에포크 단위로 정확도 계산을 새롭게 진행하기 위해.\n",
    "    train_acc_metric.reset_state()\n",
    "\n",
    "    # 에포크 진행에 걸린시간 출력\n",
    "    print(f\"  - 에포크 훈련에 걸린 시간: {time.time() - start_time:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ":::{admonition} 주의사항\n",
    ":class: note\n",
    "\n",
    "`@tf.function` 데코레이터를 추가한다 해서 모델 훈련 속도가 항상 빨라지는 것은 아님에 주의한다.\n",
    "어느 경우에 빠르고, 언제 그렇지 않은지에 대한 설명은 \n",
    "[Better performance with tf.function](https://www.tensorflow.org/guide/function)을\n",
    "참고한다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `fit()` 메서드 호출과 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "텐서플로우 모델의 `fit()` 메서드는 `@tf.function` 데코레이터를 적절하게 활용하기에 보다 빠르게 훈련을 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Sequential([layers.Dense(256, activation=\"relu\"),\n",
    "                          layers.Dense(512, activation=\"relu\"),\n",
    "                          layers.Dense(10, activation=\"softmax\")])\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9846 - loss: 0.1155\n",
      "Epoch 2/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9855 - loss: 0.0980\n",
      "Epoch 3/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 993us/step - accuracy: 0.9873 - loss: 0.0931\n",
      "Epoch 4/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 1ms/step - accuracy: 0.9883 - loss: 0.0839\n",
      "Epoch 5/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9889 - loss: 0.0881\n",
      "Epoch 6/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9888 - loss: 0.0944\n",
      "Epoch 7/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9903 - loss: 0.0775\n",
      "Epoch 8/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9898 - loss: 0.0982\n",
      "Epoch 9/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9904 - loss: 0.0950\n",
      "Epoch 10/10\n",
      "\u001b[1m469/469\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1ms/step - accuracy: 0.9916 - loss: 0.0816\n",
      "\n",
      "모델 훈련에 걸린 시간: 5.3690\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "model.fit(x_train, y_train, epochs=10, batch_size=128)\n",
    "\n",
    "# 모델 훈련에 걸린시간 출력\n",
    "print(f\"\\n모델 훈련에 걸린 시간: {time.time() - start_time:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 연습문제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [(실습) 훈련 루프 상세](https://colab.research.google.com/github/codingalzi/dlp2/blob/master/excs/exc-training_loop_from_scratch.ipynb)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
