
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>11. 자연어 처리 &#8212; Deep Learning with Python(2판)</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="12. 생성 모델" href="generative_dl.html" />
    <link rel="prev" title="10. 시계열 분석" href="dl_for_timeseries.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="index.html">
      
      
      
      <h1 class="site-logo" id="site-title">Deep Learning with Python(2판)</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="what_is_deep_learning.html">
   1. 딥러닝 소개
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="building_blocks_of_NN.html">
   2. 신경망 구성 요소
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="keras_and_tf.html">
   3. 케라스와 텐서플로우
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started_with_neural_networks.html">
   4. 신경망 활용 처음부터 끝까지: 분류와 회귀
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="fundamentals_of_ml.html">
   5. 머신러닝 모델 훈련 기법
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="unversal_workflow_of_ml.html">
   6. 머신러닝 작업 흐름 일반
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="working_with_keras.html">
   7. 케라스 모델 고급 활용법
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="computer_vision_intro.html">
   8. 컴퓨터 비전 기초: 합성곱 신경망
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="computer_vision_advanced.html">
   9. 고급 컴퓨터 비전
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="dl_for_timeseries.html">
   10. 시계열 분석
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   11. 자연어 처리
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="generative_dl.html">
   12. 생성 모델
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/codingalzi/dlp2/master?urlpath=tree/jupyter-book/dl_for_text.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/codingalzi/dlp2"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/codingalzi/dlp2/issues/new?title=Issue%20on%20page%20%2Fdl_for_text.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="_sources/dl_for_text.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   11.1. 소개
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   11.2. 텍스트 벡터화
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     11.2.1. 텍스트 표준화
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     11.2.2. 토큰화
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     11.2.3. 어휘 색인화
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#textvectorization">
     11.2.4.
     <code class="docutils literal notranslate">
      <span class="pre">
       TextVectorization
      </span>
     </code>
     층 활용
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   11.3. 문장 표현법: 집합 대 시퀀스
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imdb">
     11.3.1. IMDB 영화 리뷰 데이터 준비
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     11.3.2. 단어주머니 기법
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     11.3.3. 시퀀스 모델 기법
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>자연어 처리</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id2">
   11.1. 소개
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id3">
   11.2. 텍스트 벡터화
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id4">
     11.2.1. 텍스트 표준화
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id5">
     11.2.2. 토큰화
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id6">
     11.2.3. 어휘 색인화
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#textvectorization">
     11.2.4.
     <code class="docutils literal notranslate">
      <span class="pre">
       TextVectorization
      </span>
     </code>
     층 활용
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id7">
   11.3. 문장 표현법: 집합 대 시퀀스
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#imdb">
     11.3.1. IMDB 영화 리뷰 데이터 준비
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id8">
     11.3.2. 단어주머니 기법
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id9">
     11.3.3. 시퀀스 모델 기법
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="ch-nlp">
<span id="id1"></span><h1><span class="section-number">11. </span>자연어 처리<a class="headerlink" href="#ch-nlp" title="Permalink to this headline">#</a></h1>
<p><strong>감사의 글</strong></p>
<p>아래 내용은 프랑소와 숄레의
<a class="reference external" href="https://github.com/fchollet/deep-learning-with-python-notebooks">Deep Learning with Python(2판)</a>의
소스코드 내용을 참고해서 작성되었습니다.
자료를 공개한 저자에게 진심어린 감사를 전합니다.</p>
<p><strong>소스코드</strong></p>
<p>여기서 언급되는 코드를
<a class="reference external" href="https://colab.research.google.com/github/codingalzi/dlp2/blob/master/notebooks/NB-dl_for_text.ipynb">(구글 코랩) 자연어 처리</a>에서
직접 실행할 수 있다.</p>
<p><strong>주요 내용</strong></p>
<ul class="simple">
<li><p>자연어처리(Natural Language Processing) 소개</p>
<ul>
<li><p>단어주머니(bag-of-words) 모델</p></li>
<li><p>순차(sequence) 모델</p></li>
</ul>
</li>
<li><p>순차 모델 활용</p>
<ul>
<li><p>양방향 순환신경망(bidirectional LSTM) 적용</p></li>
</ul>
</li>
<li><p>트랜스포머(Transformer) 활용</p></li>
<li><p>시퀀스-투-시퀀스(seq2seq) 모델 활용</p></li>
</ul>
<section id="id2">
<h2><span class="section-number">11.1. </span>소개<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h2>
<p>파이썬, 자바, C, C++, C#, 자바스크립트 등 컴퓨터 프로그래밍언어와 구분하기 위해
일상에서 사용되는 한국어, 영어 등을 <strong>자연어</strong><font size='2'>natural language</font>라
부른다.</p>
<p>자연어의 특성상 정확한 분석을 위한 알고리즘을 구현하는 일은 사실상 매우 어렵다.
딥러닝 기법이 활용되기 이전까지는 적절한 규칙을 구성하여 자연어를 이해하려는
수 많은 시도가 있어왔지만 별로 성공적이지 않았다.</p>
<p>1990년대부터 인터넷으로부터 구해진 엄청난 양의 텍스트 데이터에 머신러닝 기법을
적용하기 시작했다. 단, 주요 목적이 <strong>언어의 이해</strong>가 아니라
아래 예제들처럼 입력 텍스트를 분석하여
<strong>통계적으로 유용한 정보를 예측</strong>하는 방향으로 수정되었다.</p>
<ul class="simple">
<li><p>텍스트 분류: “이 문장의 주제는?”</p></li>
<li><p>내용 필터링: “욕설이 포함되었나?”</p></li>
<li><p>감성 분석: “내용이 긍정이야 부정이야?”</p></li>
<li><p>언어 모델링: “이 문장에 이어 어떤 단어가 있어야 하지?”</p></li>
<li><p>번역: “이거를 한국어로 어떻게 말해?”</p></li>
<li><p>요약: “이 기사를 한 줄로 요약하면?”</p></li>
</ul>
<p>이와 같은 분석을 <strong>자연어 처리</strong><font size='2'>Natural Language Processing</font>이라 하며
단어, 문장, 문단 등에서 찾을 수 있는 패턴을  인식하려 시도한다.</p>
<p><strong>머신러닝 활용</strong></p>
<p>자연어처리를 위해 1990년대부터 시작된 머신러닝 활용의 변화과정은 다음과 같다.</p>
<ul class="simple">
<li><p>1990 - 2010년대 초반:
결정트리(decision trees), 로지스틱 회귀(logistic regression) 모델이 주로 활용됨.</p></li>
<li><p>2014-2015: LSTM 등 시퀀스 처리 알고리즘 활용 시작</p></li>
<li><p>2015-2017: (양방향) 순환신경망이 기본적으로 활용됨.</p></li>
<li><p>2017-2018: 트랜스포머<font size='2'> transformer</font> 모델이 최고의 성능 발휘하며,
많은 난제들을 해결함. 현재 가장 많이 활용되는 모델임.</p></li>
</ul>
</section>
<section id="id3">
<h2><span class="section-number">11.2. </span>텍스트 벡터화<a class="headerlink" href="#id3" title="Permalink to this headline">#</a></h2>
<p>딥러닝 모델은 텍스트 자체를 처리할 수 없다.
따라서 텍스트를 수치형 텐서로 변환하는 <strong>텍스트 벡터화</strong><font size='2'>text vectorization</font> 과정이 요구되며
보통 다음 세 단계를 따른다.</p>
<ol class="simple">
<li><p><strong>텍스트 표준화</strong><font size='2'>text standardization</font>: 소문자화, 마침표 제거 등등</p></li>
<li><p><strong>토큰화</strong><font size='2'>tokenization</font>: 기본 단위의 <strong>유닛</strong><font size='2'>units</font>으로 쪼개기.
문자, 단어, 단어 집합 등이 토큰으로 활용됨.</p></li>
<li><p><strong>어휘 색인화</strong><font size='2'>vocabulary indexing</font>: 토큰 각각을 하나의 수치형 벡터로 변환.</p></li>
</ol>
<p>아래 그림은 텍스트 벡터화의 기본적인 과정을 잘 보여준다.</p>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-01.png" style="width:60%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><section id="id4">
<h3><span class="section-number">11.2.1. </span>텍스트 표준화<a class="headerlink" href="#id4" title="Permalink to this headline">#</a></h3>
<p>다음 두 문장을 표준화를 통해 동일한 문장으로 변환해보자.</p>
<ul class="simple">
<li><p>“sunset came. i was staring at the Mexico sky. Isnt nature splendid??”</p></li>
<li><p>“Sunset came; I stared at the México sky. Isn’t nature splendid?”</p></li>
</ul>
<p>예를 들어 다음 표준화 기법을 사용할 수 있다.</p>
<ul class="simple">
<li><p>모두 소문자화</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.</span></code>, <code class="docutils literal notranslate"><span class="pre">;</span></code>, <code class="docutils literal notranslate"><span class="pre">?</span></code>, <code class="docutils literal notranslate"><span class="pre">'</span></code> 등 특수 기호 제거</p></li>
<li><p>특수 알파벳 변환: “é”를 “e”로, “æ”를 “ae”로 등등</p></li>
<li><p>동사/명사의 기본형 활용: “cats”를 “[cat]”로, “was staring”과 “stared”를 “[stare]”로 등등.</p></li>
</ul>
<p>그러면 위 두 문장 모두 아래 문장으로 변환된다.</p>
<ul class="simple">
<li><p>“sunset came i [stare] at the mexico sky isnt nature splendid”</p></li>
</ul>
<p>표준화 과정을 통해 어느 정도의 정보를 상실하게 되지만
학습해야할 내용을 줄여 일반화 성능이 보다 좋은 모델을 훈련시키는 장점이 있다.
하지만 분석 목적에 따라 표준화 기법은 경우에 따라 달라질 수 있음에 주의해야 한다.
예를 들어 인터뷰 기사의 경우 물음표(<code class="docutils literal notranslate"><span class="pre">?</span></code>)는 제거하면 안된다.</p>
</section>
<section id="id5">
<h3><span class="section-number">11.2.2. </span>토큰화<a class="headerlink" href="#id5" title="Permalink to this headline">#</a></h3>
<p>텍스트 표준화 이후 데이터 분석의 기본 단위인 토큰으로 쪼개야 한다.
보통 아래 세 가지 방식 중에 하나를 사용한다.</p>
<ul class="simple">
<li><p>단어 기준 토큰화(word-level tokenization)</p>
<ul>
<li><p>공백으로 구분된 단어들로 쪼개기.</p></li>
<li><p>경우에 따라 동사 어근과 어미를 구분하기도 함: “star+ing”, “call+ed” 등등</p></li>
</ul>
</li>
<li><p>N-그램 토큰화(N-gram tokenization)</p>
<ul>
<li><p>N-그램 토큰: 연속으로 위치한 N 개(이하)의 단어 묶음</p></li>
<li><p>예제: “the cat”, “he was” 등은 2-그램 토큰이다.</p></li>
</ul>
</li>
<li><p>문자 기준 토큰화</p>
<ul>
<li><p>하나의 문자를 하나의 토큰으로 지정.</p></li>
<li><p>문장 생성, 음성 인식 등에서 활용됨.</p></li>
</ul>
</li>
</ul>
<p>일반적으로 문자 기준 토큰화는 잘 사용되지 않는다.
여기서도 단어 기준과 N-그램 토큰화만 이용한다.</p>
<ul class="simple">
<li><p>단어 기준 토큰화: 단어들의 순서를 중요시하는 <strong>순차 모델</strong><font size='2'>sequence models</font>을 사용할 경우 활용</p></li>
<li><p>N-그램 토큰화: 단언들의 순서를 별로 상관하지 않는 <strong>단어 주머니</strong><font size='2'>bag-of-words</font>
모델을 사용할 경우 활용</p>
<ul>
<li><p>N-그램: 단어들 사이의 순서에 대한 지엽적 정보를 어느 정도 유지함.</p></li>
<li><p>일종의 특성 공학<font size='2'>feature engineering</font> 기법임.
트랜스포머 등 최신 기법에는 활용되지 않음.</p></li>
</ul>
</li>
</ul>
<p>단어주머니(bag-of-words)는 N-토큰으로 구성된 집합을 의미하며
<strong>N-그램 주머니</strong>라고도 불린다.
예를 들어 “the cat sat on the mat.” 문장에 대한
2-그램 집합과 3-그램 집합은 각각 다음과 같다.</p>
<ul>
<li><p>2-그램 집합</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span>
<span class="s2">&quot;sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;on the&quot;</span><span class="p">,</span> <span class="s2">&quot;the mat&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
<ul>
<li><p>3-그램 집합</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat sat&quot;</span><span class="p">,</span>
<span class="s2">&quot;sat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on the&quot;</span><span class="p">,</span>
<span class="s2">&quot;sat on the&quot;</span><span class="p">,</span> <span class="s2">&quot;the mat&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">,</span> <span class="s2">&quot;on the mat&quot;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="id6">
<h3><span class="section-number">11.2.3. </span>어휘 색인화<a class="headerlink" href="#id6" title="Permalink to this headline">#</a></h3>
<p>일반적으로 먼저 훈련셋에 포함된 모든 토큰들의 색인(인덱스)을 생성한 다음에
원-핫, 멀티-핫 인코딩 등의 방식을 사용하여 수치형 텐서로 변환한다.</p>
<p><a class="reference internal" href="getting_started_with_neural_networks.html#ch-getting-started-with-neural-networks"><span class="std std-numref">4장</span></a>에서 언급한 대로
보통 사용 빈도가 높은 2만 또는 3만 개의 단어만을 대상으로 어휘 색인화를 진행한다.
당시에 IMDB 영화 후기 데이터셋을 불러올 때
<code class="docutils literal notranslate"><span class="pre">num_words=10000</span></code>을 사용하여 사용 빈도수가 상위 1만 등 안에 들지 않는 단어는
영화 후기에서 삭제하도록 하였다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>
<span class="p">(</span><span class="n">train_data</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">),</span> <span class="p">(</span><span class="n">test_data</span><span class="p">,</span> <span class="n">test_labels</span><span class="p">)</span> <span class="o">=</span> <span class="n">imdb</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>케라스의 imdb 데이터셋은 이미 정수들의 시퀀스로 전처리가 되어 있다.
하지만 여기서는 원본 imdb 데이터셋을 대상으로 전처리를 직접 수행하는 단계부터 살펴볼 것이다.
이를 위해 먼저 0과 1은 아래 설명과 연관된 특별한 기능을 수행함을 기억해 둔다.</p>
<ul>
<li><p>OOV 인덱스 활용: 어휘 색인에 포함되지 않는 단어는 모두 1로 처리됨.
일반 문장으로 재번역되는 경우 “[UNK]” 으로 처리됨.</p>
<ul class="simple">
<li><p>OOV = Out Of Vocabulary (미동록 어휘)</p></li>
<li><p>UNK = Unknown (미확인)</p></li>
</ul>
</li>
<li><p>마스크 토큰<font size='2'>mask token</font>: 문장의 길이를 맞추기 위한 패딩으로 사용되는 0을
가리키며, 훈련 과정에서 무시됨.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">5</span><span class="p">,</span>  <span class="mi">7</span><span class="p">,</span> <span class="mi">124</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">89</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">8</span><span class="p">,</span> <span class="mi">34</span><span class="p">,</span>  <span class="mi">21</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span>  <span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
</li>
</ul>
</section>
<section id="textvectorization">
<h3><span class="section-number">11.2.4. </span><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층 활용<a class="headerlink" href="#textvectorization" title="Permalink to this headline">#</a></h3>
<p>케라스의 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층을 이용하여 텍스트 벡터화를 진행할 수 있다.</p>
<p>아래 코드는 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층 구성에 사용되는 주요 기본 설정을 보여준다.
표준화와 토큰화 방식을 임의로 지정해서 활용할 수도 있지만 여기서는 자세히 다루지 않는다.</p>
<ul class="simple">
<li><p>표준화: <code class="docutils literal notranslate"><span class="pre">standardize='lower_and_strip_punctuation'</span></code> (소문자화와 마침표 등 제거)</p></li>
<li><p>토큰화: <code class="docutils literal notranslate"><span class="pre">split='whitespace'</span></code> (단어 기준 쪼개기), <code class="docutils literal notranslate"><span class="pre">ngrams=None</span></code> (n-그램 미사용)</p></li>
<li><p>출력 모드: <code class="docutils literal notranslate"><span class="pre">output_mode=&quot;int&quot;</span></code> (정수 인코딩)</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">TextVectorization</span>

<span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">standardize</span><span class="o">=</span><span class="s1">&#39;lower_and_strip_punctuation&#39;</span><span class="p">,</span>
    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;whitespace&#39;</span><span class="p">,</span>
    <span class="n">ngrams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>예를 들어, 아래 데이터셋을 이용하여 텍스트 벡터화를 해보자.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span>
    <span class="s2">&quot;I write, erase, rewrite&quot;</span><span class="p">,</span>
    <span class="s2">&quot;Erase again, and then&quot;</span><span class="p">,</span>
    <span class="s2">&quot;A poppy blooms.&quot;</span><span class="p">,</span>
<span class="p">]</span>
</pre></div>
</div>
<p>어휘 색인화를 위해 먼저 <code class="docutils literal notranslate"><span class="pre">adapt()</span></code> 메서드를 이용하여 어휘 색인을 만든다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>생성된 어휘 색인은 다음과 같다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">text_vectorization</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocabulary</span>
<span class="go">[&#39;&#39;,</span>
<span class="go"> &#39;[UNK]&#39;,</span>
<span class="go"> &#39;erase&#39;,</span>
<span class="go"> &#39;write&#39;,</span>
<span class="go"> &#39;then&#39;,</span>
<span class="go"> &#39;rewrite&#39;,</span>
<span class="go"> &#39;poppy&#39;,</span>
<span class="go"> &#39;i&#39;,</span>
<span class="go"> &#39;blooms&#39;,</span>
<span class="go"> &#39;and&#39;,</span>
<span class="go"> &#39;again&#39;,</span>
<span class="go"> &#39;a&#39;]</span>
</pre></div>
</div>
<p>생성된 어휘 색인을 활용하여 새로운 문장을 벡터화 해보자.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">test_sentence</span> <span class="o">=</span> <span class="s2">&quot;I write, rewrite, and still rewrite again&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_sentence</span> <span class="o">=</span> <span class="n">text_vectorization</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_sentence</span><span class="p">)</span>
<span class="go">tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)</span>
</pre></div>
</div>
<p>벡터화된 텐서로부터 문장을 복원하면 표준화된 문장이 생성된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inverse_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoded_sentence</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">encoded_sentence</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">decoded_sentence</span><span class="p">)</span>
<span class="go">i write rewrite and [UNK] rewrite again</span>
</pre></div>
</div>
<div class="info admonition">
<p class="admonition-title"><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층 사용법</p>
<p><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층은 GPU 또는 TPU에서 지원되지 않는다.
따라서 모델 구성에 직접 사용하는 방식은 모델의 훈련을
늦출 수 있기에 권장되지 않는다.
여기서는 대신에 데이터셋 전처리를 모델 구성과 독립적으로 처리하는 방식을 이용한다.</p>
<p>하지만 훈련이 완성된 모델을 실전에 배치할 경우 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층을
완성된 모델에 추가해서 사용하는 게 좋다.</p>
</div>
</section>
</section>
<section id="id7">
<h2><span class="section-number">11.3. </span>문장 표현법: 집합 대 시퀀스<a class="headerlink" href="#id7" title="Permalink to this headline">#</a></h2>
<p>앞서 언급한 대로 자연어처리 모델에 따라 단어 모음을 다루는 방식이 다르다.</p>
<ul class="simple">
<li><p>단어주머니<font size='2'>bag-of-words</font> 모델</p>
<ul>
<li><p>단어들의 순서를 무시. 단어 모음을 단어들의 집합으로 다룸.</p></li>
<li><p>2015년 이전까지 주로 사용됨.</p></li>
</ul>
</li>
<li><p>시퀀스<font size='2'>sequence</font> 모델</p>
<ul>
<li><p>RNN: 단어들의 순서를 시계열 데이터의 스텝처럼 간주. 2015-2016에 주로 사용됨.</p></li>
<li><p>트랜스포머<font size='2'>Transformer</font> 아키텍처.
기본적으로 순서를 무시하지만 단어 위치를 학습할 수 있는 능력을 가짐.
2017년 이후 많이 활용됨.</p></li>
</ul>
</li>
</ul>
<p>여기서는 IMDB 영화 리뷰 데이터를 이용하여 두 모델 방식의
활용법과 차이점을 소개한다.</p>
<section id="imdb">
<h3><span class="section-number">11.3.1. </span>IMDB 영화 리뷰 데이터 준비<a class="headerlink" href="#imdb" title="Permalink to this headline">#</a></h3>
<p>IMDB 데이터셋을 직접 다운로드하여 전처리하는 과정을 자세히 살펴본다.</p>
<p><strong>준비 과정 1: 데이터셋 다운로드 압축 풀기</strong></p>
<p><a class="reference external" href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">aclIMDB_v1.tar</a> 파일을
다운로드 한 후에 압축을 풀면 아래 구조의 디렉토리가 생성된다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aclImdb</span><span class="o">/</span>
<span class="o">...</span><span class="n">train</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
<span class="o">...</span><span class="n">test</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code>의 <code class="docutils literal notranslate"><span class="pre">pos</span></code>와 <code class="docutils literal notranslate"><span class="pre">neg</span></code> 서브디렉토리에 각각 12,500개의 긍정과 부정 리뷰가
포함되어 있다. <code class="docutils literal notranslate"><span class="pre">aclImdb/train/unsup</span></code> 서브디렉토리는 필요 없기에 삭제한다.</p>
<p>긍정 리뷰 하나의 내용을 살펴보자.
모델 구성 이전에 훈련 데이터셋을 살펴 보고
모델에 대한 직관을 갖는 과정이 항상 필요하다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">I</span> <span class="n">first</span> <span class="n">saw</span> <span class="n">this</span> <span class="n">back</span> <span class="ow">in</span> <span class="n">the</span> <span class="n">early</span> <span class="mi">90</span><span class="n">s</span> <span class="n">on</span> <span class="n">UK</span> <span class="n">TV</span><span class="p">,</span> <span class="n">i</span> <span class="n">did</span> <span class="n">like</span> <span class="n">it</span> <span class="n">then</span> <span class="n">but</span> <span class="n">i</span> <span class="n">missed</span> <span class="n">the</span> <span class="n">chance</span> <span class="n">to</span> <span class="n">tape</span> <span class="n">it</span><span class="p">,</span> <span class="n">many</span> <span class="n">years</span> <span class="n">passed</span> <span class="n">but</span> <span class="n">the</span> <span class="n">film</span> <span class="n">always</span> <span class="n">stuck</span> <span class="k">with</span> <span class="n">me</span> <span class="ow">and</span> <span class="n">i</span> <span class="n">lost</span> <span class="n">hope</span> <span class="n">of</span> <span class="n">seeing</span> <span class="n">it</span> <span class="n">TV</span> <span class="n">again</span><span class="p">,</span> <span class="n">the</span> <span class="n">main</span> <span class="n">thing</span> <span class="n">that</span> <span class="n">stuck</span> <span class="k">with</span> <span class="n">me</span> <span class="n">was</span> <span class="n">the</span> <span class="n">end</span><span class="p">,</span> <span class="n">the</span> <span class="n">hole</span> <span class="n">castle</span> <span class="n">part</span> <span class="n">really</span> <span class="n">touched</span> <span class="n">me</span><span class="p">,</span> <span class="n">its</span> <span class="n">easy</span> <span class="n">to</span> <span class="n">watch</span><span class="p">,</span> <span class="n">has</span> <span class="n">a</span> <span class="n">great</span> <span class="n">story</span><span class="p">,</span> <span class="n">great</span> <span class="n">music</span><span class="p">,</span> <span class="n">the</span> <span class="nb">list</span> <span class="n">goes</span> <span class="n">on</span> <span class="ow">and</span> <span class="n">on</span><span class="p">,</span> <span class="n">its</span> <span class="n">OK</span> <span class="n">me</span> <span class="n">saying</span> <span class="n">how</span> <span class="n">good</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">but</span> <span class="n">everyone</span> <span class="n">will</span> <span class="n">take</span> <span class="n">there</span> <span class="n">own</span> <span class="n">best</span> <span class="n">bits</span> <span class="n">away</span> <span class="k">with</span> <span class="n">them</span> <span class="n">once</span> <span class="n">they</span> <span class="n">have</span> <span class="n">seen</span> <span class="n">it</span><span class="p">,</span> <span class="n">yes</span> <span class="n">the</span> <span class="n">animation</span> <span class="ow">is</span> <span class="n">top</span> <span class="n">notch</span> <span class="ow">and</span> <span class="n">beautiful</span> <span class="n">to</span> <span class="n">watch</span><span class="p">,</span> <span class="n">it</span> <span class="n">does</span> <span class="n">show</span> <span class="n">its</span> <span class="n">age</span> <span class="ow">in</span> <span class="n">a</span> <span class="n">very</span> <span class="n">few</span> <span class="n">parts</span> <span class="n">but</span> <span class="n">that</span> <span class="n">has</span> <span class="n">now</span> <span class="n">become</span> <span class="n">part</span> <span class="n">of</span> <span class="n">it</span> <span class="n">beauty</span><span class="p">,</span> <span class="n">i</span> <span class="n">am</span> <span class="n">so</span> <span class="n">glad</span> <span class="n">it</span> <span class="n">has</span> <span class="n">came</span> <span class="n">out</span> <span class="n">on</span> <span class="n">DVD</span> <span class="k">as</span> <span class="n">it</span> <span class="ow">is</span> <span class="n">one</span> <span class="n">of</span> <span class="n">my</span> <span class="n">top</span> <span class="mi">10</span> <span class="n">films</span> <span class="n">of</span> <span class="nb">all</span> <span class="n">time</span><span class="o">.</span> <span class="n">Buy</span> <span class="n">it</span> <span class="ow">or</span> <span class="n">rent</span> <span class="n">it</span> <span class="n">just</span> <span class="n">see</span> <span class="n">it</span><span class="p">,</span> <span class="n">best</span> <span class="n">viewing</span> <span class="ow">is</span> <span class="n">at</span> <span class="n">night</span> <span class="n">alone</span> <span class="k">with</span> <span class="n">drink</span> <span class="ow">and</span> <span class="n">food</span> <span class="ow">in</span> <span class="n">reach</span> <span class="n">so</span> <span class="n">you</span> <span class="n">don</span><span class="s1">&#39;t have to stop the film.&lt;br /&gt;&lt;br /&gt;Enjoy</span>
</pre></div>
</div>
<p><strong>준비 과정 2: 검증셋 준비</strong></p>
<p>훈련셋의 20%를 검증셋으로 떼어낸다.
이를 위해 <code class="docutils literal notranslate"><span class="pre">aclImdb/val</span></code> 디렉토리를 생성한 후에
긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다.</p>
<p><strong>준비 과정 3: 텐서 데이터셋 준비</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">text_dataset_from_directory()</span></code> 함수를 이용하여
훈련셋, 검증셋, 테스트셋을 준비한다.
자료형은 모두 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>이며, 배치 크기는 32를 사용한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s2">&quot;aclImdb/train&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
    <span class="p">)</span>

<span class="n">val_ds</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s2">&quot;aclImdb/val&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
    <span class="p">)</span>

<span class="n">test_ds</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s2">&quot;aclImdb/test&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span>
    <span class="p">)</span>
</pre></div>
</div>
<p>각 데이터셋은 배치로 구분되며
배치의 각 입력 데이터 샘플은 텐서플로우의 문자열 자료형인 <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> 텐서이고,
타깃은 0 또는 1의 <code class="docutils literal notranslate"><span class="pre">int32</span></code> 텐서로 지정된다.
0은 부정을, 1은 긍정을 나타낸다.</p>
<p>배치의 크기는 32이며, 예를 들어, 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>
<span class="gp">... </span>    <span class="c1"># 예제: 첫째 배치의 첫째 리뷰</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs[0]:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;targets[0]:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>    
<span class="gp">... </span>    <span class="k">break</span>
<span class="go">inputs[0]: tf.Tensor(b&#39;The film begins with a bunch of kids in reform school and focuses on a kid named \&#39;Gabe\&#39;, who has apparently worked hard to earn his parole. Gabe and his sister move to a new neighborhood to make a fresh start and soon Gabe meets up with the Dead End Kids. The Kids in this film are little punks, but they are much less antisocial than they\&#39;d been in other previous films and down deep, they are well-meaning punks. However, in this neighborhood there are also some criminals who are perpetrating insurance fraud through arson and see Gabe as a convenient scapegoat--after all, he\&#39;d been to reform school and no one would believe he was innocent once he was framed. So, when Gabe is about ready to be sent back to &quot;The Big House&quot;, it\&#39;s up to the rest of the gang to save him and expose the real crooks.&lt;br /&gt;&lt;br /&gt;The &quot;Dead End Kids&quot; appeared in several Warner Brothers films in the late 1930s and the films were generally very good (particularly ANGELS WITH DIRTY FACES). However, after the boys\&#39; contracts expired, they went on to Monogram Studios and the films, to put it charitably, were very weak and formulaic--with Huntz Hall and Leo Gorcey being pretty much the whole show and the group being renamed &quot;The Bowery Boys&quot;. Because ANGELS WASH THEIR FACES had the excellent writing and production values AND Hall and Gorcey were not constantly mugging for the camera, it\&#39;s a pretty good film--and almost earns a score of 7 (it\&#39;s REAL close). In fact, while this isn\&#39;t a great film aesthetically, it\&#39;s sure a lot of fun to watch, so I will give it a 7! Sure, it was a tad hokey-particularly towards the end when the kids take the law into their own hands and Reagan ignores the Bill of Rights--but it was also quite entertaining. The Dead End Kids are doing their best performances and Ronald Reagan and Ann Sheridan provided excellent support. Sure, this part of the film was illogical and impossible but somehow it was still funny and rather charming--so if you can suspend disbelief, it works well.&#39;, shape=(), dtype=string)</span>
<span class="go">targets[0]: tf.Tensor(1, shape=(), dtype=int32)</span>
</pre></div>
</div>
</section>
<section id="id8">
<h3><span class="section-number">11.3.2. </span>단어주머니 기법<a class="headerlink" href="#id8" title="Permalink to this headline">#</a></h3>
<p>단어주머니에 채울 토큰으로 어떤 N-그램을 사용할지 먼저 지정해야 한다.</p>
<ul class="simple">
<li><p>유니그램(unigrams): 하나의 단어가 하나의 토큰</p></li>
<li><p>N-그램(N-grams): 최대 N 개의 이어지는 단어로 이루어진 문구가 하나의 토큰</p></li>
</ul>
<p><strong>방식 1: 유니그램 멀티-핫 인코딩</strong></p>
<p>예를 들어, “the cat sat on the mat” 문장을 유니그램으로 처리하면 다음
단어주머니가 생성된다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span> <span class="s2">&quot;the&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p>그런 다음 어휘 색인을 이용하여 생성된 단어 주머니를 멀티-핫 인코딩을 이용하여 1차원 이진 텐서,
즉 0과 1로만 구성된 벡터로 변환한다.</p>
<p>유니그램 멀티-핫 인코딩을 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 클래스를 이용하면 간단하게 처리할 수 있다.</p>
<ul class="simple">
<li><p>`max_tokens=20000” 옵션: 빈도가 20,000 등 안에 드는 단어만 멀티-핫 인코딩 적용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_mode=&quot;multi_hot&quot;</span></code> 옵션: 유니그램을 멀티-핫 인코딩하기</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">TextVectorization</span>

<span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s2">&quot;multi_hot&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>훈련셋의 후기에 포함된 단어들을 이용하여 어휘 색인을 생성한다.
먼저 훈련섹에 포함된 후기만 따로 떼어낸다. 이 데이터셋은 이후에도 계속해서 사용된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 어휘 색인 생성 대상 훈련셋 후기 문장 데이터셋</span>
<span class="n">text_only_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
</pre></div>
</div>
<p>이제 유니그램으로 인코딩에 사용될 어휘 색인을 생성한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 어휘색인 생성</span>
<span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">text_only_train_ds</span><span class="p">)</span>
</pre></div>
</div>
<p>생성된 어휘색인을 이용하여 훈련셋, 검증셋, 테스트셋 모두 벡터화한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">binary_1gram_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">binary_1gram_val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">binary_1gram_test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.
<code class="docutils literal notranslate"><span class="pre">max_tokens=20000</span></code>으로 지정하였기에 모든 문장은 길이가 2만인 벡터로 변환되었다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="o">...</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20000</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
<span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">shape</span><span class="o">=</span><span class="p">(),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int32</span><span class="p">)</span>
</pre></div>
</div>
<p><em>밀집 모델 활용</em></p>
<p>단어 주머니 형식으로 전처리된 데이터를 이용하는 모델은 기본적으로
밀집 모델을 사용한다.
이유는 토큰들 사이의 순서가 중요하지 않기 때문이다.</p>
<p>아래 <code class="docutils literal notranslate"><span class="pre">get_model()</span></code> 함수는 앞으로 자주 사용할 매운 단순하며 컴파일된 밀집 모델을 반환한다.
모델의 출력값은 긍정일 확률이며,
최상위 층의 활성화 함수로 <code class="docutils literal notranslate"><span class="pre">sigmoid</span></code>를 사용한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">get_model</span><span class="p">(</span><span class="n">max_tokens</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">hidden_dim</span><span class="o">=</span><span class="mi">16</span><span class="p">):</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="n">max_tokens</span><span class="p">,))</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">hidden_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 긍정일 확률 계산</span>
    
    <span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
    
    <span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">,</span>
                  <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span>
                  <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
    
    <span class="k">return</span> <span class="n">model</span>
</pre></div>
</div>
<p>밀집 모델 훈련과정은 특별한 게 없다.
훈련 후 테스트셋에 대한 정확도가 89% 보다 조금 낮게 나온다.
최고 성능의 모델이 테스트셋에 대해 95% 정도 정확도를 내는 것보다는 낮지만
무작위로 찍는 모델보다는 훨씬 좋은 모델이다.</p>
<p><strong>방식 2: 바이그램 멀티-핫 인코딩</strong></p>
<p>바이그램(2-grams)을 유니그램 대신 이용해보자.
예를 들어 “the cat sat on the mat” 문장을 바이그램으로 처리하면 다음
단어주머니가 생성된다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span>
 <span class="s2">&quot;sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;on the&quot;</span><span class="p">,</span> <span class="s2">&quot;the mat&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">}</span>
</pre></div>
</div>
<p>바이그램 멀티-핫 인코딩을 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 클래스를 이용하면 간단하게 처리할 수 있다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">ngrams=2</span></code> 옵션 활용</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">ngrams</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s2">&quot;multi_hot&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>어휘색인 생성과 훈련셋, 검증셋, 테스트셋의 벡터화 과정은 동일하다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">text_only_train_ds</span><span class="p">)</span>

<span class="n">binary_2gram_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">binary_2gram_val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">binary_2gram_test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>이전과 동일한 밀집 모델을 훈련시킨 후 테스트셋에 대한 정확도가 90%를 조금 웃돌 정도로 많이 향상되었다.</p>
<p><strong>방식 3: 바이그램 TF-IDF 인코딩</strong></p>
<p>N-그램을 벡터화할 때 사용 빈도를 함께 저장하는 방식을 사용할 수 있다.
단어의 사용 빈도가 아무래도 문장 평가에 중요한 역할을 수행할 것이기 때문이다.
아래 코드에서처럼 <code class="docutils literal notranslate"><span class="pre">output_mode=&quot;count&quot;</span></code> 옵션을 사용하면 된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">ngrams</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s2">&quot;count&quot;</span>
<span class="p">)</span>
</pre></div>
</div>
<p>그런데 이렇게 하면 “the”, “a”, “is”, “are” 등의 사용 빈도는 매우 높은 반면에
“Chollet” 등의 단어는 빈도가 거의 0에 가깝게 나온다.
또한 생성된 벡터의 대부분은 0으로 채워질 것이다.
<code class="docutils literal notranslate"><span class="pre">max_tokens=20000</span></code>을 사용한 반면에 하나의 문장엔 많아야 몇 십개 정도의 단어만 사용되었기 때문이다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]:</span> <span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">([</span><span class="mf">1.</span> <span class="mf">1.</span> <span class="mf">1.</span> <span class="o">...</span> <span class="mf">0.</span> <span class="mf">0.</span> <span class="mf">0.</span><span class="p">],</span> <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20000</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p>이 점을 고려해서 사용 빈도를 정규화한다.
평균을 원점으로 만들지는 않고 TF-IDF 값으로 나누기만 실행한다.
이유는 평균을 옮기면 벡터의 대부분의 값이 0이 아니게 되어
훈련에 보다 많은 계산이 요구되기 때문이다.</p>
<p><strong>TF-IDF</strong>의 의미는 다음과 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">TF</span></code>(Term Frequency)</p>
<ul>
<li><p>하나의 문장에서 사용되는 단어의 빈도</p></li>
<li><p>높을 수록 중요</p></li>
<li><p>예를 들어, 하나의 리뷰에 “terrible” 이 많이 사용되었다면
해당 리뷰는 부정일 가능성 높음.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">IDF</span></code>(Inverse Document Frequency)</p>
<ul>
<li><p>데이터셋 전체 문장에서 사용된 단어의 빈도</p></li>
<li><p>낮을 수록 중요.</p></li>
<li><p>“the”, “a”, “is” 등의 <code class="docutils literal notranslate"><span class="pre">IDF</span></code> 값은 높지만 별로 중요하지 않음.</p></li>
</ul>
</li>
<li><p><code class="docutils literal notranslate"><span class="pre">TF-IDF</span> <span class="pre">=</span> <span class="pre">TF</span> <span class="pre">/</span> <span class="pre">IDF</span></code></p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">output_mode=&quot;tf_idf&quot;</span></code> 옵션을 사용하면 TF-IDF 인코딩을 지원한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">ngrams</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s2">&quot;tf_idf&quot;</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<p>어휘색인 생성과 훈련셋, 검증셋, 테스트셋의 벡터화 과정은 동일하다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">text_only_train_ds</span><span class="p">)</span>

<span class="n">tfidf_2gram_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">tfidf_2gram_val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">tfidf_2gram_test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>훈련 후 테스트셋에 대한 정확도가 다시 89% 아래로 내려간다.
여기서는 별 도움이 되지 않았지만 많은 텍스트 분류 모델에서는 1% 정도의 성능 향상을 가져온다.</p>
<div class="info admonition">
<p class="admonition-title">문자열 벡터화 전처리를 함께 처리하는 모델 내보내기</p>
<p>훈련된 모델을 실전에 배치하려면 텍스트 벡터화도 모델과 함께 내보내야 한다.
이를 위해 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층의 결과를 재활용만 하면 된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;string&quot;</span><span class="p">)</span>
<span class="c1"># 텍스트 벡터화 추가</span>
<span class="n">processed_inputs</span> <span class="o">=</span> <span class="n">text_vectorization</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
<span class="c1"># 훈련된 모델에 적용</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">processed_inputs</span><span class="p">)</span>

<span class="c1"># 최종 모델</span>
<span class="n">inference_model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">inference_model</span></code>은 일반 텍스트 문장을 직접 인자로 받을 수 있다.
예를 들어 “That was an excellent movie, I loved it.”라는 리뷰는
긍정일 확률이 매우 높다고 예측된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">raw_text_data</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">convert_to_tensor</span><span class="p">([</span>
    <span class="p">[</span><span class="s2">&quot;That was an excellent movie, I loved it.&quot;</span><span class="p">],</span>
<span class="p">])</span>

<span class="n">predictions</span> <span class="o">=</span> <span class="n">inference_model</span><span class="p">(</span><span class="n">raw_text_data</span><span class="p">)</span>
</pre></div>
</div>
</div>
</section>
<section id="id9">
<h3><span class="section-number">11.3.3. </span>시퀀스 모델 기법<a class="headerlink" href="#id9" title="Permalink to this headline">#</a></h3>
<p>앞서 살펴본 대로 바이그램(bigrams) 등을 이용하여 단어들 사이의 순서 정보를 함께 활용하면 기본적으로 훈련된 모델의 성능이 향상된다.
하지만 N-그램 등은 일종의 수동으로 진행하는 일종의 특성공학(feature engineering)이며,
딥러닝은 그런 특성공학을 가능하면 진행하지 않는 방향으로 발전해왔다.
여기서는 단언들의 순서를 그대로 함께 전달만 하고 나머지 특성은 모델 스스로 찾아내도록 하는 시퀀스 모델의 활용법을 살펴본다.</p>
<p><strong>정수 시퀀스 데이터셋 준비</strong></p>
<p>훈련셋의 모든 리뷰 문장을 정수들의 벡터로 변환한다.
단, 리뷰 문장이 최대 600개의 단어만 포함하도록 한다.
또한 사용되는 어휘는 빈도 기준 최대 2만개로 제한한다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">max_length</span> <span class="pre">=</span> <span class="pre">600</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">max_tokens</span> <span class="pre">=</span> <span class="pre">20000</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_sequence_length=max_length</span></code></p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">600</span>
<span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">20000</span>

<span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s2">&quot;int&quot;</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
<span class="p">)</span>

<span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">text_only_train_ds</span><span class="p">)</span>

<span class="n">int_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">int_val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">int_test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>변환된 첫째 배치의 입력과 타깃 데이터의 정보는 다음과 같다.
<code class="docutils literal notranslate"><span class="pre">output_sequence_length=600</span></code>으로 지정하였기에 모든 문장은 단어를 최대 600개에서
잘린다. 따라서 생성되는 정수들의 벡터는 길이가 모두 600으로 지정된다.
물론 문장이 600개보다 적은 수의 단어를 사용한다면 나머지는 0으로 채워진다.
또한 벡터에 사용된 정수는 2만보다 작은 값이며,
이는 빈도가 가장 높은 2만개의 단어만을 대상(<code class="docutils literal notranslate"><span class="pre">max_tokens=20000</span></code>)으로 했기 때문이다.</p>
<p>리뷰 문장의 길이를 600개의 단어로 제한한 이유는 리뷰가 평균적으로 233개의 단어를 사용하기 때문이다.
그리고 600 단어 이상을 사용하는 리뷰는 전체의 5% 정도에 불과하다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">int_train_ds</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs.shape:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs.dtype:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;targets.shape:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;targets.dtype:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs[0]:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;targets[0]:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    <span class="k">break</span>
</pre></div>
</div>
<p><strong>벡터 원-핫 인코딩</strong></p>
<p>아래에서 소개하는 시퀀스 모델은 정수들의 벡터를 원-핫 인코딩된 벡터들의 시퀀스로 변환해서 사용한다.
예를 들어 <code class="docutils literal notranslate"><span class="pre">[2,</span> <span class="pre">1,</span> <span class="pre">4,</span> <span class="pre">0,</span> <span class="pre">0]</span></code> 벡터를 원-핫 인코딩하면 아래 결과를 얻는다.
단, 벡터에 사용된 정수는 0에서 4까지라고 가정한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span>
 <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]]</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">tf.one_hot()</span></code> 함수가 원-핫 인코딩을 실행한다.
에를 들어 위 결과는 아래 방식으로 얻어진다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">indices</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">4</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="n">depth</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>시퀀스 모델 예제 1</strong></p>
<ul class="simple">
<li><p>원-핫 인코딩 활용: 입력값을 바로 원-핫 인코딩함.</p></li>
<li><p>양방향 LSTM 모델 활용</p>
<ul>
<li><p>1차원 합성곱 신경망도 경우에 따라 유사한 성능을 발휘하지만 거의 사용되지 않음.</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="c1"># 원-핫 인코딩</span>
<span class="n">embedded</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">one_hot</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">depth</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">)</span>  <span class="c1"># (600, 20000) 모양의 출력값 생성</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">))(</span><span class="n">embedded</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>모델 훈련</strong></p>
<p>모델 훈련이 매우 느리다.
이유는 입력 데이터가 너무 많은 특성을 갖기 때문이다.
입력 데이터 하나의 모양과 특성 수는 다음과 같다.</p>
<ul class="simple">
<li><p>모양: <code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">20000)</span></code></p></li>
<li><p>특성 수: <code class="docutils literal notranslate"><span class="pre">600</span> <span class="pre">*</span> <span class="pre">20,000</span> <span class="pre">=</span> <span class="pre">12,000,000</span></code></p></li>
</ul>
<p>양방향 LSTM은 엄청난 양의 반복을 실행하기에 당연히 훈련 시간이 길어진다.
게다가 훈련된 모델의 성능이 별로 좋지 않다.
테스테셋에 대한 정확도가 87% 정도에 불과해서
바이그램 모델보다 성능이 낮다.</p>
<p><strong>시퀀스 모델 예제 2: 단어 임베딩 활용</strong></p>
<p>앞서 보았듯이 원-핫 인코딩은 별로 적절하지 않다.
원-핫 인코딩은 단어들의 순서는 잘 반영하지만 단어들 사이의 관계는 전혀 반형하지 못한다.</p>
<ul class="simple">
<li><p>“movie”와 “film”, “비디오”와 “동영상”, “강아지”와 “개” 등이 사실상 동일하다는 사실</p></li>
<li><p>“왕”(남자)과 “여왕”(여자), “boy”와 “girl” 등의 성별 관계</p></li>
<li><p>“king”의 복수는 “kings” 등 문법 관계</p></li>
<li><p>“고양이”와 “호랑이”는 고양이과, “개”와 “늑대”는 개과, “고양이”와 “개”는 애완동물, “늑대”와 “호랑이”는 야생동물 등의 관계</p></li>
</ul>
<p>반면에 <strong>단어 임베딩</strong>(word embedding)은 단어들 사이의 관계를 모델 스스로 학습과정에서 찾도록 유도한다.
단어 임베딩을 활용하는 방법은 일반적으로 다음 두 가지이다.</p>
<ul class="simple">
<li><p>모델 훈련과 동시에 단어 임베딩 학습도 진행하는 방식</p>
<ul>
<li><p>자연어 종류와 모델 훈련 목적에 따라 기본적으로 서로 다른 단어 사이의 관계가 학습되어야 함.</p></li>
<li><p>예를 들어, 영화 리뷰 분석과 재판 판결문을 분석할 때 사용되는 단어 임베딩은 많이 다름.</p></li>
</ul>
</li>
<li><p>기존에 잘 훈련된 워드 임베딩 활용 방식</p></li>
</ul>
<p><strong>케라스의 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층 활용</strong></p>
<p>케라스의 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층은 일종의 사전처럼 작동한다.
하나의 문장에 해당하는 정수들의 벡터가 입력값으로 들어오면 단어들간에 존재하는 연관성을 (어떤식으로라도) 담은
부동소수점들의 벡터로 이루어진 시퀀스를 반환한다.
아래 그림은 원-핫 인코딩 방식과 단어 임베딩 방식의 차이점을 보여준다.</p>
<ul class="simple">
<li><p>원-핫 인코딩: 특성 수가 너무 많음</p></li>
<li><p>단어 임베딩: 단어들 사이의 연관성을 256개, 512개, 1024개 정도 수준에서 찾음.</p></li>
</ul>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-02.png" style="width:45%;"></div>
<p>그림 출처: <a class="reference external" href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(Manning MEAP)</a></p>
<p>예를 들어, 600 단어로 이루어진 문장을 단어 임베딩할 때 무엇인지 모르지만 단어들 사이의 연관성을 256개 찾으라 하면
<code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">256)</span></code> 모양의 텐서(단어 벡터)를 생성한다.
즉, 600개의 단어 각각이 총 2만개의 어휘 색인에 포함된 단어들과의 연관성을 256개 찾는다.</p>
<p>방금 설명한 것을 아래 코드가 실행한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="mi">20000</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">)</span>
</pre></div>
</div>
<p>아래 코드는 단어 임베딩을 모델 구성에 직접 활용하는 것을 보여준다.
여전히 양방향 LSTM 층을 사용한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="c1"># 단어 임베딩</span>
<span class="n">embedded</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">))(</span><span class="n">embedded</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>훈련은 원-핫 인코딩 방식보다 훨씬 빠르게 이루어지며 성능은 87% 정도로 비슷하다.
바이그램 모델보다 성능이 여전히 떨어지는 이유 중에 하나는 리뷰에 사용된 단어의 수를 600개로 제한하였기 때문이다.</p>
<p><strong>패딩과 마스킹</strong></p>
<p>반면에 리뷰 문장의 길이가 600이 되지 않는 경우 나머지는
<strong>패딩</strong><font size='2'>padding</font>에 의해 0으로 채워진다.
하지만 이렇게 의미 없이 추가된 0이 훈련에 좋지 않은 영향을 미친다.
따라서 모델이 패딩을 위해 차가된 0이 있다는 사실을 인식하도록 도와주는
<strong>마스킹</strong><font size='2'>masking</font>
기능을 활용하면 좋다.</p>
<p>아래 코드는 마스킹을 활용하는 방식을 보여준다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">mask_zero=True</span></code> 옵션: 마스킹 옵션 켜기</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="c1"># 마스킹 활용 단어 임베딩</span>
<span class="n">embedded</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="n">input_dim</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">))(</span><span class="n">embedded</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>모델 성능이 88% 정도로 살짝 향상된다.</p>
<p><strong>훈련된 단어 임베딩 활용</strong></p>
<p>합성곱 신경망에서 이미지넷 등의 대용량 데이터셋을 활용하여 잘 훈련된 모델을 재활용하였던 것처럼
잘 구성된 대용량의 어휘 색인을 활용할 수 있다.
여기서는 수 백만 개의 단어를 활용하여 생성된 2014년에 스탠포드 대학교의 연구자들이 생성한
<a class="reference external" href="https://nlp.stanford.edu/projects/glove/">GloVe(Gloval Vectors for Word Representation)</a> 단어 임베딩을 활용한다.</p>
<ul class="simple">
<li><p><a class="reference external" href="http://nlp.stanford.edu/data/glove.6B.ziphttp://nlp.stanford.edu/data/glove.6B.zip">GloVe 단어 임베딩 파일</a>다운로드</p></li>
</ul>
<ul class="simple">
<li><p>GloVe 워드 임베딩 파일 파싱</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="n">path_to_glove_file</span> <span class="o">=</span> <span class="s2">&quot;glove.6B.100d.txt&quot;</span>

<span class="n">embeddings_index</span> <span class="o">=</span> <span class="p">{}</span>

<span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">path_to_glove_file</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">word</span><span class="p">,</span> <span class="n">coefs</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="n">maxsplit</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">coefs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">coefs</span><span class="p">,</span> <span class="s2">&quot;f&quot;</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
        <span class="n">embeddings_index</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">coefs</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Found </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings_index</span><span class="p">)</span><span class="si">}</span><span class="s2"> word vectors.&quot;</span><span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>GloVe 단어 임베딩 행렬 준비</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_dim</span> <span class="o">=</span> <span class="mi">100</span>

<span class="n">vocabulary</span> <span class="o">=</span> <span class="n">text_vectorization</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()</span>
<span class="n">word_index</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">,</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))))</span>

<span class="n">embedding_matrix</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">max_tokens</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">))</span>
<span class="k">for</span> <span class="n">word</span><span class="p">,</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">word_index</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
    <span class="k">if</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">max_tokens</span><span class="p">:</span>
        <span class="n">embedding_vector</span> <span class="o">=</span> <span class="n">embeddings_index</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="n">word</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">embedding_vector</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">embedding_matrix</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">embedding_vector</span>
</pre></div>
</div>
<ul class="simple">
<li><p>임베딩 층 준비</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">,</span>
    <span class="n">embeddings_initializer</span><span class="o">=</span><span class="n">keras</span><span class="o">.</span><span class="n">initializers</span><span class="o">.</span><span class="n">Constant</span><span class="p">(</span><span class="n">embedding_matrix</span><span class="p">),</span>
    <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="n">mask_zero</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
<ul class="simple">
<li><p>GloVe 임베딩 활용 모델 구성 및 훈련</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="c1"># GloVe 단어 임베딩 활용</span>
<span class="n">embedded</span> <span class="o">=</span> <span class="n">embedding_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Bidirectional</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">LSTM</span><span class="p">(</span><span class="mi">32</span><span class="p">))(</span><span class="n">embedded</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>

<span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>

<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>

<span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s2">&quot;glove_embeddings_sequence_model.keras&quot;</span><span class="p">,</span>
                                    <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">int_train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">int_val_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="s2">&quot;glove_embeddings_sequence_model.keras&quot;</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test acc: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">int_test_ds</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="dl_for_timeseries.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title"><span class="section-number">10. </span>시계열 분석</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="generative_dl.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">12. </span>생성 모델</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By 코딩알지<br/>
  
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>