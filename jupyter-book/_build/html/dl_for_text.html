
<!DOCTYPE html>


<html lang="en" data-content_root="./" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>12. 자연어 처리 &#8212; Deep Learning with Python(2판)</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=fa44fd50" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=a3416100" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-design.min.css?v=95c83b7e" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=9a2dae69"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=887ef09a"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=f930bc37"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'dl_for_text';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="13. 생성 모델" href="generative_dl.html" />
    <link rel="prev" title="11. 시계열 분석" href="dl_for_timeseries.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="intro.html">
  
  
  
  
  
  
    <p class="title logo__title">Deep Learning with Python(2판)</p>
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        <ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="what_is_deep_learning.html">1. 딥러닝 소개</a></li>
<li class="toctree-l1"><a class="reference internal" href="building_blocks_of_NN.html">2. 신경망 기본 구성 요소</a></li>
<li class="toctree-l1"><a class="reference internal" href="tf_tensor.html">3. 텐서</a></li>
<li class="toctree-l1"><a class="reference internal" href="training_loop_from_scratch.html">4. 훈련 루프 상세</a></li>
<li class="toctree-l1"><a class="reference internal" href="keras_and_tf.html">5. 케라스와 텐서플로우</a></li>
<li class="toctree-l1"><a class="reference internal" href="classification_regression.html">6. 분류와 회귀</a></li>
<li class="toctree-l1"><a class="reference internal" href="fundamentals_of_ml.html">7. 최적화와 일반화</a></li>
<li class="toctree-l1"><a class="reference internal" href="working_with_keras.html">8. 케라스 신경망 모델 활용법</a></li>
<li class="toctree-l1"><a class="reference internal" href="computer_vision_intro.html">9. 컴퓨터 비전 기초: 합성곱 신경망</a></li>
<li class="toctree-l1"><a class="reference internal" href="computer_vision_advanced.html">10. 고급 컴퓨터 비전</a></li>
<li class="toctree-l1"><a class="reference internal" href="dl_for_timeseries.html">11. 시계열 분석</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">12. 자연어 처리</a></li>
<li class="toctree-l1"><a class="reference internal" href="generative_dl.html">13. 생성 모델</a></li>
<li class="toctree-l1"><a class="reference internal" href="best_practices.html">14. 딥러닝 실전 적용</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><button class="sidebar-toggle primary-toggle btn btn-sm" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</button></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/codingalzi/dlp2" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/codingalzi/dlp2/issues/new?title=Issue%20on%20page%20%2Fdl_for_text.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/dl_for_text.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<button class="sidebar-toggle secondary-toggle btn btn-sm" title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</button>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>자연어 처리</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">12.1. 자연어 처리 소개</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">12.2. 텍스트 벡터화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">12.2.1. 텍스트 표준화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">12.2.2. 토큰화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">12.2.3. 어휘 색인화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imdb">12.2.4. IMDB 영화 후기 데이터셋 벡터화</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">12.3. 단어 임베딩</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">12.4. 트랜스포머 아키텍처</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">12.4.1. 셀프 어텐션</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">12.4.2. 멀티헤드 어텐션</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">12.4.3. 트랜스포머 인코더</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">12.5. 시퀀스-투-시퀀스 학습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">12.5.1. 트랜스포머 모델</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">12.5.2. 트랜스포머 디코더</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">12.5.3. 기계 번역 모델</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">12.6. 연습 문제</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="ch-nlp">
<span id="id1"></span><h1><span class="section-number">12. </span>자연어 처리<a class="headerlink" href="#ch-nlp" title="Link to this heading">#</a></h1>
<p><strong>감사의 글</strong></p>
<p>아래 내용은 프랑소와 숄레의
<a class="reference external" href="https://github.com/fchollet/deep-learning-with-python-notebooks">Deep Learning with Python(2판)</a>의
소스코드 내용을 참고해서 작성되었습니다.
자료를 공개한 저자에게 진심어린 감사를 전합니다.</p>
<p><strong>소스코드</strong></p>
<p>여기서 언급되는 코드를
<a class="reference external" href="https://colab.research.google.com/github/codingalzi/dlp2/blob/master/notebooks/NB-dl_for_text.ipynb">(구글 코랩) 자연어 처리</a>에서
직접 실행할 수 있다.</p>
<p><strong>슬라이드</strong></p>
<p>본문 내용을 요약한 <a class="reference external" href="https://github.com/codingalzi/dlp2/raw/master/slides/slides-dl_for_text.pdf">슬라이드</a>를 다운로드할 수 있다.</p>
<p><strong>주요 내용</strong></p>
<ul class="simple">
<li><p>텍스트 벡터화</p></li>
<li><p>단어 임베딩</p></li>
<li><p>트랜스포머 아키텍처</p></li>
<li><p>시퀀스-투-시퀀스 학습</p></li>
<li><p>영어-한국어 번역</p></li>
</ul>
<section id="id2">
<h2><span class="section-number">12.1. </span>자연어 처리 소개<a class="headerlink" href="#id2" title="Link to this heading">#</a></h2>
<p>파이썬, 자바, C, C++, C#, 자바스크립트 등 컴퓨터 프로그래밍언어와 구분하기 위해
일상에서 사용되는 한국어, 영어 등을 <strong>자연어</strong><font size='2'>natural language</font>라
부른다.
자연어의 특성상 정확한 분석을 위한 알고리즘을 구현하는 일은 매우 어렵다.
딥러닝 기법이 활용되기 이전까지는 적절한 규칙을 구성하여 자연어를 이해하려는
수 많은 시도가 있어왔지만 별로 성공적이지 않았다.</p>
<p>1990년대부터 인터넷으로부터 구해진 엄청난 양의 텍스트 데이터에 머신러닝 기법을
적용하기 시작했다.
단, <strong>언어의 이해</strong>를 주요 목표로 삼기에는 기술, 하드웨어 측면 모두에서 부족하였으며,
아래 예제들처럼 입력 텍스트를 분석하여
<strong>통계적으로 유용한 정보를 예측</strong>하는 정도의 수준에 머물렀다.</p>
<ul class="simple">
<li><p>텍스트 분류: “이 텍스트의 주제는?”</p></li>
<li><p>내용 필터링: “욕설 사용?”</p></li>
<li><p>감성 분석: “그래서 내용이 긍정적이야 부정적이야?”</p></li>
<li><p>언어 모델링: “이 문장 다음에 어떤 단어가 와야 할까?”</p></li>
<li><p>번역: “이거를 한국어로 어떻게 말해?”</p></li>
<li><p>요약: “이 기사를 한 줄로 요약해 볼래?”</p></li>
</ul>
<p>이와 같은 일을 <strong>자연어 처리</strong><font size='2'>Natural Language Processing</font>라 하며
단어와 텍스트에서 찾을 수 있는 패턴을  인식하려 시도한다.</p>
<div class="note admonition">
<p class="admonition-title">텍스트란?</p>
<p>텍스트<font size='2'>text</font>는 문자, 단어, 문장, 글 등 자연어로 표현된 데이터를 가리킨다.</p>
</div>
<p><strong>머신러닝 활용</strong></p>
<p>자연어 처리를 위해 1990년대부터 시작된 머신러닝 활용의 변화과정은 다음과 같다.</p>
<ul class="simple">
<li><p>1990 - 2010년대 초반:
결정트리, 로지스틱 회귀 모델이 주로 활용됨.</p></li>
<li><p>2014-2015: LSTM 등 시퀀스 처리 알고리즘 활용 시작</p></li>
<li><p>2015-2017: (양방향) 순환신경망이 기본적으로 활용됨.</p></li>
<li><p>2017-2018: 트랜스포머<font size='2'>transformer</font> 아키텍처가 많은 난제들을 해결함.</p></li>
<li><p>2022 이후: 트랜스포머 아키텍처를 이용한 GPT의 혁명적 발전</p></li>
</ul>
</section>
<section id="id3">
<h2><span class="section-number">12.2. </span>텍스트 벡터화<a class="headerlink" href="#id3" title="Link to this heading">#</a></h2>
<p>딥러닝 모델은 텍스트 자체를 처리할 수 없다.
따라서 텍스트를 수치형 텐서로 변환하는 <strong>텍스트 벡터화</strong><font size='2'>text vectorization</font> 과정이 요구된다.
아래 그림이 텍스트 벡터화의 기본적인 과정을 잘 보여준다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-EmbeddingLayer-a.png" style="width:50%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://developers.google.com/machine-learning/guides/text-classification/step-3?hl=ko">머신러닝 가이드</a>&gt;</div></p><p>텍스트 벡터화는 보통 다음 세 단계를 따른다.</p>
<ol class="arabic simple">
<li><p><strong>텍스트 표준화</strong><font size='2'>text standardization</font>: 소문자화, 마침표 제거 등등</p></li>
<li><p><strong>토큰화</strong><font size='2'>tokenization</font>: 기본 단위의 <strong>유닛</strong><font size='2'>units</font>으로 쪼개기.
문자, 단어, 단어 집합 등이 토큰으로 활용됨.</p></li>
<li><p><strong>어휘 색인화</strong><font size='2'>vocabulary indexing</font>: 토큰 각각을 하나의 정수 색인(인덱스)으로 변환.</p></li>
</ol>
<section id="id4">
<h3><span class="section-number">12.2.1. </span>텍스트 표준화<a class="headerlink" href="#id4" title="Link to this heading">#</a></h3>
<p>다음 두 텍스트를 표준화를 통해 동일한 텍스트로 변환해보자.</p>
<ul class="simple">
<li><p>“sunset came. i was staring at the Mexico sky. Isnt nature splendid??”</p></li>
<li><p>“Sunset came; I stared at the México sky. Isn’t nature splendid?”</p></li>
</ul>
<p>예를 들어 다음과 같은 표준화 기법을 사용할 수 있다.</p>
<ul class="simple">
<li><p>모두 소문자화</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">.</span></code>, <code class="docutils literal notranslate"><span class="pre">;</span></code>, <code class="docutils literal notranslate"><span class="pre">?</span></code>, <code class="docutils literal notranslate"><span class="pre">'</span></code> 등 특수 기호 제거</p></li>
<li><p>특수 알파벳 변환: “é”를 “e”로, “æ”를 “ae”로 등등</p></li>
<li><p>동사/명사의 기본형 활용: “cats”를 “[cat]”로, “was staring”과 “stared”를 “[stare]”로 등등.</p></li>
</ul>
<p>그러면 위 두 텍스트 모두 아래 텍스트로 변환된다.</p>
<p>“sunset came i [stare] at the mexico sky isnt nature splendid”</p>
<p>표준화 과정을 통해 어느 정도의 정보를 상실하게 되지만
학습해야할 내용을 줄여 일반화 성능이 보다 좋은 모델을 훈련시키는 장점이 있다.
하지만 분석 목적에 따라 사용되는 표준화 기법이 달라질 수 있음에 주의해야 한다.
예를 들어 인터뷰 기사의 경우 물음표(<code class="docutils literal notranslate"><span class="pre">?</span></code>)는 제거하면 안된다.</p>
</section>
<section id="id5">
<h3><span class="section-number">12.2.2. </span>토큰화<a class="headerlink" href="#id5" title="Link to this heading">#</a></h3>
<p>텍스트 표준화 이후 데이터 분석의 기본 단위인 토큰으로 쪼개야 한다.
보통 아래 세 가지 방식 중에 하나를 사용한다.</p>
<ul class="simple">
<li><p>단어 토큰화</p>
<ul>
<li><p>공백으로 구분된 단어들로 쪼개기.</p></li>
<li><p>경우에 따라 동사 어근과 어미를 구분하기도 함: “star+ing”, “call+ed” 등등</p></li>
</ul>
</li>
<li><p>N-그램 토큰화</p>
<ul>
<li><p>N-그램 토큰: 연속으로 위치한 N 개(이하)의 단어 묶음</p></li>
<li><p>예제: “the cat”, “he was” 등은 2-그램(바이그램) 토큰</p></li>
</ul>
</li>
<li><p>문자 토큰화</p>
<ul>
<li><p>하나의 문자를 하나의 토큰으로 지정.</p></li>
<li><p>텍스트 생성, 음성 인식 등에서 활용됨.</p></li>
</ul>
</li>
</ul>
<p>일반적으로 문자 토큰화는 잘 사용되지 않는다.
단어 토큰화와 N-그램 토큰화를 간략하게 소개하면 다음과 같다.</p>
<ul class="simple">
<li><p>단어 토큰화: 단어들의 순서를 중요시하는 <strong>시퀀스 모델</strong><font size='2'>sequence models</font>을 사용할 경우 주로 활용된다.</p></li>
<li><p>N-그램 토큰화: 단어들의 순서를 별로 상관하지 않는 <strong>단어 주머니</strong><font size='2'>bag-of-words</font>
모델을 사용할 경우 주로 활용된다.</p>
<ul>
<li><p>N-그램: 단어들 사이의 순서에 대한 지엽적 정보를 어느 정도 유지</p></li>
<li><p>일종의 특성 공학<font size='2'>feature engineering</font> 기법이며, 트랜스포머 등 최신 기법에는 활용되지 않음.</p></li>
</ul>
</li>
</ul>
<p>단어 주머니<font size='2'>bag-of-words</font>는 N-그램으로 구성된 집합을 의미하며
<strong>N-그램 주머니</strong>라고도 불린다.
예를 들어 “the cat sat on the mat.”에 대한
바이그램과 3-그램 주머니는 각각 다음과 같다.</p>
<ul>
<li><p>2-그램(바이그램) 주머니</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat&quot;</span><span class="p">,</span>
<span class="s2">&quot;sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;on the&quot;</span><span class="p">,</span> <span class="s2">&quot;the mat&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
<ul>
<li><p>3-그램 주머니</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="p">{</span><span class="s2">&quot;the&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat&quot;</span><span class="p">,</span> <span class="s2">&quot;the cat sat&quot;</span><span class="p">,</span>
<span class="s2">&quot;sat&quot;</span><span class="p">,</span> <span class="s2">&quot;sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on&quot;</span><span class="p">,</span> <span class="s2">&quot;cat sat on&quot;</span><span class="p">,</span> <span class="s2">&quot;on the&quot;</span><span class="p">,</span>
<span class="s2">&quot;sat on the&quot;</span><span class="p">,</span> <span class="s2">&quot;the mat&quot;</span><span class="p">,</span> <span class="s2">&quot;mat&quot;</span><span class="p">,</span> <span class="s2">&quot;on the mat&quot;</span><span class="p">}</span>
</pre></div>
</div>
</li>
</ul>
<p>여기서는 단어 토큰화를 이용한 자연어 처리 분석을 소개한다.</p>
</section>
<section id="id6">
<h3><span class="section-number">12.2.3. </span>어휘 색인화<a class="headerlink" href="#id6" title="Link to this heading">#</a></h3>
<p>훈련셋에 포함된 모든 단어 토큰들의 색인(인덱스)을 생성한다.
<a class="reference internal" href="classification_regression.html#ch-classification-regression"><span class="std std-numref">6장</span></a>에서 언급한 대로
사용 빈도가 높은 1만, 2만, 또는 3만 개의 단어만을 대상으로 어휘 색인화를 진행하는 게 일반적이다.
당시에 IMDB 영화 후기 데이터셋을 아래와 같이 불러오면서 불러올 때
<code class="docutils literal notranslate"><span class="pre">num_words=10000</span></code>을 사용하여 사용 빈도수가 상위 1만 등 안에 들지 않는 단어는
영화 후기에서 무시되도록 하였다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">tensorflow.keras.datasets</span> <span class="kn">import</span> <span class="n">imdb</span>

<span class="n">imdb</span><span class="o">.</span><span class="n">load_data</span><span class="p">(</span><span class="n">num_words</span><span class="o">=</span><span class="mi">10000</span><span class="p">)</span>
</pre></div>
</div>
<p>어휘 색인화를 통해 생성된 <strong>어휘집</strong><font size='2'>vocabulary</font>을 이용하여
텍스트를 아래와 같은 색인으로 구성된 리스트로 변환하는 과정이 <strong>텍스트 벡터화</strong>다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">14</span><span class="p">,</span> <span class="mi">22</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">43</span><span class="p">,</span> <span class="mi">530</span><span class="p">,</span> <span class="mi">973</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1385</span><span class="p">,</span> <span class="mi">65</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">]</span>
</pre></div>
</div>
<p>참고로 텍스트를 변환한 벡터에 사용되는 0과 1은 특별한 기능을 수행한다.
바로 위의 색인 리스트에는 1이 두 번, 0이 5번 사용되었다.</p>
<ul class="simple">
<li><p>1의 기능: 미등록 어휘 인덱스. 즉, 어휘 색인에 미등록된(out-of-vocabulary, OOV) 단어는 모두 1로 처리된다.
그런 단어는 일반 텍스트로 재번역되는 경우 “[UNK]”, 즉 모르는(unknown) 단어로 표현된다.</p></li>
<li><p>0의 기능: 텍스트의 길이를 통일시키기 위해 패딩으로 사용하며, <strong>마스크 토큰</strong><font size='2'>mask token</font>이라 부르리도 한다.</p></li>
</ul>
<p>영화 후기 분석을 위해
<a class="reference internal" href="classification_regression.html#ch-classification-regression"><span class="std std-numref">6장</span></a>에서 사용한
케라스의 imdb 데이터셋은 앞서 설명한 방식으로 이미 텍스트 벡터화가 완료된 상태였다.
여기서는 영어 텍스트로 작성된 원본 영화 후기로 구성된 imdb 데이터셋을
다운로드하여 텍스트 벡터화 전처리를 직접 수행하는 단계부터 살펴보려 한다.</p>
<p><strong><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층</strong></p>
<p>케라스의 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층을 이용하여 텍스트 벡터화를 진행할 수 있다.
아래 코드는 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층의 구성에 사용되는 주요 기본 설정을 보여준다.
표준화, 토큰화, 출력 모드를 다양한 방식으로 지정할 수 있지만 기본값만 다룬다.
자세한 내용은 <a class="reference external" href="https://keras.io/api/layers/preprocessing_layers/text/text_vectorization/">케라스 벡터화 층 공식문서</a>를 참고한다.</p>
<ul class="simple">
<li><p>표준화</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">standardize='lower_and_strip_punctuation'</span></code>: 소문자화와 마침표 등 제거</p></li>
</ul>
</li>
<li><p>토큰화</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">split='whitespace'</span></code>: 단어 기준 쪼개기</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">ngrams=None</span></code>: n-그램 미사용</p></li>
</ul>
</li>
<li><p>출력 모드</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">output_mode=&quot;int&quot;</span></code>: 단어를 정수 색인으로 인코딩</p></li>
</ul>
</li>
<li><p>텍스트 길이 제한</p>
<ul>
<li><p><code class="docutils literal notranslate"><span class="pre">output_sequence_length=None</span></code>: 단어 길이 제한 지정. 제한 없음이 기본값.</p></li>
</ul>
</li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">tensorflow.keras.layers</span> <span class="kn">import</span> <span class="n">TextVectorization</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">TextVectorization</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">standardize</span><span class="o">=</span><span class="s1">&#39;lower_and_strip_punctuation&#39;</span><span class="p">,</span>  <span class="c1"># 기본값</span>
<span class="gp">... </span>    <span class="n">split</span><span class="o">=</span><span class="s1">&#39;whitespace&#39;</span><span class="p">,</span>                         <span class="c1"># 기본값</span>
<span class="gp">... </span>    <span class="n">ngrams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>                                <span class="c1"># 기본값</span>
<span class="gp">... </span>    <span class="n">output_mode</span><span class="o">=</span><span class="s1">&#39;int&#39;</span><span class="p">,</span>                          <span class="c1"># 기본값</span>
<span class="gp">... </span>    <span class="n">output_sequence_length</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>                <span class="c1"># 기본값</span>
<span class="gp">... </span><span class="p">)</span>
</pre></div>
</div>
<p>예를 들어, 아래 데이터셋을 이용하여 텍스트 벡터화에 사용될 어휘집을 생성해보자.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">dataset</span> <span class="o">=</span> <span class="p">[</span>
<span class="gp">... </span>    <span class="s2">&quot;I write, erase, rewrite&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;Erase again, and then&quot;</span><span class="p">,</span>
<span class="gp">... </span>    <span class="s2">&quot;A poppy blooms.&quot;</span><span class="p">,</span>
<span class="gp">... </span><span class="p">]</span>
</pre></div>
</div>
<p>어휘 색인화와 어휘집 생성은 <code class="docutils literal notranslate"><span class="pre">adapt()</span></code> 메서드가 담당한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span>
</pre></div>
</div>
<p>생성된 어휘집은 다음과 같다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">vocabulary</span> <span class="o">=</span> <span class="n">text_vectorization</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">vocabulary</span>
<span class="go">[&#39;&#39;,</span>
<span class="go"> &#39;[UNK]&#39;,</span>
<span class="go"> &#39;erase&#39;,</span>
<span class="go"> &#39;write&#39;,</span>
<span class="go"> &#39;then&#39;,</span>
<span class="go"> &#39;rewrite&#39;,</span>
<span class="go"> &#39;poppy&#39;,</span>
<span class="go"> &#39;i&#39;,</span>
<span class="go"> &#39;blooms&#39;,</span>
<span class="go"> &#39;and&#39;,</span>
<span class="go"> &#39;again&#39;,</span>
<span class="go"> &#39;a&#39;]</span>
</pre></div>
</div>
<p><strong>텍스트 벡터화</strong></p>
<p>생성된 어휘집을 활용하여 새로운 텍스트를 벡터화 해보자.
어휘집에 포함되지 않은 <code class="docutils literal notranslate"><span class="pre">'still'</span></code> 단어는 1로 변환되며
텍스트의 길이 설정은 없기에 0, 즉 마스크 토큰을 이용한 패딩은 사용되지 않는다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">test_sentence</span> <span class="o">=</span> <span class="s2">&quot;I write, rewrite, and still rewrite again&quot;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">encoded_sentence</span> <span class="o">=</span> <span class="n">text_vectorization</span><span class="p">(</span><span class="n">test_sentence</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">encoded_sentence</span><span class="p">)</span>
<span class="go">tf.Tensor([ 7  3  5  9  1  5 10], shape=(7,), dtype=int64)</span>
</pre></div>
</div>
<p>벡터화된 텐서로부터 텍스트를 복원하면 표준화된 텍스트가 생성된다.
1로 변환된 <code class="docutils literal notranslate"><span class="pre">'still'</span></code>은 <code class="docutils literal notranslate"><span class="pre">[UNK]</span></code>로 복원된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">inverse_vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">enumerate</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">decoded_sentence</span> <span class="o">=</span> <span class="s2">&quot; &quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">inverse_vocab</span><span class="p">[</span><span class="nb">int</span><span class="p">(</span><span class="n">i</span><span class="p">)]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">encoded_sentence</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="nb">print</span><span class="p">(</span><span class="n">decoded_sentence</span><span class="p">)</span>
<span class="go">i write rewrite and [UNK] rewrite again</span>
</pre></div>
</div>
<div class="warning admonition">
<p class="admonition-title"><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층과 GPU</p>
<p><code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층은 GPU 또는 TPU에서 지원되지 않는다.
따라서 모델 구성에 직접 사용하는 방식은 모델의 훈련을
늦출 수 있기에 권장되지 않는다.</p>
<p>일반적으로 모델 훈련을 위한 데이터셋 전처리는
모델 구성과 독립적으로 처리한다.
하지만 훈련이 완성된 모델을 실전에 배치할 경우 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층을
완성된 모델에 추가해서 사용한다.</p>
</div>
</section>
<section id="imdb">
<h3><span class="section-number">12.2.4. </span>IMDB 영화 후기 데이터셋 벡터화<a class="headerlink" href="#imdb" title="Link to this heading">#</a></h3>
<p>IMDB 영화 후기 데이터셋을 이용하여 텍스트 벡타화 과정을 상세히 살펴 본다.</p>
<p><strong>과정 1: 데이터셋 다운로드 후 압축 풀기</strong></p>
<p>먼저 자연어로 구성된 IMDB 영화 후기 데이터셋을 다운로드한다.
5만 개의 IMDB 영화 후기를 압축한
<a class="reference external" href="https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz">aclImdb_v1.tar,gz</a> 파일을
다운로드 한 후에 압축을 풀면 아래 구조의 디렉토리가 생성된다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aclImdb</span><span class="o">/</span>
<span class="o">...</span><span class="n">test</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
<span class="o">...</span><span class="n">train</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">train</span></code>과 <code class="docutils literal notranslate"><span class="pre">test</span></code> 디렉토리 각각에 포함된 <code class="docutils literal notranslate"><span class="pre">pos</span></code>와 <code class="docutils literal notranslate"><span class="pre">neg</span></code> 서브디렉토리는
각각 12,500 개의 긍정과 부정 후기를 포함한다.</p>
<p><strong>과정 2: 검증셋 준비</strong></p>
<p>훈련셋의 20%를 검증셋으로 떼어낸다.
이를 위해 <code class="docutils literal notranslate"><span class="pre">aclImdb/val</span></code> 디렉토리를 생성한 후에
긍정과 부정 훈련셋 모두 무작위로 섞은 후 그중 20%를 검증셋 디렉토리로 옮긴다.
최종적으로 자연어로 구성된 IMDB 영화 후기 원본 데이터셋이 아래 디렉토리 구조로 나뉜다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">train</span></code>: <code class="docutils literal notranslate"><span class="pre">pos</span></code>와 <code class="docutils literal notranslate"><span class="pre">neg</span></code> 서브디렉토리에 각각 10,000 개의 긍정과 부정 후기 포함</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">test</span></code>: <code class="docutils literal notranslate"><span class="pre">pos</span></code>와 <code class="docutils literal notranslate"><span class="pre">neg</span></code> 서브디렉토리에 각각 12,500 개의 긍정과 부정 후기 포함</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">val</span></code>: <code class="docutils literal notranslate"><span class="pre">pos</span></code>와 <code class="docutils literal notranslate"><span class="pre">neg</span></code> 서브디렉토리에 각각 2,500 개의 긍정과 부정 후기 포함</p></li>
</ul>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">aclImdb</span><span class="o">/</span>
<span class="o">...</span><span class="n">test</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
<span class="o">...</span><span class="n">train</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
<span class="o">...</span><span class="n">val</span><span class="o">/</span>
<span class="o">......</span><span class="n">pos</span><span class="o">/</span>
<span class="o">......</span><span class="n">neg</span><span class="o">/</span>
</pre></div>
</div>
<p><strong>과정 3: 텐서 데이터셋 준비</strong></p>
<p><code class="docutils literal notranslate"><span class="pre">keras.utils.text_dataset_from_directory()</span></code> 함수를 이용하여
훈련셋 텐서, 검증셋 텐서, 테스트셋 텐서를 생성한다.
생성된 값들의 자료형은 모두 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code>이다.
<code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 자료형은
데이터 샘플을 일정 크기로 묶은 배치 데이터셋으로 구성된다.
여기서는 데이터 샘플을 32 개씩 묶은 배치를 사용한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">batch_size</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">train_ds</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s2">&quot;aclImdb/train&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">val_ds</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s2">&quot;aclImdb/val&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>

<span class="n">test_ds</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">text_dataset_from_directory</span><span class="p">(</span>
    <span class="s2">&quot;aclImdb/test&quot;</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">Dataset</span></code>의 항목이 입력값과 타깃으로 구성된 튜플 형식임에 주의한다.
사실 다운로드한 IMDB 데이터셋은 영화 후기만 포함한다.
그런데 <code class="docutils literal notranslate"><span class="pre">text_dataset_from_directory()</span></code> 함수가 컴퓨터 하드디스크의 디렉토리에 저장된
영화 후기를 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 자료형의 텐서로 불러올 때
<code class="docutils literal notranslate"><span class="pre">pos</span></code>와 <code class="docutils literal notranslate"><span class="pre">neg</span></code> 서브디렉토리에 포함된 영화 후기의 타깃으로
각각 1과 0을 지정한다.</p>
<p>예를 들어, 훈련셋 텐서인 <code class="docutils literal notranslate"><span class="pre">train_ds</span></code>에 포함된 첫째 배치 항목에 포함된 첫째 훈련 샘플의 입력값과 타깃의 정보는 다음과 같다.
입력 샘플은 텐서플로우의 <code class="docutils literal notranslate"><span class="pre">tf.string</span></code> 이라는 문자열 텐서로,
타깃은 <code class="docutils literal notranslate"><span class="pre">int32</span></code> 정수 텐서로 지정된다.
<code class="docutils literal notranslate"><span class="pre">tf.string</span></code>은 파이썬의 기본 자료형인 <code class="docutils literal notranslate"><span class="pre">str</span></code>과 다름에 주의한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">train_ds</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs.shape:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs.dtype:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;targets.shape:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;targets.dtype:&quot;</span><span class="p">,</span> <span class="n">targets</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>

<span class="go">...     # 예제: 첫째 배치의 첫째 후기</span>
<span class="go">...     print(&quot;inputs[0]:&quot;, inputs[0])</span>
<span class="go">...     print(&quot;targets[0]:&quot;, targets[0])    </span>
<span class="go">...     break</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>inputs.shape: (32,)
inputs.dtype: &lt;dtype: &#39;string&#39;&gt;
targets.shape: (32,)
targets.dtype: &lt;dtype: &#39;int32&#39;&gt;

inputs[0]: tf.Tensor(b&#39;The film begins with a bunch of kids in reform school and
focuses on a kid named \&#39;Gabe\&#39;, who has apparently worked hard to earn his parole.
Gabe and his sister move to a new neighborhood to make a fresh start and soon Gabe
meets up with the Dead End Kids. The Kids in this film are little punks, but they
are much less antisocial than they\&#39;d been in other previous films and down deep,
they are well-meaning punks. However, in this neighborhood there are also some
criminals who are perpetrating insurance fraud through arson and see Gabe as a
convenient scapegoat--after all, he\&#39;d been to reform school and no one would
believe he was innocent once he was framed. So, when Gabe is about ready to be sent
back to &quot;The Big House&quot;, it\&#39;s up to the rest of the gang to save him and expose
the real crooks.&lt;br /&gt;&lt;br /&gt;The &quot;Dead End Kids&quot; appeared in several Warner Brothers
films in the late 1930s and the films were generally very good (particularly ANGELS
WITH DIRTY FACES). However, after the boys\&#39; contracts expired, they went on to
Monogram Studios and the films, to put it charitably, were very weak and formulaic
--with Huntz Hall and Leo Gorcey being pretty much the whole show and the group
being renamed &quot;The Bowery Boys&quot;. Because ANGELS WASH THEIR FACES had the excellent
writing and production values AND Hall and Gorcey were not constantly mugging for
the camera, it\&#39;s a pretty good film--and almost earns a score of 7 (it\&#39;s REAL
close). In fact, while this isn\&#39;t a great film aesthetically, it\&#39;s sure a lot of
fun to watch, so I will give it a 7! Sure, it was a tad hokey-particularly towards
the end when the kids take the law into their own hands and Reagan ignores the Bill
of Rights--but it was also quite entertaining. The Dead End Kids are doing their
best performances and Ronald Reagan and Ann Sheridan provided excellent support.
Sure, this part of the film was illogical and impossible but somehow it was still
funny and rather charming--so if you can suspend disbelief, it works well.&#39;,
shape=(), dtype=string)
targets[0]: tf.Tensor(1, shape=(), dtype=int32)
</pre></div>
</div>
<p><strong>과정 4: 텍스트 벡터화</strong></p>
<p>데이터셋에 포함된 모든 영화 후기를 대상으로 텍스트 벡터화를 진행한다.
단, 영화 후기가 최대 600 개의 단어만 사용하도록 한다.
또한 사용되는 어휘는 사용빈도 기준으로 최대 2만 개로 제한한다.</p>
<p>영화 후기의 길이를 600 개의 단어로 제한한 이유는 후기가 평균적으로 233 개의 단어를 사용하고,
600 단어 이상을 사용하는 후기는 전체의 5% 정도에 불과하기 때문이다.
600 개 이상의 단어를 사용하는 영화 후기는 모두 600 단어로 끊는다.
600 개보다 적은 수의 단어를 사용한다면 마스크 토큰 0을 패딩으로 사용한다.</p>
<p>아래 코드는 텍스트 벡터화를 진행할 <code class="docutils literal notranslate"><span class="pre">TextVectorization</span></code> 층을 지정한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">max_length</span> <span class="o">=</span> <span class="mi">600</span>                       <span class="c1"># 후기 길이 제한</span>
<span class="n">max_tokens</span> <span class="o">=</span> <span class="mi">20000</span>                     <span class="c1"># 단어 사용빈도 제한</span>

<span class="n">text_vectorization</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">TextVectorization</span><span class="p">(</span>
    <span class="n">max_tokens</span><span class="o">=</span><span class="n">max_tokens</span><span class="p">,</span>              <span class="c1"># 후기에 사용되는 단어의 종류는 총 2만 종류</span>
    <span class="n">output_mode</span><span class="o">=</span><span class="s2">&quot;int&quot;</span><span class="p">,</span>
    <span class="n">output_sequence_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>  <span class="c1"># 하나의 후기에 포함된 최대 단어는 최대 600 개</span>
<span class="p">)</span>
</pre></div>
</div>
<p>어휘 색인화는 훈련셋만 대상으로 진행한다.
따라서 훈련셋에 포함되지 않은 단어는 모두 1로 처리되어 무시된다.
어휘 색인화를 위해 먼저 <code class="docutils literal notranslate"><span class="pre">map()</span></code> 메서드를 이용하여 훈련셋에서 타깃을 제외한 영화 후기만으로 구성된
데이터셋을 생성한다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">map()</span></code> 메서드: <code class="docutils literal notranslate"><span class="pre">map()</span></code> 메서드의 반환값은 인자로 지정된 함수가 각각의 샘플에 대해 적용된 결과를
동일한 크기의 배치로 묶은 <code class="docutils literal notranslate"><span class="pre">Dataset</span></code> 객체이다.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">lambda</span> <span class="pre">x,</span> <span class="pre">y:</span> <span class="pre">x</span></code> 함수: 튜플 형식의 인자를 받아 첫째 항목을 반환하는 람다 함수이다.
여기서는 <code class="docutils literal notranslate"><span class="pre">train_ds</span></code> 텐서에 포함된 샘플이 영화 후기와 타깃을 하나로 묶은 튜플 형식의 값이기 때문에
영화 후기만을 추출하기 위해 사용된다.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 어휘 색인 생성 대상 훈련셋 후기 텍스트 데이터셋</span>
<span class="n">text_only_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>

<span class="c1"># 어휘 색인</span>
<span class="n">text_vectorization</span><span class="o">.</span><span class="n">adapt</span><span class="p">(</span><span class="n">text_only_train_ds</span><span class="p">)</span>
</pre></div>
</div>
<p>생성된 어휘집을 이용하여 훈련셋, 검증셋, 테스셋 모두 텍스트 벡터화를 진행한다.
영화 후기만을 대상으로 벡터화를 진행해야 하기에
<code class="docutils literal notranslate"><span class="pre">map()</span></code> 메서드를 이용하여 영화 후기와 타깃을 구분한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 후기를 길이가 2만인 정수들의 리스트로 변환</span>
<span class="n">int_train_ds</span> <span class="o">=</span> <span class="n">train_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">int_val_ds</span> <span class="o">=</span> <span class="n">val_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
<span class="n">int_test_ds</span> <span class="o">=</span> <span class="n">test_ds</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="p">(</span><span class="n">text_vectorization</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">y</span><span class="p">))</span>
</pre></div>
</div>
<p>예를 들어, 벡터화된 훈련셋의 첫째 영화 후기, 즉 첫째 배치의 첫째 샘플은 다음과 같다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">targets</span> <span class="ow">in</span> <span class="n">int_train_ds</span><span class="p">:</span>
<span class="gp">... </span>    <span class="nb">print</span><span class="p">(</span><span class="s2">&quot;inputs[0]:&quot;</span><span class="p">,</span> <span class="n">inputs</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="gp">... </span>    <span class="k">break</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="p">[</span>   <span class="mi">11</span>     <span class="mi">7</span>     <span class="mi">4</span>  <span class="mi">8614</span>    <span class="mi">18</span>    <span class="mi">38</span>     <span class="mi">9</span>   <span class="mi">139</span>   <span class="mi">138</span>   <span class="mi">197</span>   <span class="mi">640</span>    <span class="mi">12</span>
    <span class="mi">30</span>    <span class="mi">22</span>   <span class="mi">167</span>     <span class="mi">6</span>  <span class="mi">3035</span>     <span class="mi">2</span>    <span class="mi">86</span>  <span class="mi">3146</span>   <span class="mi">664</span>    <span class="mi">19</span>    <span class="mi">12</span>   <span class="mi">291</span>
    <span class="mi">11</span>    <span class="mi">14</span>  <span class="mi">2400</span>  <span class="mi">2996</span>    <span class="mi">13</span>    <span class="mi">55</span>   <span class="mi">322</span>   <span class="mi">429</span>    <span class="mi">11</span>    <span class="mi">19</span>   <span class="mi">172</span>     <span class="mi">4</span>
   <span class="mi">337</span>    <span class="mi">35</span>   <span class="mi">116</span>   <span class="mi">230</span>   <span class="mi">172</span>     <span class="mi">4</span>  <span class="mi">1107</span>     <span class="mi">2</span>   <span class="mi">196</span>  <span class="mi">1562</span>    <span class="mi">14</span>    <span class="mi">12</span>
    <span class="mi">10</span>   <span class="mi">399</span>     <span class="mi">9</span>   <span class="mi">100</span>     <span class="mi">9</span>    <span class="mi">14</span>   <span class="mi">478</span>    <span class="mi">46</span>  <span class="mi">1368</span>   <span class="mi">162</span>    <span class="mi">31</span>    <span class="mi">47</span>
   <span class="mi">509</span>    <span class="mi">56</span>     <span class="mi">2</span>  <span class="mi">7585</span>   <span class="mi">645</span>    <span class="mi">66</span>   <span class="mi">733</span>     <span class="mi">5</span>   <span class="mi">239</span>  <span class="mi">1428</span>     <span class="mi">1</span>    <span class="mi">17</span>
     <span class="mi">2</span>    <span class="mi">86</span>    <span class="mi">18</span>     <span class="mi">3</span>    <span class="mi">56</span>    <span class="mi">47</span>   <span class="mi">645</span>    <span class="mi">12</span>    <span class="mi">23</span>    <span class="mi">66</span>     <span class="mi">6</span>    <span class="mi">28</span>
   <span class="mi">920</span>     <span class="mi">6</span>   <span class="mi">376</span>    <span class="mi">19</span>   <span class="mi">197</span>   <span class="mi">107</span> <span class="mi">14487</span>    <span class="mi">39</span>     <span class="mi">8</span>  <span class="mi">8227</span>    <span class="mi">83</span>    <span class="mi">23</span>
   <span class="mi">103</span>   <span class="mi">235</span>     <span class="mi">1</span>    <span class="mi">16</span>  <span class="mi">6307</span>    <span class="mi">13</span>     <span class="mi">4</span>   <span class="mi">309</span>   <span class="mi">869</span>    <span class="mi">21</span>     <span class="mi">2</span>  <span class="mi">7585</span>
   <span class="mi">645</span>    <span class="mi">10</span>    <span class="mi">14</span>   <span class="mi">776</span>     <span class="mi">6</span>   <span class="mi">158</span>    <span class="mi">12</span>   <span class="mi">593</span>     <span class="mi">5</span>     <span class="mi">2</span>   <span class="mi">645</span>    <span class="mi">67</span>
    <span class="mi">41</span>  <span class="mi">3488</span>  <span class="mi">5321</span>     <span class="mi">8</span>   <span class="mi">188</span>    <span class="mi">48</span>    <span class="mi">67</span>   <span class="mi">208</span>    <span class="mi">57</span>     <span class="mi">1</span>    <span class="mi">31</span>    <span class="mi">32</span>
     <span class="mi">2</span>  <span class="mi">1990</span>    <span class="mi">67</span>   <span class="mi">154</span>   <span class="mi">239</span>  <span class="mi">1265</span>    <span class="mi">35</span>   <span class="mi">154</span>    <span class="mi">66</span>     <span class="mi">4</span>     <span class="mi">1</span>     <span class="mi">3</span>
    <span class="mi">67</span>   <span class="mi">208</span>     <span class="mi">8</span>    <span class="mi">50</span>  <span class="mi">1244</span>   <span class="mi">450</span>    <span class="mi">39</span>    <span class="mi">55</span>   <span class="mi">322</span>     <span class="mi">6</span>   <span class="mi">103</span>    <span class="mi">12</span>
   <span class="mi">217</span>    <span class="mi">53</span>     <span class="mi">6</span>   <span class="mi">493</span>    <span class="mi">72</span>   <span class="mi">167</span>     <span class="mi">6</span>     <span class="mi">2</span>  <span class="mi">3925</span>     <span class="mi">3</span>    <span class="mi">11</span>    <span class="mi">18</span>
     <span class="mi">7</span>   <span class="mi">479</span>     <span class="mi">8</span>   <span class="mi">144</span>     <span class="mi">1</span>    <span class="mi">13</span>  <span class="mi">8499</span>    <span class="mi">49</span>   <span class="mi">330</span>     <span class="mi">2</span>   <span class="mi">223</span>    <span class="mi">14</span>
  <span class="mi">5673</span>    <span class="mi">22</span>   <span class="mi">730</span>    <span class="mi">15</span>  <span class="mi">1428</span>    <span class="mi">15</span>     <span class="mi">8</span>     <span class="mi">2</span>    <span class="mi">86</span>    <span class="mi">42</span>   <span class="mi">327</span>    <span class="mi">18</span>
    <span class="mi">19</span>   <span class="mi">943</span>     <span class="mi">5</span>   <span class="mi">250</span>    <span class="mi">16</span>     <span class="mi">2</span>   <span class="mi">322</span>    <span class="mi">57</span>  <span class="mi">2027</span>  <span class="mi">1932</span>   <span class="mi">383</span>    <span class="mi">62</span>
    <span class="mi">14</span>     <span class="mi">4</span> <span class="mi">13077</span>    <span class="mi">16</span>    <span class="mi">70</span>     <span class="mi">4</span>   <span class="mi">110</span>   <span class="mi">215</span>    <span class="mi">19</span>   <span class="mi">157</span>   <span class="mi">100</span>   <span class="mi">609</span>
     <span class="mi">2</span>  <span class="mi">1013</span>     <span class="mi">5</span>     <span class="mi">1</span>   <span class="mi">500</span>    <span class="mi">55</span>   <span class="mi">322</span>  <span class="mi">3987</span>    <span class="mi">22</span>   <span class="mi">242</span>     <span class="mi">4</span>  <span class="mi">3852</span>
   <span class="mi">690</span>    <span class="mi">14</span>  <span class="mi">2207</span>    <span class="mi">16</span>    <span class="mi">12</span>  <span class="mi">2227</span>    <span class="mi">13</span>    <span class="mi">32</span>     <span class="mi">8</span>    <span class="mi">32</span>   <span class="mi">450</span>   <span class="mi">129</span>
    <span class="mi">11</span>     <span class="mi">7</span>     <span class="mi">4</span>    <span class="mi">84</span>    <span class="mi">18</span>    <span class="mi">16</span>   <span class="mi">322</span>     <span class="mi">5</span>    <span class="mi">98</span>   <span class="mi">588</span>    <span class="mi">29</span>   <span class="mi">172</span>
  <span class="mi">1319</span>  <span class="mi">2224</span>     <span class="mi">6</span>   <span class="mi">381</span>    <span class="mi">99</span>   <span class="mi">104</span>    <span class="mi">10</span>   <span class="mi">328</span>    <span class="mi">22</span>     <span class="mi">6</span>    <span class="mi">28</span>  <span class="mi">2012</span>
  <span class="mi">2677</span>    <span class="mi">19</span>   <span class="mi">193</span>    <span class="mi">66</span>     <span class="mi">6</span>  <span class="mi">1810</span>    <span class="mi">58</span>     <span class="mi">3</span>   <span class="mi">460</span>   <span class="mi">127</span>     <span class="mi">2</span>   <span class="mi">247</span>
   <span class="mi">301</span>     <span class="mi">4</span>   <span class="mi">163</span>    <span class="mi">93</span>    <span class="mi">12</span>    <span class="mi">67</span>   <span class="mi">324</span>     <span class="mi">1</span>    <span class="mi">72</span>   <span class="mi">848</span>    <span class="mi">19</span>   <span class="mi">321</span>
  <span class="mi">2224</span>     <span class="mi">6</span>   <span class="mi">544</span>     <span class="mi">2</span>   <span class="mi">698</span>   <span class="mi">301</span>    <span class="mi">11</span>    <span class="mi">29</span>   <span class="mi">450</span>   <span class="mi">129</span>  <span class="mi">1245</span>   <span class="mi">183</span>
   <span class="mi">574</span>   <span class="mi">149</span>    <span class="mi">23</span>   <span class="mi">225</span>   <span class="mi">158</span>    <span class="mi">12</span>    <span class="mi">23</span>   <span class="mi">341</span>     <span class="mi">9</span>   <span class="mi">100</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>
     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span>     <span class="mi">0</span><span class="p">],</span> 
<span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">600</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">int64</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id7">
<h2><span class="section-number">12.3. </span>단어 임베딩<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>단어 임베딩<font size='2'>word embedding</font>은
텍스트 벡터화를 통해 생성된 어휘집의 단어가 문맥에 따라 가질 수 있는 여러 의미와
다른 단어들과의 연관성 정보를 표현하는 부동소수점들로 구성된 벡터로 변환하는 과정이다.</p>
<p><strong><code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층</strong></p>
<p>단어 임베딩은 모델 훈련을 통해 학습되도록 유도한다.
이유는 언어 종류, 텍스트 내용, 모델 훈련 목적에 따라
단어들 사이의 관계가 다르게 학습되어야 하기 때문이다.
예를 들어, 영화 후기에 포함된 텍스트와 재판 판결문 텍스트는
단어의 종류뿐만 아니라 동일한 단어의 의미와 용도까지
많이 다를 수 있기에 당연히 단어 임베딩 또한
달라져야 한다.</p>
<p>케라스의 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층이 훈련셋에 따라 다른 단어 임베딩이 학습되도록 하는 기능을 제공한다.
<code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층을 선언하는 방식은 아래 코드와 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">input_dim</span></code>: 총 어휘 수. 여기서는 7 개.</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">output_dim</span></code>: 단어 임베딩에 사용되는 벡터의 크기. 여기서는 단어별로 4 개의 특성을 찾도록 유도함.</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">7</span> <span class="c1"># 총 어휘 수</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">4</span>  <span class="c1"># 단어별로 4 개의 특성 파악.</span>

<span class="n">embedding_layer</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">vocab_size</span><span class="p">,</span> 
                                            <span class="n">output_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
</pre></div>
</div>
<p>위와 같이 선언된 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층은 (7, 4) 모양의 2차원 텐서로 구성되며
모델 훈련이 시작될 때 균등 분포를 이용하여 초기화된다.
이후 훈련을 통해 <code class="docutils literal notranslate"><span class="pre">7x4</span></code> 개의 항목이 훈련 파라미터로써 학습된다.</p>
<p>아래 그림은 앞서 정의한 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층의 작동 과정을 보여준다.
예를 들어 6개의 단어로 구성된 “The mouse ran up the clock” 텍스트를 단어 임베딩하는 과정은 다음과 같다.</p>
<ul class="simple">
<li><p>먼저 단어들의 정수 색인으로 구성된 길이가 6인 벡터인 <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">1,</span> <span class="pre">5]</span></code>로 변환한다.</p></li>
<li><p>단어 임베딩을 통해 4 개의 부동소수점을 포함한 벡터 6 개로 구성된 (6, 4) 모양의 텐서로 변환된다.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-EmbeddingLayer.png" style="width:95%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://developers.google.com/machine-learning/guides/text-classification/step-3?hl=ko">머신러닝 가이드</a>&gt;</div></p><p>위 그림에서의 단어 임베딩에 사용된 <code class="docutils literal notranslate"><span class="pre">(7,</span> <span class="pre">4)</span></code> 모양의 2차원 텐서는 다음과 같다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">embedding_matrix</span> <span class="o">=</span> <span class="p">[[</span><span class="o">-</span><span class="mf">0.012</span><span class="p">,</span> <span class="mf">0.005</span><span class="p">,</span> <span class="mf">0.008</span><span class="p">,</span> <span class="mf">0.001</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.236</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.141</span><span class="p">,</span> <span class="mf">0.000</span><span class="p">,</span> <span class="mf">0.045</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.006</span><span class="p">,</span> <span class="mf">0.652</span><span class="p">,</span> <span class="mf">0.270</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.556</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.305</span><span class="p">,</span> <span class="mf">0.569</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.028</span><span class="p">,</span> <span class="mf">0.496</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.421</span><span class="p">,</span> <span class="mf">0.195</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.058</span><span class="p">,</span> <span class="mf">0.477</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.844</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.001</span><span class="p">,</span> <span class="mf">0.763</span><span class="p">,</span> <span class="mf">0.201</span><span class="p">],</span>
                    <span class="p">[</span><span class="mf">0.466</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.326</span><span class="p">,</span> <span class="mf">0.884</span><span class="p">,</span> <span class="mf">0.007</span><span class="p">]]</span>
</pre></div>
</div>
<p>그리고 “The mouse ran up the clock” 벡터화된
<code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">4,</span> <span class="pre">1,</span> <span class="pre">5]</span></code> 가 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층에 입력되면
아래와 같이 위 <code class="docutils literal notranslate"><span class="pre">embedding_matrix</span></code>로부터
각각의 색인에 해당하는 벡터를 추출하여
<code class="docutils literal notranslate"><span class="pre">(6,</span> <span class="pre">4)</span></code> 모양의 출력 텐서를 생성한다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'the'</span>&#160;&#160; <span class="pre">=&gt;</span> <span class="pre">1</span> <span class="pre">=&gt;</span> <span class="pre">[0.236,</span> <span class="pre">-0.141,</span> <span class="pre">0.000,</span> <span class="pre">0.045]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'mouse'</span> <span class="pre">=&gt;</span> <span class="pre">2</span> <span class="pre">=&gt;</span> <span class="pre">[0.006,</span> <span class="pre">0.652,</span> <span class="pre">0.270,</span> <span class="pre">-0.556]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'ran'</span>&#160;&#160; <span class="pre">=&gt;</span> <span class="pre">3</span> <span class="pre">=&gt;</span> <span class="pre">[0.305,</span> <span class="pre">0.569,</span> <span class="pre">-0.028,</span> <span class="pre">0.496]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'up'</span>&#160;&#160;&#160; <span class="pre">=&gt;</span> <span class="pre">4</span> <span class="pre">=&gt;</span> <span class="pre">[0.421,</span> <span class="pre">0.195,</span> <span class="pre">-0.058,</span> <span class="pre">0.477]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'the'</span>&#160;&#160; <span class="pre">=&gt;</span> <span class="pre">1</span> <span class="pre">=&gt;</span> <span class="pre">[0.236,</span> <span class="pre">-0.141,</span> <span class="pre">0.000,</span> <span class="pre">0.045]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'clock'</span> <span class="pre">=&gt;</span> <span class="pre">5</span> <span class="pre">=&gt;</span> <span class="pre">[0.844,</span> <span class="pre">-0.001,</span> <span class="pre">0.763,</span> <span class="pre">0.201]</span></code></p></li>
</ul>
<p>반면에 “The mouse ran down”는 <code class="docutils literal notranslate"><span class="pre">[1,</span> <span class="pre">2,</span> <span class="pre">3,</span> <span class="pre">6]</span></code>으로 벡터화 되기에 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층을
통과할 때 <code class="docutils literal notranslate"><span class="pre">(4,</span> <span class="pre">4)</span></code> 모양으로 단어 임베딩된다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">'the'</span>&#160;&#160; <span class="pre">=&gt;</span> <span class="pre">1</span> <span class="pre">=&gt;</span> <span class="pre">[0.236,</span> <span class="pre">-0.141,</span> <span class="pre">0.000,</span> <span class="pre">0.045]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'mouse'</span> <span class="pre">=&gt;</span> <span class="pre">2</span> <span class="pre">=&gt;</span> <span class="pre">[0.006,</span> <span class="pre">0.652,</span> <span class="pre">0.270,</span> <span class="pre">-0.556]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'ran'</span>&#160;&#160; <span class="pre">=&gt;</span> <span class="pre">3</span> <span class="pre">=&gt;</span> <span class="pre">[0.305,</span> <span class="pre">0.569,</span> <span class="pre">-0.028,</span> <span class="pre">0.496]</span></code></p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">'down'</span>&#160; <span class="pre">=&gt;</span> <span class="pre">6</span> <span class="pre">=&gt;</span> <span class="pre">[0.466,</span> <span class="pre">-0.326,</span> <span class="pre">0.884,</span> <span class="pre">0.007]</span></code></p></li>
</ul>
<p><strong>단어 임베딩의 의미</strong></p>
<p>단어 임베딩의 결과로 생성된 벡터에 포함된
각각의 항목은 해당 단어가 가질 수 있는 다양한 의미를 가리킨다.
아래 그림은 3차원 벡터로 변환된 단어들의 예시를 담고 있다.</p>
<ul class="simple">
<li><p>왼쪽: 남성을 가리키는 용어와 여성을 가리키는 용어의 관계</p></li>
<li><p>가운데: 동사의 시제 관계</p></li>
<li><p>오른쪽: 국가와 국가의 수도 관계</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-WordEmbeddings-b.png" style="width:95%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://developers.google.com/machine-learning/guides/text-classification/step-3?hl=ko">머신러닝 가이드</a>&gt;</div></p><p><strong>단어 임베딩 활용법</strong></p>
<p>단어 임베딩은 입력층 다음에 위치시킨다.
그러면 어휘 인덱스로 구성된 훈련 배치 데이터셋에 대해
바로 단어 임베딩을 실행한다.
그 이후에 텍스트의 내용을 파악하는 훈련을 시작할 수 있다.</p>
<p>아래 코드에서 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층 바로 다음에 위치하는
<code class="docutils literal notranslate"><span class="pre">TransformerEncoder</span></code> 층이 텍스트의 내용을 파악한다.
그런 다음에 최종적으로 모델의 예측값을 <code class="docutils literal notranslate"><span class="pre">Dense</span></code> 층을 통해 결정한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">20000</span> <span class="c1"># 총 어휘 수</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">256</span>    <span class="c1"># 단어별로 256 개의 특성 파악.</span>

<span class="c1"># 입력층: 단어 벡터화된 배치 데이터셋</span>
<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="c1"># 단어 임베딩 실행</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>

<span class="c1"># 트랜스포머 인코더: 텍스트 내용 파악</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 출력층</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling1D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 모델 선언</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id8">
<h2><span class="section-number">12.4. </span>트랜스포머 아키텍처<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>앞서 언급한 트랜스포머 인코더는
2017년에 발표된 논문인 <a class="reference external" href="https://arxiv.org/abs/1706.03762">“Attention is all you need”</a>에서
<strong>트랜스포머</strong><font size='2'>Transformer</font> 아키텍처의 주요 구성 요소 중 하나로 소개되었다.
트랜스포머 아키텍처는 <strong>셀프 어텐션</strong><font size='2'>Self attention</font> 이라는
뉴럴 어텐션<font size='2'>neural attention</font> 기법을 이용하며 자연어처리 분야에서 혁명을 불러왔다.</p>
<section id="id9">
<h3><span class="section-number">12.4.1. </span>셀프 어텐션<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>입력값의 특성 중에 보다 중요한 특성에 <strong>집중</strong><font size='2'>attention</font>하면 보다 효율적으로
훈련이 진행될 수 있다.
아래 그림에서 볼 수 있듯이 이미지의 주요 부분에 집중하여 보다 효율적인 이미지 분석을 진행할 수 있다.
그림 왼쪽 하단에 위치한 어텐션 점수<font size='2'>attention score</font>는
위쪽 고양이 사진에 포함된 각각의 픽셀에 대한 가중치로 구성된다.
오른쪽 사진은 원본 이미지와 어텐션 점수를 곱한 결과로
고양이의 머리에만 집중한다.
이와같이 모든 입력 사진에 어텐션 점수를 계산하여 활용한다면
사진 분석 모델을 보다 효율적으로 훈련시킬 수 있다.</p>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-05.png" style="width:70%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p><strong>자연어 처리에서의 셀프 어텐션</strong></p>
<p><strong>셀프 어텐션</strong><font size='2'>self-attention</font>은 샘플의 어텐션 점수를 계산할 때
샘플 자신을 활용하는 기법이며 자연어 처리에서
텍스트에 포함된 단어들의 문맥상의 의미를 파악하기 위해 활용된다.</p>
<p>아래 그림은 “The train  left the station on time.” 이라는 텍스트를
먼저 단어 임베딩한 다음에 셀프 어텐션을 적용하는 과정을 보여준다.</p>
<ul class="simple">
<li><p>먼저 텍스트를 <code class="docutils literal notranslate"><span class="pre">Embedding</span></code> 층을 이용하여 단어 임베딩시킨다.</p></li>
<li><p>생성된 벡터 시퀀스에 셀프 어텐션을 적용하여 문맥이 적용된 동일 모양의 새로운 벡터 시퀀스를 생성한다.</p>
<ul>
<li><p>1단계: 텍스트에 사용된 각 토큰들 사이의 연관성을 계산하여 어텐션 점수 계산</p></li>
<li><p>2단계: 계산된 어텐션 점수를 시퀀스의 각 단어 벡터와 결합시킨 후 더해서 단어별로 문맥이 반명된 새로운 단어 벡터 생성</p></li>
</ul>
</li>
</ul>
<p>그림 맨 오른쪽에 생성된 벡터는 “station”이 “train” 단어와 연결되어 “기차역”을 가리킨다는
문맥상의 의미를 반영하여 변환된 벡터를 가리킨다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-self_attention.jpg" style="width:100%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p>아래 <code class="docutils literal notranslate"><span class="pre">self_attention()</span></code> 함수는 셀프 어텐션이 작동하는 과정을 설명하는 유사코드다.
함수의 입력값은 단어 임베딩된 시퀀스이다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">self_attention</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">):</span>
    <span class="c1"># 문맥이 반영된 시퀀스 저장</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">input_sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
    
    <span class="c1"># 단어 임베딩된 시퀀스의 단어 벡터 각각(pivor_vector)에 대해 </span>
    <span class="c1"># 셀프 어텐션 실행. 위 그림 참고.</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">pivot_vector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">):</span>

        <span class="c1"># pivot_vector와 다른 단어 벡터들 사이의 문맥 점수 계산</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">),))</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">):</span>
            <span class="n">scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">pivot_vector</span><span class="p">,</span> <span class="n">vector</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>

        <span class="c1"># 계산된 점수를 스케일링 후 소프트맥스 적용</span>
        <span class="n">scores</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">input_sequence</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">)</span>

        <span class="c1"># 문맥이 반영된 단어 벡터로 변환</span>
        <span class="c1"># 각 단어 벡터와 점수를 곱한 결과를 더함.</span>
        <span class="n">new_pivot_representation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="n">pivot_vector</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">j</span><span class="p">,</span> <span class="n">vector</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">input_sequence</span><span class="p">):</span>
            <span class="n">new_pivot_representation</span> <span class="o">+=</span> <span class="n">vector</span> <span class="o">*</span> <span class="n">scores</span><span class="p">[</span><span class="n">j</span><span class="p">]</span>
        
        <span class="c1"># pivot_vector를 변환한 벡터 저장</span>
        <span class="n">output</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">new_pivot_representation</span>

    <span class="k">return</span> <span class="n">output</span>
</pre></div>
</div>
<p>케라스에서는 셀프 어텐션 기능을 <code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code> 층이 지원한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">num_heads</span> <span class="o">=</span> <span class="mi">2</span> <span class="c1"># 두 개의 셀프 어텐션 동시 진행. 각각 다른 문맥을 파악.</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">256</span> <span class="c1"># 단어 임베딩된 벡터의 길이</span>

<span class="n">mha_layer</span> <span class="o">=</span> <span class="n">MultiHeadAttention</span><span class="p">(</span><span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
<span class="n">outputs</span> <span class="o">=</span> <span class="n">mha_layer</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>질문-열쇠-값<font size='2'>query-key-value</font></strong></p>
<p>동일한 값을 인자로 세 번 사용하는 <code class="docutils literal notranslate"><span class="pre">mha_layer(inputs,</span> <span class="pre">inputs,</span> <span class="pre">inputs)</span></code>, 즉 셀프 어텐션의 작동 과정을
식으로 표현하면 다음과 같다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">inputs</span> <span class="o">*</span> <span class="n">pairwise_scores</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">))</span>
</pre></div>
</div>
<p>위 식은 원래 검색 엔진 또는 추천 시스템 등에서
질문-열쇠-값<font size='2'>query-key-value</font>
세 개의 입력값을 받는 보다 일반화된 어텐션의 작동과정을 표현한 식의 특별한 경우이다.</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">outputs</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">values</span> <span class="o">*</span> <span class="n">pairwise_scores</span><span class="p">(</span><span class="n">query</span><span class="p">,</span> <span class="n">keys</span><span class="p">))</span>
</pre></div>
</div>
<p>예를 들어, 아래 그림은 “dogs on the beach.”라는 <strong>질문</strong><font size='2'>query</font>에 가장
적절한 사진을 검색할 때 각 사진과의 <strong>핵심 연관성</strong><font size='2'>key</font> 점수를 계산한 후
<strong>해당 사진</strong><font size='2'>value</font>과 결합하여 가장 높은 점수를 갖는 사진을
추천하는 과정을 보여준다.</p>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-07.png" style="width:80%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p>셀프 어텐션은 질문, 열쇠, 값 모두 동일한 값을 사용하여 주어진 대상이 자신을 비교 대상으로
활용하여 스스로 어느 부분에 집중할지를 판단하는 과정으로 이해할 수 있다.</p>
</section>
<section id="id10">
<h3><span class="section-number">12.4.2. </span>멀티헤드 어텐션<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p><strong>멀티헤드 어텐션</strong><font size='2'>multi-head attention</font>은
어텐션 변환을 수행하는 <strong>헤드</strong><font size='2'>head</font>를 여러 개 사용해서
다양한 관점에서 단어들 사이의 연관성을 알아낸 후에 알아낸 결과를 합치는 기법이다.
아래 그림은 두 개의 헤드가 작동하는 과정을 보여준다.
각각의 헤드에 별도의 밀집층이 사용됨에 주목한다.
질문, 열쇠, 값을 어텐션 층에 넣어 주기 전에 먼저 밀집층을 이용하여
모델 스스로 질문, 열쇠, 값을 적절하게 변환하도록 유도한다.</p>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-08.png" style="width:70%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p>두 개의 헤드를 사용하는 경우 입력 텐서를 아래처럼 각 단어의 특성을 이등분하여 각각의 헤드의 입력값으로 지정한다.</p>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-multiheadattention01.png" style="width:70%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://towardsdatascience.com/transformers-explained-visually-part-3-multi-head-attention-deep-dive-1c1ff1024853">Transformers Explained Visually</a>&gt;</div></p><p>예를 들어 <code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">256)</span></code> 으로 단어 임베딩된 텍스트가 두 개의 헤드를 사용하는 <code class="docutils literal notranslate"><span class="pre">MultiHeadAtention</span></code>에 입력되면
<code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">128)</span></code> 모양의 두 개의 입력 텐서로 쪼개져서 각각의 헤드에서 셀프 어텐션이 적용되어
<code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">128)</span></code> 모양의 출력 텐서로 변환된다.
이후에 두 출력 텐서를 다시 하나로 합쳐서 <code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">256)</span></code> 모양의 텐서가 <code class="docutils literal notranslate"><span class="pre">MultiHeadAtention</span></code>의 최종 출력 텐서가 된다.</p>
</section>
<section id="id11">
<h3><span class="section-number">12.4.3. </span>트랜스포머 인코더<a class="headerlink" href="#id11" title="Link to this heading">#</a></h3>
<p>헤드에 사용된 밀집층의 역할이 큰 것으로 밝혀지면서
멀티헤드 어텐션 층, 밀집<font size='2'>dense</font> 층, 정규화 층, 잔차 연결을
조합한 <strong>트랜스포머 인코더</strong><font size='2'>transformer encoder</font>가 완성되었다.
정규화 층으로 사용되는 <code class="docutils literal notranslate"><span class="pre">LayerNormalization</span></code>은 배치 단위가 아닌 시퀀스 단위로 정규화를 실행하며
<code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code>과는 다르다.</p>
<p>트랜스포머 인코더에 포함된 <code class="docutils literal notranslate"><span class="pre">LayerNormalization</span></code>은
정규화를 배치 단위가 아닌 시퀀스 단위로 정규화를 실행하는 층이며
<code class="docutils literal notranslate"><span class="pre">BatchNormalization</span></code>과는 다르게 작동한다.</p>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-09.png" style="width:35%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p>트랜스포머 인코더는 트랜스포머 아키텍처의 두 구성 요소중의 하나이다.
다른 요소는 아래 그림의 오른쪽에 위치한 트랜스포머 디코더<font size='2'>transformer decoder</font>이다.
트랜스포머 아키텍처는 언어 번역 등 텍스트를 텍스트로 변환하는 모델로 가장 많이 활용된다.
트랜스포머 디코더에 대해서는 아래에서 자세히 다룬다.</p>
<p><strong>트랜스포머 인코더 구현</strong></p>
<p>위 그림에서 설명된 트랜스포머 인코더를 층으로 구현하면 다음과 같다.
생성자의 인자는 다음과 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>: 예를 들어 <code class="docutils literal notranslate"><span class="pre">embed_dim=256</span></code>은 단어 임베딩 <code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">256)</span></code> 모양의 샘플 생성</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_dim</span></code>: 밀집 층에서 사용되는 유닛<font size='2'>unit</font> 개수</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span></code>: 헤드<font size='2'>head</font> 개수</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">call()</span></code> 메서드의 <code class="docutils literal notranslate"><span class="pre">attention_mask</span></code> 옵션은 <code class="docutils literal notranslate"><span class="pre">MultiHeadAttention</span></code> 층을 호출할 때
사용되며 질문<font size='2'>query</font>에 들어온 입력값의 특정 위치를 무시할 필요가 있을 때 사용한다.
하지만 여기서는 굳이 사용하지 않는다.</p>
<p>어텐션 층은 원래 query-key-value에 해당하는 세 개의 인자를 요구하지만
query와 key 두 개의 인자만 지정하면 key 인자를 value 인자로 함께 사용한다.
아래 코드에서는 query와 key 모두 동일한 입력값을 사용하기에 셀프-어텐션 기능으로 작동한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerEncoder</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>  <span class="c1"># 예를 들어 256</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span> <span class="o">=</span> <span class="n">dense_dim</span>  <span class="c1"># 예를 들어 32</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>  <span class="c1"># 예를 들어 2</span>
        
        <span class="c1"># 어텐션 층 지정</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="c1"># 밀집층 블록 지정</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_proj</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
             <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">),]</span>
        <span class="p">)</span>
        <span class="c1"># 층 정규화 지정</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()</span>

    <span class="c1"># 트랜스포머 인코더의 순전파</span>
    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">mask</span> <span class="o">=</span> <span class="n">mask</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">attention_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">inputs</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">mask</span><span class="p">)</span>
        <span class="n">proj_input</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">attention_output</span><span class="p">)</span>
        <span class="n">proj_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_proj</span><span class="p">(</span><span class="n">proj_input</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_2</span><span class="p">(</span><span class="n">proj_input</span> <span class="o">+</span> <span class="n">proj_output</span><span class="p">)</span>

    <span class="c1"># 트랜스포머 인코더의 속성 지정</span>
    <span class="c1"># 훈련된 모델을 저장할 때 활용됨</span>
    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="s2">&quot;num_heads&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="s2">&quot;dense_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">,</span>
        <span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>
</div>
<p><strong>트랜스포머 인코더 활용</strong></p>
<p>단어 벡터화된 훈련셋이 입력되면 먼저 단어 임베딩을 통과시켜 하나의 단어가 가질 수 있는 일반적인 특성을 찾는다.
이후 트랜스포머 인코더로 셀프 어텐션을 적용하여 단어가 사용되는 텍스트에서의 문맥상의 특성을 추가한다.</p>
<p>사용되는 변수들은 다음과 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span> <span class="pre">=</span> <span class="pre">20000</span></code>: 어휘 색인 크기</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span> <span class="pre">=</span> <span class="pre">256</span></code>: 단어 임베딩 특성 수</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span> <span class="pre">=</span> <span class="pre">2</span></code>: 트랜스포머 인코더에 사용되는 밀집층의 헤드(head) 수</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_dim</span> <span class="pre">=</span> <span class="pre">32</span></code>: 트랜스포머 인코더에 사용되는 밀집층의 유닛(unit) 수</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># (600, 256) 모양의 단어 임베딩된 텍스트 텐서를</span>
<span class="c1"># 길이가 256인 1차원 어레이로 변환.</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling1D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<div class="info admonition">
<p class="admonition-title">GlobalMaxPooling1D</p>
<p>벡터(1차원 텐서)로 구성된 시퀀스가 <code class="docutils literal notranslate"><span class="pre">GlobalMaxPooling1D</span></code> 층을 통과하면 벡터의 특성별로 최댓값만 추출해서 사용한다.
따라서 벡터의 길이에 해당하는 하나의 벡터가 생성된다.</p>
<p>예를 들어, (4, 2, 3) 모양의 텐서, 즉, 길이가 3인 두 개의 벡터로 구성된 시퀀스 네 개를 묶은 배치가 <code class="docutils literal notranslate"><span class="pre">GlobalMaxPooling1D</span></code> 층을
통과해서 (4, 3) 모양의 텐서가 생성된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">(</span>
<span class="gp">... </span>      <span class="p">[[[</span> <span class="mf">1.</span><span class="p">,</span>  <span class="mf">5.</span><span class="p">,</span>  <span class="mf">3.</span><span class="p">],</span>
<span class="gp">... </span>        <span class="p">[</span> <span class="mf">4.</span><span class="p">,</span>  <span class="mf">2.</span><span class="p">,</span>  <span class="mf">6.</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span> <span class="mf">7.</span><span class="p">,</span>  <span class="mf">8.</span><span class="p">,</span>  <span class="mf">12.</span><span class="p">],</span>
<span class="gp">... </span>        <span class="p">[</span><span class="mf">10.</span><span class="p">,</span> <span class="mf">11.</span><span class="p">,</span> <span class="mf">9.</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="mf">16.</span><span class="p">,</span> <span class="mf">14.</span><span class="p">,</span> <span class="mf">15.</span><span class="p">],</span>
<span class="gp">... </span>        <span class="p">[</span><span class="mf">13.</span><span class="p">,</span> <span class="mf">17.</span><span class="p">,</span> <span class="mf">18.</span><span class="p">]],</span>
<span class="gp">... </span>       <span class="p">[[</span><span class="mf">19.</span><span class="p">,</span> <span class="mf">20.</span><span class="p">,</span> <span class="mf">21.</span><span class="p">],</span>
<span class="gp">... </span>        <span class="p">[</span><span class="mf">22.</span><span class="p">,</span> <span class="mf">23.</span><span class="p">,</span> <span class="mf">24.</span><span class="p">]]])</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">max_pool_1d</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">keras</span><span class="o">.</span><span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling1D</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">max_pool_1d</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="go">&lt;tf.Tensor: shape=(4, 3), dtype=float32, numpy=</span>
<span class="go">array([[ 4.,  5.,  6.],</span>
<span class="go">       [10., 11., 12.],</span>
<span class="go">       [16., 17., 18.],</span>
<span class="go">       [22., 23., 24.]], dtype=float32)&gt;</span>
</pre></div>
</div>
</div>
<p>앞서 살펴 본 모델에서 사용된 단어 임베딩은
단어순서를 제대로 활용하지는 못한다.
그런데 단어 인코딩 과정에서 단어의 순서 정보까지 추가하면
트랜스포머 인코더에게 단어들의 기능과 문맥을
보다 잘 추출하는 데에 도움이 된다.</p>
<p>다음 <code class="docutils literal notranslate"><span class="pre">PositionalEmbedding</span></code> 층 클래스는 두 개의 임베딩 클래스를 순전파에 사용한다.</p>
<ul>
<li><p>단어 임베딩</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> 
                                         <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
</pre></div>
</div>
</li>
<li><p>위치 임베딩</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span><span class="n">input_dim</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span> 
                                            <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
</pre></div>
</div>
</li>
</ul>
<p>순전파를 담당하는 <code class="docutils literal notranslate"><span class="pre">call()</span></code> 메서드가 호출되면
<code class="docutils literal notranslate"><span class="pre">self.token_embeddings</span></code>는 단어 임베딩을,
<code class="docutils literal notranslate"><span class="pre">self.position_embeddings</span></code>는 단어의 위치 정보 임베딩을 수행한다.
최종적으로 각 임베딩의 출력값을 합친 값이 반환된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">PositionalEmbedding</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">,</span> <span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">input_dim</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
            <span class="n">input_dim</span><span class="o">=</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">output_dim</span><span class="o">=</span><span class="n">output_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="n">sequence_length</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span> <span class="o">=</span> <span class="n">input_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span> <span class="o">=</span> <span class="n">output_dim</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 텍스트의 단어 수. 예를 들어 600.</span>
        <span class="n">positions</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">start</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">limit</span><span class="o">=</span><span class="n">length</span><span class="p">,</span> <span class="n">delta</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span> <span class="c1"># range(600)에 해당</span>
        <span class="n">embedded_tokens</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">token_embeddings</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">embedded_positions</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">position_embeddings</span><span class="p">(</span><span class="n">positions</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">embedded_tokens</span> <span class="o">+</span> <span class="n">embedded_positions</span> <span class="c1"># 단어 임베딩 + 위치 임베딩</span>

    <span class="k">def</span> <span class="nf">compute_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">math</span><span class="o">.</span><span class="n">not_equal</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="mi">0</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="s2">&quot;output_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">output_dim</span><span class="p">,</span>
            <span class="s2">&quot;sequence_length&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">sequence_length</span><span class="p">,</span>
            <span class="s2">&quot;input_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">input_dim</span><span class="p">,</span>
        <span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span>
</pre></div>
</div>
<p><strong>단어위치 인식 트랜스포머 인코더 활용</strong></p>
<p>영화후기 분석등 이진분류 모델로 사용될 수 있는 모델을
<code class="docutils literal notranslate"><span class="pre">PositionalEmbedding</span></code> 층과 트랜스포머 인코더를 이용하여
구성하면 다음과 같다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">20000</span>
<span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">600</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">256</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dense_dim</span> <span class="o">=</span> <span class="mi">32</span>

<span class="n">inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)(</span><span class="n">inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">GlobalMaxPooling1D</span><span class="p">()(</span><span class="n">x</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;sigmoid&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>모델 컴파일과 훈련 진행은 별다른 점이 없다.</p>
<ul class="simple">
<li><p>모델 컴파일</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span><span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">,</span>
              <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;binary_crossentropy&quot;</span><span class="p">,</span>
              <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
<span class="n">model</span><span class="o">.</span><span class="n">summary</span><span class="p">()</span>
</pre></div>
</div>
<ul class="simple">
<li><p>모델 훈련</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
    <span class="n">keras</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span><span class="s2">&quot;full_transformer_encoder&quot;</span><span class="p">,</span>
                                    <span class="n">save_best_only</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">]</span>

<span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">int_train_ds</span><span class="p">,</span> <span class="n">validation_data</span><span class="o">=</span><span class="n">int_val_ds</span><span class="p">,</span> <span class="n">epochs</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">)</span>
</pre></div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">keras.models</span></code> 모듈의 <code class="docutils literal notranslate"><span class="pre">load_model()</span></code> 함수를 이용하여
훈련과정 중에 저장된 최적의 모델을 불러올 때는
모델 구성에 사용된 층중에서 사용자가 직접 정의한 층의
클래스를 <code class="docutils literal notranslate"><span class="pre">custom_objects</span></code> 인자로 지정해야 함에 주의한다.</p>
<ul class="simple">
<li><p>최고 성능 모델의 정확도</p></li>
</ul>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">models</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span>
    <span class="s2">&quot;full_transformer_encoder&quot;</span><span class="p">,</span>
    <span class="n">custom_objects</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;TransformerEncoder&quot;</span><span class="p">:</span> <span class="n">TransformerEncoder</span><span class="p">,</span>
                    <span class="s2">&quot;PositionalEmbedding&quot;</span><span class="p">:</span> <span class="n">PositionalEmbedding</span><span class="p">})</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Test acc: </span><span class="si">{</span><span class="n">model</span><span class="o">.</span><span class="n">evaluate</span><span class="p">(</span><span class="n">int_test_ds</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span><span class="si">:</span><span class="s2">.3f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</section>
</section>
<section id="id12">
<h2><span class="section-number">12.5. </span>시퀀스-투-시퀀스 학습<a class="headerlink" href="#id12" title="Link to this heading">#</a></h2>
<p>기계 번역에 사용되는 시퀀스-투-시퀀스 모델은 하나의 텍스트가 들어오면 새로운 텍스트를 생성하는 모델이며,
자연어 처리의 다양한 분야에서 활용된다.</p>
<ul class="simple">
<li><p>기계 번역</p></li>
<li><p>텍스트 내용 요약</p></li>
<li><p>질문 답변</p></li>
<li><p>챗봇 기능</p></li>
<li><p>텍스트 생성</p></li>
</ul>
<section id="id13">
<h3><span class="section-number">12.5.1. </span>트랜스포머 모델<a class="headerlink" href="#id13" title="Link to this heading">#</a></h3>
<p>기계 번역에 사용되는 트랜스포머 모델은 트랜스포머 인코더와 트랜스포머 디코더로 구성된
아래 구조의 아키첵처를 활용한다.</p>
<ul class="simple">
<li><p>그림 왼쪽: 트랜스포머 인코더</p></li>
<li><p>그림 오른쪽: 트랜스포머 디코더</p></li>
</ul>
<div align="center"><img src="https://drek4537l1klr.cloudfront.net/chollet2/Figures/11-14.png" style="width:75%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p><strong>텍스트 데이터셋</strong></p>
<p>예를 들어 영어 텍스트를 스페인어 텍스트로 기계 번역하는 트랜스포머 모델의
훈련에 사용되는 데이터셋은 영어 텍스트 데이터셋과 스페인어 텍스트 데이터셋으로 구성된
튜플을 사용한다.
아래 표는 각각 3 개의 영어 텍스트와 스페인어 텍스트로 구성된 데이터셋 예제이다.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>영어 데이터셋</p></th>
<th class="head text-left"><p>스페인어 데이터셋</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p>How is the weather today?</p></td>
<td class="text-left"><p>¿Qué tiempo hace hoy?</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>You are welcome.</p></td>
<td class="text-left"><p>De nada.</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>It will be raining at this time tomorrow.</p></td>
<td class="text-left"><p>Mañana a estas horas estará lloviendo.</p></td>
</tr>
</tbody>
</table>
</div>
<p>기계 번역 모델의 훈련에 사용되는 데이터셋은 <a class="reference external" href="https://www.manythings.org/anki/">Tab-delimited Bilingual Sentence Pairs</a>에서 다운로드 받는다.
영어-스페인어 텍스트 데이터셋은 <a class="reference external" href="https://www.manythings.org/anki/spa-eng.zip">spa-eng.zip</a>이다.
다른 언어 데이터셋도 제공된다.
예를 들어 <a class="reference external" href="https://www.manythings.org/anki/kor-eng.zip">kor-eng.zip</a>은 영어-한국에 텍스트 데이터셋이다.</p>
<p><strong>훈련셋</strong></p>
<p>훈련은 하지만 텍스트의 길이를 일정 단어의 수로 제한한다.
아래 설명에서는 색인화된 텍스트의 길이를 5로 지정하였다.
또한 영어 텍스트와 스페인어 텍스트 모두
어휘 인덱스로 구성된 벡터로 텍스트 벡터화되었다고 가정한다.
단어수가 지정된 크기보다 적은 경우 0-패딩을 사용한다.
또한 스페인어 텍스트의 시작은 <code class="docutils literal notranslate"><span class="pre">'[start]'</span></code>로, 끝은 <code class="docutils literal notranslate"><span class="pre">'[end]'</span></code>로 지정하여
텍스트의 시작과 끝을 모델이 인지하도록 훈련시킨다.
단, 두 단어 포함 단어수가 지정된 크기보다 크면 <code class="docutils literal notranslate"><span class="pre">'[end]'</span></code>는 생략된다.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>영어 훈련셋</p></th>
<th class="head text-left"><p>스페인어 훈련셋</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['how',</span> <span class="pre">'is',</span> <span class="pre">'the',</span> <span class="pre">'weather',</span> <span class="pre">'today']</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['[start]',</span> <span class="pre">'qué',</span> <span class="pre">'tiempo',</span> <span class="pre">'hace',</span> <span class="pre">'hoy']</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['you',</span> <span class="pre">'are',</span> <span class="pre">'welcome',</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['[start]',</span> <span class="pre">'de',</span> <span class="pre">'nada',</span> <span class="pre">'[end]',</span> <span class="pre">0]</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['it',</span> <span class="pre">'will',</span> <span class="pre">'be',</span> <span class="pre">'raining',</span> <span class="pre">'at']</span></code></p></td>
<td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['[start]',</span> <span class="pre">'mañana',</span> <span class="pre">'a',</span> <span class="pre">'estas',</span> <span class="pre">'horas']</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p><strong>타깃셋</strong></p>
<p>타깃셋은 스페인어 훈련셋으로부터 생성된다.
단어수는 동일하다.
하지만 <code class="docutils literal notranslate"><span class="pre">[start]</span></code>가 제거되고 대신 입력 텍스트를 계속 이어갈 단어 하나가 마지막 단어로 추가된다.
즉, 모델은 훈련을 통해 입력된 스페인어 텍스트를 어떤 단어로 이어갈 것인지를 학습한다.</p>
<div class="pst-scrollable-table-container"><table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>스페인어 타깃셋</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['qué',</span> <span class="pre">'tiempo',</span> <span class="pre">'hace',</span> <span class="pre">'hoy',</span> <span class="pre">'[end]']</span></code></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['de',</span> <span class="pre">'nada',</span> <span class="pre">'[end]',</span> <span class="pre">0,</span> <span class="pre">0]</span></code></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><code class="docutils literal notranslate"><span class="pre">['mañana',</span> <span class="pre">'a',</span> <span class="pre">'estas',</span> <span class="pre">'horas',</span> <span class="pre">'estará']</span></code></p></td>
</tr>
</tbody>
</table>
</div>
<p>아래 그림은 영어와 스페인어 입력값이 사용되는 과정을 보여준다.
영어는 트랜스포머 인코더에 입력되고 그 출력값이 스페인어 입력값과 함께 트랜스포머 디코더에 입력된다.
모델의 최종 출력값은 트랜스포머 디코더가 생성한다.</p>
<ul class="simple">
<li><p>트랜스포머 인코더</p>
<ul>
<li><p>입력값: 영어 텍스트 배치 데이터셋</p></li>
<li><p>출력값: 셀프 어텐션을 이용하여 단어 임베딩된 배치 데이터셋</p></li>
</ul>
</li>
<li><p>트랜스포머 디코더</p>
<ul>
<li><p>입력값: 스페인어 텍스트 배치 데이터셋과 트랜스포머 인코더의 출력값</p></li>
<li><p>출력값: 두 입력값을 어텐션 층을 통과시켜 생성한 스페인어 텍스트 배치 데이터셋.
query는 스페인어 입력값, key와 value는 트랜스포머 인코더의 영어 출력값.</p></li>
</ul>
</li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-transformer-02.png" style="width:70%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p><strong>트랜스포머 모델 활용</strong></p>
<p>트랜스포머 모델은 실전에서는 입력된 영어 텍스트를 번역한 스페인어 텍스트를 생성하는 데에 활용된다.
앞서 설명한 훈련 과정에서와는 달리 입력값으로 영어 텍스트만 주어진 상황에서 스페인어 텍스트를 생성해야 한다.
즉, 모델의 둘째 입력값인 스페인어 텍스트가 없이 모델의 예측값을 생성해야 한다.
하지만 위 트랜스포머 모델은 두 종류의 입력값을 요구한다.
이에 대한 해결책은 다음과 같다.</p>
<ul class="simple">
<li><p>아래 그림에서처럼 <code class="docutils literal notranslate"><span class="pre">'[start]'</span></code> 단어 한 개로 구성된 스페인어 텍스트를 둘째 입력값으로 지정한다.
그러면 모델은 입력된 영어 텍스트를 참고하여 해당 텍스트를 스페인어로 번역한다.
이때 가장 마지막에 사용된 스페인어 단어가 입력 스페인어 텍스트를 연잘할 때 사용할 수 있는 단어다.
예를 들어, 아래 그림은 <code class="docutils literal notranslate"><span class="pre">&quot;how</span> <span class="pre">is</span> <span class="pre">the</span> <span class="pre">weather</span> <span class="pre">today&quot;</span></code> 텍스트를 번역할 때 스페인어 텍스트의 첫째
단어로 <code class="docutils literal notranslate"><span class="pre">'qué'</span></code> 를 추천한다.</p></li>
<li><p>생성된 텍스트의 마지막 단어를 기존에 입력값으로 사용된 스페인어 텍스트 문장에 추가한 후에 새로운 스페인어 입력값으로 사용하여
트랜스포머 모델로 하여금 입력된 스페인어 문장을 이어갈 새로운 단어를 추천하도록 한다.
이 과정을 <code class="docutils literal notranslate"><span class="pre">'[end]'</span></code> 단어가 추천될 때까지 반복한다.</p></li>
</ul>
<div align="center"><img src="https://raw.githubusercontent.com/codingalzi/dlp2/master/jupyter-book/imgs/ch11-transformer-03.png" style="width:70%;"></div>
<p><div style="text-align: center">&lt;그림 출처: <a href="https://www.manning.com/books/deep-learning-with-python-second-edition">Deep Learning with Python(2판)</a>&gt;</div></p><p>정리하면 다음과 같다. 영어 텍스트가 입력되면 <code class="docutils literal notranslate"><span class="pre">'[start]'</span></code>로만 구성된 텍스트를
스페인어 입력 텍스르로 사용해서 <code class="docutils literal notranslate"><span class="pre">'[end]'</span></code>가 이전 스페인어 텍스트를 이어갈 단어로 추천될 때까지
트랜스포머 모델을 반복 활용하며, 이 과정을 통해 하나의 완성된 스펜인어 텍스트를 생성한다.</p>
</section>
<section id="id14">
<h3><span class="section-number">12.5.2. </span>트랜스포머 디코더<a class="headerlink" href="#id14" title="Link to this heading">#</a></h3>
<p>트랜스포머 디코더를 하나의 층으로 구현하면 다음과 같다.
생성자의 인자는 다음과 같다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span></code>: 예를 들어 <code class="docutils literal notranslate"><span class="pre">embed_dim=256</span></code>은 단어 임베딩 <code class="docutils literal notranslate"><span class="pre">(600,</span> <span class="pre">256)</span></code> 모양의 샘플 생성</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_dim</span></code>: 밀집층에서 사용되는 유닛<font size='2'>unit</font> 개수</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span></code>: 헤드<font size='2'>head</font> 개수</p></li>
</ul>
<p><code class="docutils literal notranslate"><span class="pre">get_causal_attention_mask()</span></code> 메서드는 스페인어 입력 텍스트에 대한 마스크를 지정할 때 활용되지만
여기서는 마스크를 사용하지 않는다.</p>
<p>순전파를 담당하는 <code class="docutils literal notranslate"><span class="pre">call()</span></code> 메서드는 두 개의 어텐션 층을 사용한다.
입력값으로는 스페인어 텍스트 배치 데이터셋과
트랜스포머 디코더의 출력값으로 셀프 어텐션이 적용되어 변환된 영어 텍스트 배치 데이터셋이 사용된다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">attention_1</span></code>: 스페인어 텍스트 입력값에 대해 셀프 어텐션 적용</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">attention_2</span></code>: <code class="docutils literal notranslate"><span class="pre">attention_1</span></code> 의 출력값을 query로, 트랜스포머 인코더의 출력값을 key와 value로 사용해서 어텐션 적용.</p></li>
</ul>
<p>최종적으로 두 개의 밀집층을 통과시킨다.
또한 하나의 블록을 통과시킬 때마다 잔차연결과 층정규화를 진행한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">TransformerDecoder</span><span class="p">(</span><span class="n">layers</span><span class="o">.</span><span class="n">Layer</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span> <span class="o">=</span> <span class="n">embed_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span> <span class="o">=</span> <span class="n">dense_dim</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">attention_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">MultiHeadAttention</span><span class="p">(</span>
            <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span> <span class="n">key_dim</span><span class="o">=</span><span class="n">embed_dim</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dense_proj</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
            <span class="p">[</span><span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">dense_dim</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;relu&quot;</span><span class="p">),</span>
             <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">),]</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_3</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">LayerNormalization</span><span class="p">()</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">supports_masking</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="k">def</span> <span class="nf">get_config</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="n">config</span> <span class="o">=</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">get_config</span><span class="p">()</span>
        <span class="n">config</span><span class="o">.</span><span class="n">update</span><span class="p">({</span>
            <span class="s2">&quot;embed_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed_dim</span><span class="p">,</span>
            <span class="s2">&quot;num_heads&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="s2">&quot;dense_dim&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_dim</span><span class="p">,</span>
        <span class="p">})</span>
        <span class="k">return</span> <span class="n">config</span>

    <span class="k">def</span> <span class="nf">get_causal_attention_mask</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">):</span>
        <span class="n">input_shape</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">shape</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="n">batch_size</span><span class="p">,</span> <span class="n">sequence_length</span> <span class="o">=</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">i</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">]</span>
        <span class="n">j</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">range</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span><span class="n">i</span> <span class="o">&gt;=</span> <span class="n">j</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int32&quot;</span><span class="p">)</span>
        <span class="n">mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">input_shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span>
        <span class="n">mult</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">concat</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="n">expand_dims</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">),</span>
             <span class="n">tf</span><span class="o">.</span><span class="n">constant</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">int32</span><span class="p">)],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">tf</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">mask</span><span class="p">,</span> <span class="n">mult</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">call</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">,</span> <span class="n">mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
        <span class="c1"># 마스크 활용</span>
        <span class="n">causal_mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_causal_attention_mask</span><span class="p">(</span><span class="n">inputs</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">cast</span><span class="p">(</span>
                <span class="n">mask</span><span class="p">[:,</span> <span class="n">tf</span><span class="o">.</span><span class="n">newaxis</span><span class="p">,</span> <span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int32&quot;</span><span class="p">)</span>
            <span class="n">padding_mask</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">minimum</span><span class="p">(</span><span class="n">padding_mask</span><span class="p">,</span> <span class="n">causal_mask</span><span class="p">)</span>
            
        <span class="c1"># 셀프 어텐션 적용: 번역 언어(예를 들어 스페인어) 입력값 대상</span>
        <span class="n">attention_output_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_1</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">inputs</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">causal_mask</span><span class="p">)</span>
        <span class="n">attention_output_1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_1</span><span class="p">(</span><span class="n">inputs</span> <span class="o">+</span> <span class="n">attention_output_1</span><span class="p">)</span>
        <span class="c1"># 셀프 어텐션이 적용된 (예를 들어 스페인어) 입력 텍스트를 query로</span>
        <span class="c1"># 셀프 어텐션이 적용된 번역 대상 (예를 들어 영어) 입력 텍스트를 key와 value로</span>
        <span class="c1"># 지정하여 어텐션 적용</span>
        <span class="n">attention_output_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention_2</span><span class="p">(</span>
            <span class="n">query</span><span class="o">=</span><span class="n">attention_output_1</span><span class="p">,</span>
            <span class="n">value</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="n">key</span><span class="o">=</span><span class="n">encoder_outputs</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">padding_mask</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">attention_output_2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_2</span><span class="p">(</span>
            <span class="n">attention_output_1</span> <span class="o">+</span> <span class="n">attention_output_2</span><span class="p">)</span>
        
        <span class="n">proj_output</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dense_proj</span><span class="p">(</span><span class="n">attention_output_2</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm_3</span><span class="p">(</span><span class="n">attention_output_2</span> <span class="o">+</span> <span class="n">proj_output</span><span class="p">)</span>
</pre></div>
</div>
</section>
<section id="id15">
<h3><span class="section-number">12.5.3. </span>기계 번역 모델<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<p>이미 설명한 대로 트랜스포머 인코더와 트랜스포머 디코더를 조합하여
기계 번역 트랜스포머 모델을 구성한다.</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">sequence_length</span> <span class="pre">=</span> <span class="pre">20</span></code>: 텍스트에 포함되는 단어수를 20으로 지정</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">vocab_size</span> <span class="pre">=</span> <span class="pre">15000</span></code>: 어휘집 크기를 15,000으로 지정</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">embed_dim</span> <span class="pre">=</span> <span class="pre">256</span></code>: 단어 임베딩 크기</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">dense_dim</span> <span class="pre">=</span> <span class="pre">2048</span></code>: 밀집층에 사용되는 유닛 개수</p></li>
<li><p><code class="docutils literal notranslate"><span class="pre">num_heads</span> <span class="pre">=</span> <span class="pre">8</span></code>: 어텐션 모델에 사용될 헤드 개수</p></li>
</ul>
<p>모델의 입력값은 앞서 설명한 대로 예를 들어 일정 길이로 단어 벡터화된 영어 텍스트 데이터셋과
스페인어 텍스트 데이터셋의 튜플이다.
스페인어 텍스트는 모두 <code class="docutils literal notranslate"><span class="pre">[start]</span></code> 로 시작하도록 전처리되어 있다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">20</span> <span class="c1"># 텍스트의 단어수</span>
<span class="n">vocab_size</span> <span class="o">=</span> <span class="mi">15000</span> <span class="c1"># 어휘집 크기</span>
<span class="n">embed_dim</span> <span class="o">=</span> <span class="mi">256</span>    <span class="c1"># 단어 임베딩 크기</span>
<span class="n">dense_dim</span> <span class="o">=</span> <span class="mi">2048</span>   <span class="c1"># 밀집층 유닛수</span>
<span class="n">num_heads</span> <span class="o">=</span> <span class="mi">8</span>      <span class="c1"># 어텐션 헤드수</span>

<span class="c1"># 트랜스포머 인코더 활용</span>

<span class="c1"># 첫째 입력값: 예를 들어 영어 텍스트셋</span>
<span class="n">encoder_inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;english&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)(</span><span class="n">encoder_inputs</span><span class="p">)</span>
<span class="n">encoder_outputs</span> <span class="o">=</span> <span class="n">TransformerEncoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="c1"># 트랜스포머 디코더 활용</span>

<span class="c1"># 둘째 입력값: 예를 들어 스페인어 텍스트셋</span>
<span class="n">decoder_inputs</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Input</span><span class="p">(</span><span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="kc">None</span><span class="p">,),</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;int64&quot;</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;spanish&quot;</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">PositionalEmbedding</span><span class="p">(</span><span class="n">sequence_length</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">,</span> <span class="n">embed_dim</span><span class="p">)(</span><span class="n">decoder_inputs</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">TransformerDecoder</span><span class="p">(</span><span class="n">embed_dim</span><span class="p">,</span> <span class="n">dense_dim</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)(</span><span class="n">x</span><span class="p">,</span> <span class="n">encoder_outputs</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="mf">0.5</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>
<span class="n">decoder_outputs</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">Dense</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="n">activation</span><span class="o">=</span><span class="s2">&quot;softmax&quot;</span><span class="p">)(</span><span class="n">x</span><span class="p">)</span>

<span class="n">transformer</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">Model</span><span class="p">([</span><span class="n">encoder_inputs</span><span class="p">,</span> <span class="n">decoder_inputs</span><span class="p">],</span> <span class="n">decoder_outputs</span><span class="p">)</span>
</pre></div>
</div>
<p>모델의 출력값은 예를 들어 출력 스페인어 텍스트로 지정될 단어들에 대한 위치별 확률값을 계산한다.
아래 코드에서는 스페인어 텍스트에 포함될 20 개 단어들의 후보를 위치별로 확률값으로 계산한다.
예를 들어 출력 텍스트의 i-번 인덱스에 위치할 단어의 확률값을 계산하기 위해
어휘집에 포함된 15,000 개 단어를 대상으로 각각의 단어가 해당 위치에 자리할 확률을
소프트맥스 함수를 이용하여 계산한다.</p>
<p>예를 들어 크기가 64인 배치 입력에 대한 모델의 최종 출력값은 <code class="docutils literal notranslate"><span class="pre">(64,</span> <span class="pre">20,</span> <span class="pre">15000)</span></code> 모양의 텐서로 구성된다.
<code class="docutils literal notranslate"><span class="pre">(20,</span> <span class="pre">15000)</span></code>은 아래와 같은 20개의 단어로 구성된 하나의 스페인어 텍스트에 해당하는 텐서이며,
텐서의 i-번째 행은 15,000개 단어 각각을 대상으로 출력 텍스트의 i-번째 단어로 사용될
(어휘)인덱스의 확률값으로 구성된 벡터이다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">tf</span><span class="o">.</span><span class="n">Tensor</span><span class="p">(</span>
<span class="p">[[</span><span class="mf">5.5714343e-05</span><span class="p">,</span> <span class="mf">1.0322933e-04</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">6.9425951e-05</span><span class="p">],</span> <span class="c1"># 길이 15000, 열별 합: 1</span>
 <span class="p">[</span><span class="mf">5.5894801e-05</span><span class="p">,</span> <span class="mf">5.3953008e-05</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">6.7670873e-05</span><span class="p">],</span>
 <span class="p">[</span><span class="mf">5.9929422e-05</span><span class="p">,</span> <span class="mf">5.7787420e-05</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">5.9436632e-05</span><span class="p">],</span>
  <span class="o">...</span>
 <span class="p">[</span><span class="mf">5.9041427e-05</span><span class="p">,</span> <span class="mf">4.8372585e-05</span><span class="p">,</span> <span class="o">...</span><span class="p">,</span> <span class="mf">6.5162356e-05</span><span class="p">]],</span> <span class="c1"># 총 20개 단어 대상</span>
 <span class="n">shape</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span> <span class="mi">15000</span><span class="p">),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">float32</span><span class="p">)</span>
</pre></div>
</div>
<p><strong>모델 훈련과 활용</strong></p>
<p>모델 훈련은 특별할 게 없다.
다만 앞서 설명한대로 모델의 최종 출력값이 소프트맥스를 사용하여
<code class="docutils literal notranslate"><span class="pre">(20,</span> <span class="pre">15000)</span></code> 모양을 갖는 반면에
타깃셋은 20 개의 어휘 인덱스로 구성된 벡터로 구성되기에
<code class="docutils literal notranslate"><span class="pre">categorical_crossentropy</span></code> 가 아닌 <code class="docutils literal notranslate"><span class="pre">sparse_categorical_crossentropy</span></code>를
손실함수로 지정한다.
그러면 20개 단어 각각에 대해 가장 높은 확률을 갖는 (어휘) 인덱스에 해당하는 단어가
15,000 개 중에 선택되어 타깃 단어와 비교된다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">transformer</span><span class="o">.</span><span class="n">compile</span><span class="p">(</span>
    <span class="n">optimizer</span><span class="o">=</span><span class="s2">&quot;rmsprop&quot;</span><span class="p">,</span>
    <span class="n">loss</span><span class="o">=</span><span class="s2">&quot;sparse_categorical_crossentropy&quot;</span><span class="p">,</span>
    <span class="n">metrics</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;accuracy&quot;</span><span class="p">])</span>
</pre></div>
</div>
<p>아래 <code class="docutils literal notranslate"><span class="pre">decode_sequence()</span></code>는 함수는 영어 텍스트가 하나 입력되면
앞서 훈련된 트랜스포머 모델을 이용하여 지정된 길이인 20 개의 단어로
구성된 스페인어 텍스트를 생성한다.</p>
<p>함수 본문에 포함된 <code class="docutils literal notranslate"><span class="pre">for</span></code> 반복문은
<strong>트랜스포머 모델 활용</strong> 부분에서 설명한 방식 그대로
<code class="docutils literal notranslate"><span class="pre">[start]</span></code>로만 구성된 텍스트로 시작해서
계속해서 텍스트에 추가할 단어를 하나씩 선택해서 이어가는 과정을
<code class="docutils literal notranslate"><span class="pre">[end]</span></code> 키워드가 나올 때까지 반복한다.
단, 반복횟수는 20으로 제한한다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>

<span class="c1"># 어휘집 확인</span>
<span class="n">spa_vocab</span> <span class="o">=</span> <span class="n">target_vectorization</span><span class="o">.</span><span class="n">get_vocabulary</span><span class="p">()</span>
<span class="c1"># (단어 인덱스, 단어)로 구성된 사전 지정</span>
<span class="n">spa_index_lookup</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">spa_vocab</span><span class="p">)),</span> <span class="n">spa_vocab</span><span class="p">))</span>
<span class="c1"># 텍스트에 포함되는 단어수</span>
<span class="n">max_decoded_sentence_length</span> <span class="o">=</span> <span class="mi">20</span>

<span class="k">def</span> <span class="nf">decode_sequence</span><span class="p">(</span><span class="n">input_sentence</span><span class="p">):</span>
    <span class="n">tokenized_input_sentence</span> <span class="o">=</span> <span class="n">source_vectorization</span><span class="p">([</span><span class="n">input_sentence</span><span class="p">])</span>
    <span class="c1"># 기계 번역 시작</span>
    <span class="n">decoded_sentence</span> <span class="o">=</span> <span class="s2">&quot;[start]&quot;</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_decoded_sentence_length</span><span class="p">):</span>
        <span class="c1"># 트랜스포머 모델 적용</span>
        <span class="n">tokenized_target_sentence</span> <span class="o">=</span> <span class="n">target_vectorization</span><span class="p">(</span>
            <span class="p">[</span><span class="n">decoded_sentence</span><span class="p">])[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
        <span class="n">predictions</span> <span class="o">=</span> <span class="n">transformer</span><span class="p">(</span>
            <span class="p">[</span><span class="n">tokenized_input_sentence</span><span class="p">,</span> <span class="n">tokenized_target_sentence</span><span class="p">])</span>
        <span class="c1"># i-번째 단어로 사용될 어휘 인덱스 확인</span>
        <span class="n">sampled_token_index</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">predictions</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">i</span><span class="p">,</span> <span class="p">:])</span>
        <span class="c1"># i-번째 단어 확인</span>
        <span class="n">sampled_token</span> <span class="o">=</span> <span class="n">spa_index_lookup</span><span class="p">[</span><span class="n">sampled_token_index</span><span class="p">]</span>
        <span class="c1"># 스페인어 입력 텍스트에 i-번째 단어로 추가</span>
        <span class="n">decoded_sentence</span> <span class="o">+=</span> <span class="s2">&quot; &quot;</span> <span class="o">+</span> <span class="n">sampled_token</span>
        <span class="c1"># 기계 번역 종료 조건 확인</span>
        <span class="k">if</span> <span class="n">sampled_token</span> <span class="o">==</span> <span class="s2">&quot;[end]&quot;</span><span class="p">:</span>
            <span class="k">break</span>
            
    <span class="k">return</span> <span class="n">decoded_sentence</span>
</pre></div>
</div>
<p>아래 코드는 <code class="docutils literal notranslate"><span class="pre">decode_sequence()</span></code> 함수를 이용하여
무작위로 5개의 영어 텍스트를 선택하여 기계 번역한 결과이다.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="o">-</span>
<span class="n">Tom</span> <span class="n">isn</span><span class="s1">&#39;t a good person.</span>
<span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="n">tom</span> <span class="n">no</span> <span class="n">es</span> <span class="n">una</span> <span class="n">persona</span> <span class="p">[</span><span class="n">end</span><span class="p">]</span>
<span class="o">-</span>
<span class="n">Automobile</span> <span class="n">sales</span> <span class="n">suffered</span> <span class="n">a</span> <span class="n">setback</span> <span class="n">at</span> <span class="n">the</span> <span class="n">end</span> <span class="n">of</span> <span class="n">the</span> <span class="n">financial</span> <span class="n">year</span><span class="o">.</span>
<span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="n">el</span> <span class="n">examen</span> <span class="n">de</span> <span class="p">[</span><span class="n">UNK</span><span class="p">]</span> <span class="n">a</span> <span class="n">la</span> <span class="n">luz</span> <span class="n">al</span> <span class="n">final</span> <span class="n">de</span> <span class="n">la</span> <span class="n">año</span> <span class="p">[</span><span class="n">end</span><span class="p">]</span>
<span class="o">-</span>
<span class="n">I</span> <span class="n">don</span><span class="s1">&#39;t need an explanation.</span>
<span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="n">no</span> <span class="n">necesito</span> <span class="n">una</span> <span class="n">explicación</span> <span class="p">[</span><span class="n">end</span><span class="p">]</span>
<span class="o">-</span>
<span class="n">Tom</span> <span class="ow">is</span> <span class="n">going</span> <span class="n">to</span> <span class="n">college</span> <span class="n">now</span><span class="o">.</span>
<span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="n">tom</span> <span class="n">está</span> <span class="n">a</span> <span class="n">ir</span> <span class="n">a</span> <span class="n">la</span> <span class="n">universidad</span> <span class="n">ahora</span> <span class="p">[</span><span class="n">end</span><span class="p">]</span>
<span class="o">-</span>
<span class="n">The</span> <span class="n">dish</span> <span class="ow">is</span> <span class="n">too</span> <span class="n">sweet</span> <span class="k">for</span> <span class="n">Tom</span><span class="o">.</span>
<span class="p">[</span><span class="n">start</span><span class="p">]</span> <span class="n">la</span> <span class="n">carta</span> <span class="n">es</span> <span class="n">demasiado</span> <span class="n">bajo</span> <span class="n">para</span> <span class="n">tom</span> <span class="p">[</span><span class="n">end</span><span class="p">]</span>
</pre></div>
</div>
</section>
</section>
<section id="id16">
<h2><span class="section-number">12.6. </span>연습 문제<a class="headerlink" href="#id16" title="Link to this heading">#</a></h2>
<ol class="arabic simple">
<li><p><a class="reference external" href="https://colab.research.google.com/github/codingalzi/dlp2/blob/master/excs/exc-dl_for_text.ipynb">(실습) 자연어 처리</a></p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="dl_for_timeseries.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">11. </span>시계열 분석</p>
      </div>
    </a>
    <a class="right-next"
       href="generative_dl.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">13. </span>생성 모델</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">12.1. 자연어 처리 소개</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">12.2. 텍스트 벡터화</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">12.2.1. 텍스트 표준화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">12.2.2. 토큰화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">12.2.3. 어휘 색인화</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#imdb">12.2.4. IMDB 영화 후기 데이터셋 벡터화</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">12.3. 단어 임베딩</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">12.4. 트랜스포머 아키텍처</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">12.4.1. 셀프 어텐션</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">12.4.2. 멀티헤드 어텐션</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">12.4.3. 트랜스포머 인코더</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">12.5. 시퀀스-투-시퀀스 학습</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">12.5.1. 트랜스포머 모델</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">12.5.2. 트랜스포머 디코더</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">12.5.3. 기계 번역 모델</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">12.6. 연습 문제</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By 코딩알지
</p>

  </div>
  
  <div class="footer-item">
    

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>