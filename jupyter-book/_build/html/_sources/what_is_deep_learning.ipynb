{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 딥러닝이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 인공지능, 머신러닝, 딥러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**관계 1: 연구 분야 관점**\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ai-ml-relation.png\" style=\"width:600px;\"></div>\n",
    "\n",
    "그림 출처: [교보문고(에이지 오브 머신러닝)](http://www.kyobobook.co.kr/readIT/readITColumnView.laf?thmId=00198&sntnId=14142)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**관계 2: 역사**\n",
    "\n",
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ai-ml-relation2.png\" style=\"width:600px;\"></div>\n",
    "\n",
    "그림 출처: [NVIDIA 블로그](https://blogs.nvidia.com/blog/2016/07/29/whats-difference-artificial-intelligence-machine-learning-deep-learning-ai/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 인공지능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- 인공지능: 인간의 지적 활동을 모방하여 컴퓨터로 자동화하려는 시도. 머신러닝과 딥러닝을 포괄함.\n",
    "- (1950년대) 컴퓨터가 생각할 수 있는가? 라는 질문에서 출발\n",
    "- (1956년) 존 맥카시(John McCarthy)\n",
    "    - 컴퓨터로 인간의 모든 지적 활동 구현하는 것이 가능하다고 판단.\n",
    "- (1980년대까지) __학습__(러닝)이 아닌 모든 가능성을 논리적으로 전개하는 기법 활용\n",
    "    - 서양장기(체스) 등에서 우수한 성능 발휘\n",
    "    - 반면에 이미지 분류, 음석 인식, 자연어 번역 등 보다 복잡한 문제는 제대로 다루지 못함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 머신러닝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "머신러닝은 1990년대에 본격적으로 발전했다. \n",
    "기본적으로 통계학과 연관되어 있지만 다음 두 측면에서 통계학과 다르다. \n",
    "첫째, 아주 큰 데이터(빅데이터)를 단순히 통계학적으로 다룰 수는 없다.\n",
    "예를 들어, 수 백만장의 사진을 통계 기법으로 다루지 못하지만 머신러닝은 가능하다.\n",
    "둘째, 머신러닝, 특히 딥러닝의 경우 수학적, 통계적 이론 보다는 공학적 접근법이 보다 중요하다.\n",
    "또한 소프트웨어와 하드웨어의 발전이 딥러닝 연구에 중요한 역할을 수행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**전통적 인공지능 프로그램 대 머신러닝 프로그램**\n",
    "\n",
    "- 전통적 인공지능 프로그램\n",
    "    - 컴퓨터가 수행해야 할 규칙을 순서대로 적어 놓은 프로그램 작성\n",
    "    - 입력값이 지정되면 지정된 규칙을 수행하여 적절한 답을 생성함.\n",
    "\n",
    "- 머신러닝 프로그램\n",
    "    - 주어진 입력 데이터와 출력 데이터로부터 입력과 출력 사이에 존재하는\n",
    "        특정 통계적 구조를 스스로 알아내어\n",
    "        이를 이용하여 입력값으로부터 출력값을 생성하는 규칙을 생성함.\n",
    "    - 예제: 사진 태그 시스템. 태그 달린 사진 데이터셋을 학습한 후 \n",
    "        자동으로 사진의 태그 작성."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-a-new-programming-paradigm.png\" style=\"width:400px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**머신러닝 모델 학습의 필수 요소**\n",
    "\n",
    "- **입력 데이터셋**: 음성 인식 모델을 위한 음성 파일, 이미지 태깅 모델을 위한 사진 등.\n",
    "- **타깃 데이터셋**: 음성 인식 작업의 경우 사람이 직접 작성한 글, \n",
    "    이미지 작업의 경우 '강아지', '고양이', 등의 사람이 직접 붙힌 태그.\n",
    "- **모델 평가지표**: 출력 예상값과 기대 출력값 사이의 거리(차이) 측정법. \n",
    "    거리를 줄이는 방향으로 알고리즘에 사용되는 파라미터를 반복 수정하는\n",
    "    과정을 **학습**이라 부름."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**데이터 표현법 학습**\n",
    "\n",
    "머신러닝 모델은 입력 데이터를 적절하게 변환하여 원하는 결과를 생성하는\n",
    "규칙을 입력과 타깃 데이터셋을 이용하여 학습해 나간다.\n",
    "즉, 주어진 과제 해결에 가장 적절한 **데이터 표현법**을 모델 학습을 통해 알아낸다.\n",
    "\n",
    "데이터는 다양한 방식으로 표현될 수 있으며\n",
    "과제에 따라 보다 적절한 표현법을 채택해야 한다. \n",
    "예를 들어, 컬러 사진을 \n",
    "빨간색-초록색-파란색을 사용하는 RGB 방식으로 표현하거나\n",
    "색상-채도-명도를 사용하는 HSV 방식으로 표현할 수 있다.\n",
    "컬러 사진에서 빨간색 픽셀만을 선택하고자 할 때는 RGB 방식으로 컬러 사진을\n",
    "표현하는 것이 좋은 반면에 \n",
    "컬러 사진의 채도를 조정하고자 할 때는 HSV 방식으로 표현된\n",
    "데이터를 사용해야 한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    ":::{prf:example} 선형 분류\n",
    ":label: exp-lin-reg\n",
    "\n",
    "검은 점과 흰 점이 아래 왼쪽 그림에서첨 분포되어 있다. \n",
    "\n",
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-learning_representations.png\" style=\"width:600px;\"></div>\n",
    "\n",
    "<그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)>\n",
    "\n",
    "좌표 $(x, y)$가 주어지면 해당 좌표의 점의 색깔을 예측하는 머신러닝 모델을 구현하고자 한다.\n",
    "즉, 머신러닝 모델의 학습에 필요한 세 요소가 다음과 같다.\n",
    "\n",
    "- 입력 데이터셋: 2차원 좌표로 표현된 데이터셋\n",
    "- 타깃 데이터셋: 각 좌표의 점이 갖는 색깔\n",
    "- 모델 평가지표: 점의 색을 정확하게 예측한 비율\n",
    "\n",
    "점의 색깔을 예측하기 위한 한 가지 방식은 좌표의 축을 위 가운데 그림에서처럼 다르게 선택하는 것이다.\n",
    "그러면 위 오른쪽 그림에서처럼 흰 점과 검은 점을 단순히 $x$-좌표의 음수와 양수 여부에 따라 \n",
    "간단하게 판단할 수 있다.\n",
    "즉, 새로운 축을 사용하여 각 점의 좌표를 새롭게 표현하도록 변환하는 머신러닝 알고리즘을\n",
    "구현하면 흰 점과 검은 점을 선형적으로 분류하는 머신러닝 모델을 구현할 수 있다.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**데이터 변환 수동화의 어려움**\n",
    "\n",
    "- 위 예제의 경우 수동으로 데이터 변환 방식을 어렵지 않게 알아낼 수 있음.\n",
    "- 반면에 손글씨 숫자 인식(MNIST)의 경우 간단하지 않음(아래 그림 참조). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ch03/homl03-10.png\" width=\"300\"/></div>\n",
    "\n",
    "그림 출처: [핸즈온 머신러닝(2판)](https://www.hanbit.co.kr/store/books/look.php?p_code=B9267655530)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**데이터 변환 자동화**\n",
    "\n",
    "- 머신러닝 모델 학습: 보다 유용한 데이터 표현으로 변환하는 과정을 자동으로 찾는 과정\n",
    "- __데이터 표현의 유용성__에 대한 기준: 주어진 과제 해결을 위한 보다 쉬운 규칙 제공\n",
    "- 변환 방식 종류\n",
    "    - 좌표 변환, 픽셀 개수, 닫힌 원의 개수, 선형/비선형 변환, 이동, ...\n",
    "    - 기본적으로 주어진 문제에 따라 다른 변환 방식 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**가설 공간**\n",
    "\n",
    "- 주어진 문제에 가장 적절한 변환을 머신러닝 알고리즘 스스로 알아내기는 기본적으로 불가능.\n",
    "- __가설 공간__: 프로그래머에 의해 지정된 함수들의 집합\n",
    "- 머신러닝 알고리즘: 가설공간 내에서 적합한 변환 함수 탐색\n",
    "- 예제: 위 2차원 좌표 변환 문제의 가설 공간은 '모든 가능한 좌표 변환 함수'들의 집합"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝의 '딥'(deep)이란?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- '딥'(deep)이란?: __데이터 표현의 연속적 변환__을 지원하는 여러 개의 '층'(layer)을 활용한 학습\n",
    "- 즉 __계층적 표현 학습__을 지원하는 머신러닝을 딥러닝이라 부름.\n",
    "- 딥러닝 모델의 깊이 = 계층으로 쌓아 올린 층의 높이\n",
    "    - 수 십개 또는 수 백개의 층으로 구성된 모델 존재\n",
    "    - 모든 층에서 데이터 표현의 변환이 __자동__으로 이루어지는 것이 핵심!\n",
    "- 섈로우 러닝(shallow learning): 한 두 개의 층만 사용하는 학습"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ch14/homl14-15b.png\" width=\"700\"/></div>\n",
    "\n",
    "<그림 참조: [ImageNet Classification with Deep Convolutional Neural Networks](https://papers.nips.cc/paper/2012/hash/c399862d3b9d6b76c8436e924a68c45b-Abstract.html)>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 신경망\n",
    "\n",
    "- 유닛(unit): 층(layer)을 구성하는 요소. 몇 개에서 몇 십개로 이루어짐.\n",
    "- 신경망(neural network): 계층적 표현 학습이 이루어지는 모델\n",
    "    - __계층적 '인풋-투-타깃'(input-to-target) 변환__ 학습 모델\n",
    "    - 예제: 숫자 이미지 $\\Rightarrow \\cdots \\Rightarrow$ 숫자\n",
    "- 단순하지만 매우 강력한 결과를 생산하는 아이디어! __뇌 과학과 아무 상관 없음!__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 예제: 손글씨 숫자 인식\n",
    "\n",
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-mnist_representations.png\" style=\"width:550px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝 작동 원리\n",
    "\n",
    "딥러닝 모델의 작동 원리를 이해하려면 다음 세 개념에 집중해야 함\n",
    "- 가중치(weight)\n",
    "- 손실 함수(loss function)\n",
    "- 역전파(backpropagation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### A. 가중치\n",
    "\n",
    "- 데이터 표현의 변환에 사용되는 __파라미터__(parameter)\n",
    "- 학습: 적절한 가중치를 모든 층에 대해 동시에(!) 찾는 과정\n",
    "    - 하나의 가중치가 변하면 모든 다른 가중치도 변함!\n",
    "- 많게는 수 천만 개의 가중치를 학습해야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-deep-learning-in-3-figures-1.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### B. 손실 함수\n",
    "\n",
    "- 신경망의 출력값(output)과 타깃(target) 사이의 거리 측정. 가중치에 의존.\n",
    "- __목적 함수__(objective function) 또는 __비용 함수__(cost function)라고도 불림\n",
    "- 손실 함수의 반환값을 학습 과정에서 성능 평가용 피드백으로 활용. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-deep-learning-in-3-figures-2.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### C. 역전파\n",
    "\n",
    "- 역전파(backpropagation) 알고리즘: \n",
    "    경사하강법에 기초하여 손실함수의 출력값과 \n",
    "    타깃 사이의 거리를 좁혀주는 알고리즘\n",
    "- 옵티마이저(optimizer): 역전파 알고리즘을 구현한 프로그램. \n",
    "    - 모든 가중치 무작위 초기화: 결과적으로 손실값 매우 높음.\n",
    "    - 훈련 반복: 손실값이 낮아지도록 가중치 조절. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-deep-learning-in-3-figures-3.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝의 지금까지 성과\n",
    "\n",
    "- 사람과 비슷한 수준의 이미지 분류, 음성 인식, 필기 인식, 자율 주행\n",
    "- 상당한 성능의 기계 번역, TTS(text-to-speech) 변환\n",
    "- 구글 어시스턴트, 아마존 알레사 등의 디지털 도우미\n",
    "- 향상된 광고 타게팅, 웹 검색\n",
    "- 자연어 질문 대처 능력\n",
    "- 초인류 바둑 실력(2013 알파고)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 전망\n",
    "\n",
    "- 단기적으로 너무 높은 기대를 갖는 것은 위험함.\n",
    "    - 실망할 경우 AI에 대한 투자가 급속도로 줄어들 수 있음. \n",
    "    - 1970년대와 1990년대 1차, 2차 AI 겨울(AI winter) 경험\n",
    "- 2020년대 초반 현재 중요한 문제에 본격적으로 딥러닝 적용되고 있지만 대중화는 아직.\n",
    "- 1995년의 인터넷 처럼 앞으로 딥러닝의 가져올 영향에 대해 제대로 알 수 없음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.2 딥러닝 이전: 머신러닝의 역사"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    }
   },
   "source": [
    "- 산업계에서 사용되는 머신러닝 알고리즘의 대부분은 딥러닝 알고리즘이 아님.\n",
    "- 훈련 데이터가 너무 적거나, 딥러닝과 다른 알고리즘이 보다 좋은 성능 발휘 가능."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 확률적 모델링\n",
    "\n",
    "- 나이브 베이즈 알고리즘(Naive Bayes algorithm)을 활용하는 분석 기법이 대표적임.\n",
    "- 베이즈 정리(Bayes theorem)에 기초하는 전통적인 기법\n",
    "- 1950년대 부터 컴퓨터 없이 적용 시작\n",
    "- 베이즈 정리 등 확률론의 기초는 18세기에 시작\n",
    "- 예제: 로지스틱 회귀(logistic regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 초창기 신경망\n",
    "\n",
    "- 기본 아이디어: 1950년대부터 연구됨.\n",
    "- LeNet 합성곱 신경망: 손글씨 숫자 이미지 자동 분류 시스템\n",
    "    - 1989년 벨 연구소(Bell Labs)의 얀 르쿤(Yann LeCun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/handson-ml2/master/slides/images/ch14/homl14-16.gif\" width=\"400\"/></div>\n",
    "\n",
    "< 그림 출처: [LeNet-T CNN](http://yann.lecun.com/exdb/lenet/index.html) > "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 커널 기법\n",
    "\n",
    "- 1990년대: 서포트 벡터 머신(SVM) + 커널 기법\n",
    "- 초창기 신경망 성능을 뛰어 넘음.\n",
    "- 한계\n",
    "    - 대용량 데이터셋 처리에 부적합(매우 느림)\n",
    "    - 이미지 분류 등 지각 문제 해결 어려움\n",
    "- __특성 공학__(feature engineering)에 약함\n",
    "    - 유용한 데이터 표현으로의 변환을 수동을 해결해야 함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 결정트리, 랜덤 포레스트, 그레이디언트 부스팅 머신\n",
    "\n",
    "- (2000년대) 결정트리: 입력값을 순서도 형식으로 특정 기준으로 분류하는 방식\n",
    "- 랜덤 포레스트: 2010년 경까지 커널 기법보다 선호됨.\n",
    "- 그레이디언트 부스팅 머신: 2014년 경 가장 선호되는 앙상블 학습 기법\n",
    "    - 지각 문제 이외의 경우 여전히 가장 성능이 좋은 모델 중 하나임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/ch01-decision_tree.png\" style=\"width:350px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝의 본격적 발전\n",
    "\n",
    "- 2011년: GPU를 활용한 딥 신경망 훈련 시작\n",
    "- 2012년: ImageNet Challenge(이미지 분류 경진대회)의 획기적 성공\n",
    "    - 2011년 최고 성능: 74.3%의 정확도\n",
    "    - 2012년 합성곱 신경망(convnet)의 최고 성능: 83.6%의 정확도\n",
    "    - 2015년 최고 성능: 96.4%의 정확도\n",
    "    - ImageNet Challenge 대회 더 이상 진행되지 않음.\n",
    "- 2015년 이후: 많은 문제 영역에서 SVM, 결정트리 등을 딥러닝 모델로 대체함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝의 특징\n",
    "\n",
    "- 자동화된 데이터 표현의 변환, 즉 특성 공학 자동화\n",
    "- 층을 거치면서 점진적으로 더 복잡한 데이터 표현을 만들어 냄.\n",
    "- 모든 과정의 데이터 표현의 변환, 즉 모든 층에 대한 특성 공학 스스로 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 최근 머신러닝 분야의 동향 1\n",
    "\n",
    "- 2019년 캐글(Kaggle) 경진대회에서 상위팀이 사용한 도구 설문조사 결과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/kaggle_top_teams_tools.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "그림 출처: [Deep Learning with Python](https://www.manning.com/books/deep-learning-with-python-second-edition)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### 최근 머신러닝 분야의 동향 2\n",
    "\n",
    "- 데이터과학 일반에서 가장 많이 사용되는 도구(캐글 설문조사 2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/kaggle_ds_survey_2020.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "그림 출처: [www.kaggle.com/kaggle-survey-2020(20쪽)](https://www.kaggle.com/kaggle-survey-2020)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## 1.3 딥러닝 발전 동력"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  딥러닝 주요 요소\n",
    "\n",
    "- 컴퓨터 비전, 자연어 인식 분야가 2012년 이후 획기적으로 발전하였음.\n",
    "- 획기적 발전에 기여한 아래 기법은 하지만 1990년대에 제시됨\n",
    "    - 1990년: 합성곱 신경망(convnet, convolutional neural network)과 역전파(backpropagation)\n",
    "    - 1997년: LSTM(Long Short-Term Memory)\n",
    "- 2010년대에 딥러닝의 급격한 발전에 기여한 세 가지 요소\n",
    "    - 하드웨어\n",
    "    - 데이터셋과 벤치마크\n",
    "    - 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 하드웨어\n",
    "\n",
    "- CPU: 1990년에 비해 5,000배 이상 빨라짐\n",
    "- GPU(Graphical Processing Unit): \n",
    "    - NVIDIA, AMD: 2000년대부터 게임용 그래픽 카드 개발에 천문학적으로 투자\n",
    "    - 2007년 NVIDIA의 CUDA 개발: GPU를 위한 프로그래밍 인터페이스. 다량의 행렬 계산을 병렬처리 가능해짐.\n",
    "    - 2011년: 신경망용 CUDA 개발.\n",
    "- TPU(Tensor Processing Unit): 2016년 구글이 소개한 딥러닝 전용 칩. \n",
    "    - GPU보다 훨씬 빠르고 에너지 효율적임. \n",
    "    - 2020년에 3세대 TPU 카드 발표. 1990년의 최고 슈퍼컴퓨터보다 10,000배 이상 빠름.\n",
    "    - 2020년 최고의 슈퍼컴퓨터 = 27,000 개의 NVIDIA GPUs = 10개의 pod 성능\n",
    "        (1 pod = 1024개의 TPU 카드)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 데이터\n",
    "\n",
    "- 인터넷과 저장장치의 발전으로 인한 엄청난 양의 데이터 축적\n",
    "    - __무어의 법칙__(Moore's law): 2년마다 반도체 집적회로의 성능이 2배로 향상됨.\n",
    "- Flickr(이미지), YouTube(동영상), Wikipedia(문서) 등이 컴퓨터 비전과 자연어 처리(NLP)의\n",
    "    혁신적 발전의 기본 전제조건이었음.\n",
    "- 벤치마크(성능비교)의 활성화\n",
    "    - ImageNet 데이터셋: 140만 개의 이미지와 손으로 작성된 1,000개의 클래스 태그\n",
    "    - IamgeNet Challenge, Kaggle Competitions 등과 같은 경진대회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 알고리즘\n",
    "\n",
    "- 2000년대 후반까지 딥러닝 네트워크를 효율적으로 훈련시킬 수 있는 알고리즘 부재(역전파 문제 미해결)\n",
    "- 2009-2010: 주요 알고리즘 개선\n",
    "    - 보다 좋은 신경망 층에 사용되는 활성화 함수 \n",
    "    - 보다 좋은 가중치 초기화\n",
    "    - 보다 좋은 옵티마이저(RMSProp, Adam 등)\n",
    "- 2014-2016: 역전파에 도움되는 다양한 기법 개발\n",
    "    - 배치 정규화(batch normalization), 잔차 연결(residual connection), \n",
    "        깊이별 분리 합성곱(depthwise separable convolution) 등\n",
    "- 현재: 수십 개의 층을 가지며 수천 만개의 가중치(파라미터)를 갖는\n",
    "    깊은 층으로 구성된 신경망 네트워크 훈련 가능"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 투자\n",
    "\n",
    "- 2013년 이후 투자가 획기적으로 증가함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/startup_investment_oecd.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "그림 출처: [OECD estimate of total investments in AI startups](https://www.oecd-ilibrary.org/sites/3abc27f1-en/index.html?itemId=/content/component/3abc27f1-en&mimeType=text/html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝의 대중화\n",
    "\n",
    "- 이전: C++, CUDA 등을 이용한 어려운 프로그램을 구현할 수 있었어야 함.\n",
    "- 지금: 파이썬 기초 프로그래밍 수준에서 시작 가능\n",
    "    - Sci-kit Learn, Theano, Tensorflow, Keras 등의 라이브러리 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### 딥러닝의 미래\n",
    "\n",
    "- 여전히 딥러닝 혁신적 발전이 진행중임.\n",
    "    - 최근의 가장 큰 혁신: 트랜스포머(transformer) 기법를 이용한 자연어 처리\n",
    "- 20년 뒤엔 알 수 없음. 하지만 딥러닝 발전의 토대를 이룬 아래 요소는 계속해서 활용될 것으로 기대함.\n",
    "    - 단순함: 특성 공학의 자동화로 인해 모델 생성과 훈련이 단순화됨.\n",
    "    - 확장성: GPU 또는 TPU 등을 이용한 병렬화가 가능하기에 무어의 법칙을 최대한 활용할 수 있음.\n",
    "        작은 크기의 데이터 배치(묶음)로 나눈 후 훈련 반복을 병렬화하면 임의의 크기의 데이터셋을\n",
    "        이용한 훈련이 가능해짐.\n",
    "    - 다용도와 재사용성: 추가 입력된 데이터로 훈련을 이어갈 수 있음. \n",
    "        또한 잘 훈련된 모델을 다른 용도의 모델 훈련에 재활용할 수 있음. \n",
    "        이는 또한 아주 작은 데이터셋을 대상으로 딥러닝 모델을 적용할 수 있도록 해줌.\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
