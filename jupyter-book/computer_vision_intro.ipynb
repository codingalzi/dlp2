{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d068e69a",
   "metadata": {},
   "source": [
    "(ch:computer-vision-intro)=\n",
    "# 컴퓨터 비전 기초: 합성곱 신경망"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**감사의 글**\n",
    "\n",
    "아래 내용은 프랑소와 숄레의 \n",
    "[Deep Learning with Python(2판)](https://github.com/fchollet/deep-learning-with-python-notebooks)의 \n",
    "소스코드 내용을 참고해서 작성되었습니다.\n",
    "자료를 공개한 저자에게 진심어린 감사를 전합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**소스코드**\n",
    "\n",
    "여기서 언급되는 코드를\n",
    "[(구글 코랩) 컴퓨터 비전 기초: 합성곱 신경망](https://colab.research.google.com/github/codingalzi/dlp2/blob/master/notebooks/NB-computer_vision_intro.ipynb)에서 \n",
    "직접 실행할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ98mOIdysHn"
   },
   "source": [
    "**주요 내용**\n",
    "\n",
    "- 합성곱 신경망(convnet) 소개\n",
    "- 데이터 증식\n",
    "- convnet 재활용\n",
    "    - 특성 추출\n",
    "    - 하이퍼파라미터 튜닝\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g2BqDhykysHo"
   },
   "source": [
    "## 합성곱 신경망 소개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2011년부터 2015년 사이에 벌어진 \n",
    "컴퓨터 비전 분야에서의 딥러닝의 획기적 활용이 딥러닝 분야의 약진을 불러왔다.\n",
    "현재 사진 앱, 사진 검색, 유튜브, 카메라 필터, 글자 인식, 자율주행, 로봇공학, 의학 진단 프로그램,\n",
    "얼굴 확인, 스마트 팜 등 일상의 많은 영역에서 딥러닝 모델이 사용되고 있다.\n",
    "\n",
    "컴퓨터 비전 분야에서 일반적으로 가장 많이 사용되는 딥러닝 모델은\n",
    "**convnet**으로 불리는 \n",
    "**합성곱 신경망**<font size='2'>convolutional neural networks</font>(CNN)이다.\n",
    "여기서는 작은 크기의 훈련 데이터셋을 이용하여 이미지 분류 문제에\n",
    "convnet을 적용하는 방법을 소개한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XVaymubysHp"
   },
   "source": [
    "**예제: MNIST 데이터셋 분류**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4nbEW1zysHp"
   },
   "source": [
    "**convnet 모델 구성**\n",
    "\n",
    "- `Input()`의 `shape`: `(28, 28, 1)`\n",
    "- `Conv2D`와 `MaxPooling2D` 층을 함수형 API 방식으로 층쌓기\n",
    "    - 채널수: `filters` 인자로 결정\n",
    "    - `kernel_size=3`에 유의할 것.\n",
    "    - 출력값: 3D 텐서 (높이, 너비, 채널수). `filers`와 `kernel_size`에 의존."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j3xl1kRgysHq"
   },
   "source": [
    "```python\n",
    "# 입력층\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "\n",
    "# 은닉층\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "\n",
    "# 출력층으로 넘기기 전에 1차원 텐서로 변환\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# 출력층\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "\n",
    "# 모델\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSSZBW3hysHs"
   },
   "source": [
    "**모델 구성 요약**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> model.summary()\n",
    "Model: \"model\" \n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param # \n",
    "================================================================= \n",
    "input_1 (InputLayer)         [(None, 28, 28, 1)]       0 \n",
    "_________________________________________________________________\n",
    "conv2d (Conv2D)              (None, 26, 26, 32)        320 \n",
    "_________________________________________________________________\n",
    "max_pooling2d (MaxPooling2D) (None, 13, 13, 32)        0 \n",
    "_________________________________________________________________\n",
    "conv2d_1 (Conv2D)            (None, 11, 11, 64)        18496 \n",
    "_________________________________________________________________\n",
    "max_pooling2d_1 (MaxPooling2 (None, 5, 5, 64)          0 \n",
    "_________________________________________________________________\n",
    "conv2d_2 (Conv2D)            (None, 3, 3, 128)         73856 \n",
    "_________________________________________________________________\n",
    "flatten (Flatten)            (None, 1152)              0 \n",
    "_________________________________________________________________\n",
    "dense (Dense)                (None, 10)                11530 \n",
    "=================================================================\n",
    "Total params: 104,202 \n",
    "Trainable params: 104,202 \n",
    "Non-trainable params: 0 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eGdgA_xTysHu"
   },
   "source": [
    "**MNIST 이미지 분류 훈련**\n",
    "\n",
    "모델 훈련은 이전과 동일하다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WnqfoP5qysHu",
    "outputId": "6b557aec-4baf-497c-ddad-43ce7384f443"
   },
   "source": [
    "```python\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",  # 레이블이 정수인 경우\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFsX1OF3ysHv"
   },
   "source": [
    "**훈련된 convnet 평가**\n",
    "\n",
    "테스트셋에 대한 성능이 이전에 사용한 `Sequential` 모델보다 훨씬 좋다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YqO_83JMysHw"
   },
   "source": [
    "### 합성곱 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4t7jk5-YysHw"
   },
   "source": [
    "- `Dense` 층: 입력값 전체를 학습 대상으로 삼음.\n",
    "    - MNIST의 경우: 숫자 이미지 전체\n",
    "- `Conv2D` 층: 예를 들어 `kernel_size=3`으로 설정된 경우\n",
    "    `3x3` 크기의 국소적 특성만 대상으로 삼음.\n",
    "    다음 두 가지 장점을 발휘함."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W0nWtLBwysHx"
   },
   "source": [
    "**Conv2D 모델의 장점**\n",
    "\n",
    "첫째, 패턴의 위치와 무관하다.\n",
    "\n",
    "한 번 인식된 패턴을 다른 위치에서도 인식된다.\n",
    "따라서 적은 수의 샘플을 이용하여 높은 일반화 성능의 모델을 훈련시킬 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-RWbL4yysHx"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/local_patterns.jpg\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7RtlxM9kysHx"
   },
   "source": [
    "둘째, 패턴 공간의 계층을 파악한다.\n",
    "\n",
    "위 층으로 진행할 수록 보다 복잡한 패턴을 파악한다.\n",
    "이를 **패턴 공간의 계층**(spatial hierarchy of patterns)이라 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bewYIucwysHy"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/visual_hierarchy_hires.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ksf6XLIwysHy"
   },
   "source": [
    "**특성맵, 채널, 필터**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UKqPzrirysHy"
   },
   "source": [
    "합성곱 연산의 작동법을 이해하려면 아래 세 개념을 이해해야 한다.\n",
    "\n",
    "- **특성맵**(feature maps): 높이, 너비, 깊이로 구성된 3D 텐서\n",
    "    - 예제: MNIST 데이터셋에 포함된 흑백 이미지 샘플의 경우 `(28, 28, 1)` 모양의 3D 텐서.\n",
    "- **채널**(channel): 특성맵의 깊이를 구성하는 값.\n",
    "    - 예제: MNIST 데이터 샘플은 흑백 사진이기에 하나의 채널로 구성됨.\n",
    "    - 예제: 컬러 이미지 샘플은 `(28, 28, 3)` 과 같은 모양의 3D 텐서이기에\n",
    "        세 개의 채널로 구성됨. 3은 RGB를 나타냄.        \n",
    "- **필터**(filter): `kernel_size`를 이용한 3D 텐서. \n",
    "    - 예제: `kernel_size=3`인 경우 필터는 `(3, 3, 입력특성맵깊이)` 모양의 3D 텐서.\n",
    "    - 하나의 필터가 하나의 **출력맵**(response map), 즉 하나의 채널을 생성하는 데에 사용됨.\n",
    "    - 필터 수: `filters` 인자에 의해 결정됨.\n",
    "    - 출력 특성맵: 출력맵으로 구성된 `(높이, 너비, 필터수)` 모양의 3D 텐서."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwgLw0d5ysHz"
   },
   "source": [
    "아래 그림은 MNIST 데이터 샘플에 대해 하나의 필터가 작동하여 하나의 출력맵을 생성하는 과정을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pLwdvJDRysHz"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/response_map_hires.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J99i3L6bysHz"
   },
   "source": [
    "아래 그림은 세 개의 필터를 슬라이딩 시키면서 출력 특성맵을 생성하는 과정을 보여주며\n",
    "사용된 예제는 다음과 같다.\n",
    "\n",
    "- 입력 특성맵: `(5, 5, 2)` 모양의 3D 텐서\n",
    "- 필터 모양: `(3, 3, 2)`\n",
    "- 필터 개수: 3개\n",
    "- 출력 특성맵: `(3, 3, 3)` 모양의 3D 텐서"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BVEaM7TiysH0"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://raw.githubusercontent.com/codingalzi/dlp/master/notebooks/images/how_convolution_works.jpg\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ALFl59_PysH0"
   },
   "source": [
    "**합성곱 커널**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyS9TgnjysH0"
   },
   "source": [
    "필터는 입력 특성맵을 출력 특성맵으로 변환하는 과정에서 사용되는 **가중치들의 행렬**이며,\n",
    "이런 필터를 출력 특성맵의 깊이(채널 수)만큼 모아놓은 (4D) 텐서가 **합성곱 커널**이다.\n",
    "**합성곱 신경망**은 바로 이 합성곱 커널(필터)을 학습시키는 모델을 가리킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "합성곱 커널(필터 모음)을 적용하여 생성된 출력 특성맵의 모양이\n",
    "입력 특성맵의 모양과 다를 수 있다.\n",
    "이점을 이해하려면 먼저 패딩과 보폭의 의미를 알아야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GpwQEcDFysH0"
   },
   "source": [
    "**패딩과 보폭**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sgrsASocysH1"
   },
   "source": [
    "출력 특성맵의 높이와 너비는\n",
    "**패딩**(padding)의 사용 여부와 **보폭**(stride)의 크기에 의에 결정된다.\n",
    "\n",
    "다음 세 개의 그림은 입력 특성맵의 높이와 너비가 `5x5`일 때 \n",
    "패딩의 사용 여부와 보폭의 크기에 따라 \n",
    "출력 특성맵의 높이와 너비가 어떻게 달라지는가를 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pNx72UmtysH1"
   },
   "source": [
    "- 경우 1: 패딩 없음, 보폭은 1.\n",
    "    - 출력 특성맵의 깊이와 너비: `3x3`\n",
    "    - 즉, 출력 특성맥의 깊이와 너비가 줄어듦."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OF3dIJqrysH1"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/3x3_patches_in_5x5_input.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NSnZEWj_ysH2"
   },
   "source": [
    "- 경우 2: 패딩 있음, 보폭은 1.\n",
    "    - 출력 특성맵의 깊이와 너비: `5x5`\n",
    "    - 즉, 출력 특성맵의 깊이와 너비가 동일하게 유지됨."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzBHRKfLysH2"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/padding_of_5x5_input.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6F8asYYEysH2"
   },
   "source": [
    "- 경우 3: 패딩 없음, 보폭은 2.\n",
    "    - 출력 특성맵의 깊이와 너비: `2x2`\n",
    "    - 즉, 출력 특성맵의 깊이와 너비가 보폭의 반비례해서 줄어듦."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hl_0Z0NysH3"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/strides.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aMk79wVZysH3"
   },
   "source": [
    "- 경우 4: 패딩 있음, 보폭은 2.\n",
    "    - 이 경우는 굳이 사용할 필요 없음. \n",
    "        이유는 보폭이 1보다 크기에 출력 특성맵의 깊이와 너비가 어차피 보폭에 반비례해서 줄어들기 때문임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VEg9uZtGysH3"
   },
   "source": [
    "### 맥스 풀링 연산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPI-sEFTpYu9"
   },
   "source": [
    "합성곱 신경망의 전형적인 모습은 다음과 같이 풀링 층을 합성곱 층 이후에 바로 위치시킨다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_m5Vo__pYu-"
   },
   "source": [
    "<div align=\"center\"><img src=\"http://formal.hknu.ac.kr/handson-ml2/slides/images/ch14/homl14-03.png\" style=\"width:700px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.hanbit.co.kr/store/books/look.php?p_code=B7033438574\">핸즈온 머신러닝(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "풀링 층 중에서 **맥스 풀링**(max-pooling) 층이 많이 사용된다.\n",
    "맥스 풀링 층은 일정 크기의 영역에서 최댓값만을 선택하여 특성맵의 높이와 너비를 일정 비율로 줄인다. \n",
    "예를 들어, 아래 맥스 풀링 층은 `2x2`영역에서 최댓값 하나만을 남기고 나머지는 버리며,\n",
    "이 연산을 보폭 2만큼씩 이동하며 특성맵의 모든 영역(`높이x너비`)에 대해 실행한다.\n",
    "맥스 풀링 연산은 특성맵, 즉 채녈 별로 이루어지기에 채널 수는 변하지 않는다.\n",
    "따라서 만약 `x`가 `(26, 26, 32)` 모양의 3D 텐서이면\n",
    "다음 맥스 풀링 층의 출력값은 `(13, 13, 32)` 모양의 3D 텐서가 된다.\n",
    "\n",
    "```python\n",
    "layers.MaxPooling2D(pool_size=2)(x)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GDORRzcmpYu9"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://media.geeksforgeeks.org/wp-content/uploads/20190721025744/Screenshot-2019-07-21-at-2.57.13-AM.png\" style=\"width:500px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.geeksforgeeks.org/cnn-introduction-to-pooling-layer/\">GeeksforGeeks: Introduction to Pooling Layer</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BT04iceDpYu9"
   },
   "source": [
    "맥스 풀링 층을 합성곱 층(`Conv2D`)과 함께 사용하는 이유는 두 가지이다. \n",
    "\n",
    "- 학습해야할 가중치 파라미터의 수 줄이기.\n",
    "- 상위 층으로 갈 수록 입력 특성맵의 보다 넓은 영역에 대한 정보를 얻기 위해."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MDweLmUNysH7"
   },
   "source": [
    "아래 코드는 맥스 풀링 층을 사용하지 않는 경우 가중치 파라미터의 수가 엄청나게 증가함을 잘 보여준다. \n",
    "\n",
    "- 맥스 풀링 사용하는 경우: 104,202개\n",
    "- 그렇지 않은 경우: 712,202개"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xk2Uzc0OysH7"
   },
   "source": [
    "```python\n",
    "inputs = keras.Input(shape=(28, 28, 1))\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model_no_max_pool = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> model_no_max_pool.summary()\n",
    "Model: \"model_1\" \n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param # \n",
    "=================================================================\n",
    "input_2 (InputLayer)         [(None, 28, 28, 1)]       0 \n",
    "_________________________________________________________________\n",
    "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320 \n",
    "_________________________________________________________________\n",
    "conv2d_4 (Conv2D)            (None, 24, 24, 64)        18496 \n",
    "_________________________________________________________________\n",
    "conv2d_5 (Conv2D)            (None, 22, 22, 128)       73856 \n",
    "_________________________________________________________________\n",
    "flatten_1 (Flatten)          (None, 61952)             0 \n",
    "_________________________________________________________________\n",
    "dense_1 (Dense)              (None, 10)                619530 \n",
    "=================================================================\n",
    "Total params: 712,202 \n",
    "Trainable params: 712,202 \n",
    "Non-trainable params: 0 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vf5oXWpBysH8"
   },
   "source": [
    "**적절한 `kernel_size`, `stride`, `pool_size`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hwe_t-hAysH8"
   },
   "source": [
    "- `Conv2D` 층의 기본값: `kernel_size=3`, `stride=1`\n",
    "- `MaxPooling2D` 층의 기본값: `pool_size=2`, `strides=2`\n",
    "    - 주의: `strides`의 기본값을 지정하지 않으면 `pool_size`의 값과 동일하게 지정됨.\n",
    "\n",
    "다른 설정 또는 최댓값 대신에 평균값을 사용하는 `AveragePooling2D`를 \n",
    "활용할 수 있으나 케라스의 기본 설정이 일반적으로 가장 좋은 성능을 보인다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0hkHKIi4ysH9"
   },
   "source": [
    "## 합성곱 신경망 실전 활용 예제"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qONLZBjTysH9"
   },
   "source": [
    "개와 고양이 사진을 대상으로 이항분류를 구현하는 모델을 합성곱 신경으로 구현한다.\n",
    "실전 상황을 묘사하기 위해 5천 개의 이미지로 이루어진 작은 데이터셋을 훈련, 검증, 테스트 용도로 사용한다. \n",
    "실제로 훈련셋의 크기가 수 백에서 수 천인 경우가 매우 일반적으로 발생한다.\n",
    "\n",
    "모델 훈련 과정은 다음과 같다.\n",
    "\n",
    "- 직접 합성곱 신경망 모델 구현 및 훈련\n",
    "    - 합성곱 신경망은 작은 크기의 데이터셋으로도 적절한 성능을 얻을 수 있음.\n",
    "    - 하지만 데이터 증식을 적용하거나 기존에 잘 훈련된 모델을 재활용하여 보다 높은 성능의 모델 구현 가능.\n",
    "- 데이터 증식 기법 추가\n",
    "    - 작은 데이터셋의 크기를 늘리는 기법\n",
    "- 기존에 알려진 모델 재활용\n",
    "    - 특성 추출 기법\n",
    "    - 파인 튜닝(fine-tunig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aw46AzPrysH9"
   },
   "source": [
    "### 데이터 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSJjUTf8ysH9"
   },
   "source": [
    "훈련에 필요한 데이터를 다운로드한다.\n",
    "하지만 이어지는 데이터 다운로드 관련 코드의 실행은 주의를 기울여야 한다.\n",
    "\n",
    "- 캐글(kaggle) 계정을 갖고 있어야 하며, 로그인된 상태에서 아래 두 과정을 먼저 해결해야 한다.\n",
    "- 캐글에 로그인한 후 \"Account\" 페이지의 계정 설정 창에 있는 \"API\" 항목에서\n",
    "    \"Create New API Token\"을 생성하여 다운로드한다.\n",
    "- [캐글: Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/rules)를\n",
    "    방문해서 \"I Understand and Accept\" 버튼을 클릭해야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v7yc6D4mysH-"
   },
   "source": [
    "다음 네 개의 이어지는 코드셀은 구글 코랩에서만 실행해야 한다.\n",
    "현재 구글 코랩을 사용하고 있는지 여부는 아래와 같이 확인할 수 있다.\n",
    "\n",
    "```python\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('구글 코랩 사용중!')\n",
    "else:\n",
    "    print('구글 코랩 환경 아님!')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_-D9qm2FysH-"
   },
   "source": [
    "개인 PC에서 실행하고자 할 경우 다음 사항들을 고려해야 한다.\n",
    "\n",
    "- `kaggle.json`: 캐글 API 키를 의미하며, 캐글 웹사이트에 로그인한 후\n",
    "    \"My Account\"의 API 섹션에서 하나 생성한 후 다운로드한다.\n",
    "\n",
    "- `kaggle.json` 파일을 아래 둘째 코드셀에서 \n",
    "    지정한 경로와 동일한 곳에 저장한 후에 3 단계부터 실행하면 된다.\n",
    "    단, `kaggle` API가 설치되어 있어야 한다. \n",
    "    아니면 주피터 노트북에서 아래 명령문을 실행한다.\n",
    "\n",
    "    ```\n",
    "    !pip install kaggle\n",
    "    ```\n",
    "\n",
    "- `kaggle` API를 굳이 설치하지 않으려면 \n",
    "    [캐글: Dogs vs. Cats](https://www.kaggle.com/c/dogs-vs-cats/data)에서\n",
    "    `train.zip` 파일을 다운로드하여 현재 주피터 노트북이 실행되는 디렉토리에 저장한 후에\n",
    "    4 단계부터 실행하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fdWhmKPgysH_"
   },
   "source": [
    "- 다운로드 1단계"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 92,
     "resources": {
      "http://localhost:8080/nbextensions/google.colab/files.js": {
       "data": "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",
       "headers": [
        [
         "content-type",
         "application/javascript"
        ]
       ],
       "ok": true,
       "status": 200,
       "status_text": ""
      }
     }
    },
    "id": "HKuAUrH4ysH_",
    "outputId": "01c12e55-de0e-4bc2-9e78-dd3335e13e75"
   },
   "source": [
    "```python\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    print('구글 코랩 사용중!')\n",
    "    from google.colab import files\n",
    "    files.upload()\n",
    "\n",
    "print('이어지는 코드는 kaggle.json 파일이 현재 디렉토리에 있다고 가정함.')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JwI48LzzysH_"
   },
   "source": [
    "- 다운로드 2단계: kaggle.json 파일이 현재 워킹 디렉토리에 있다고 가정한 상태에서 아래 명령문 실행."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WGGMqPH7ysH_"
   },
   "source": [
    "```python\n",
    "import os, shutil, pathlib\n",
    "\n",
    "# kaggle 인증서 현재 저장 위치\n",
    "where_kaggle_json = pathlib.Path(\"kaggle.json\")\n",
    "\n",
    "# kaggle 인증서를 사용자 홈디렉토리의 \".kaggle/\" 디렉토리로 옮기기\n",
    "if where_kaggle_json.is_file():\n",
    "    # 홈디렉토리 경로 지정\n",
    "    homeDir = pathlib.Path.home()\n",
    "    kaggleDir = homeDir / \".kaggle\"\n",
    "    kaggleJsonFile = kaggleDir / \"kaggle.json\"\n",
    "\n",
    "    # \".kaggle\" 디렉토리 존재 여부 확인. 없으면 생성.\n",
    "    if not kaggleDir.is_dir():\n",
    "        os.makedirs(kaggleDir)\n",
    "\n",
    "    # \"kaggle.json\" 파일 존재 여부 확인. 없으면 복사.\n",
    "    if not kaggleJsonFile.is_file():\n",
    "        shutil.copyfile(src=where_kaggle_json, \n",
    "                        dst=kaggleJsonFile)\n",
    "        os.chmod(kaggleJsonFile, 0o600)\n",
    "else:\n",
    "    print(\"kaggle.json 파일을 지정된 사용자 홈폴더의 '.kaggle' 폴더에 저장하세요!\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Man6VmnsysIA"
   },
   "source": [
    "- 다운로드 3단계: 강아지-고양이 이미지셋 다운로드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ht0FVko9ysIA",
    "outputId": "64ba3e8c-e0df-41b8-8ab5-e09e65dbe8c1"
   },
   "source": [
    "```\n",
    "# 강아지-고양이 이미지셋 다운로드 및 압축 해제\n",
    "try: \n",
    "    !kaggle competitions download -c dogs-vs-cats\n",
    "except: \n",
    "    !pip install kaggle\n",
    "    !kaggle competitions download -c dogs-vs-cats\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wOfTnBOaysIB"
   },
   "source": [
    "- 다운로드 4단계: 압축 풀기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3-2usDNQysIB"
   },
   "source": [
    "```python\n",
    "import zipfile\n",
    "\n",
    "try:\n",
    "    with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "except:\n",
    "    with zipfile.ZipFile('dogs-vs-cats.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "    with zipfile.ZipFile('train.zip', 'r') as zip_ref:\n",
    "        zip_ref.extractall('./')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pdlqC4q_ysIB"
   },
   "source": [
    "다운로드된 데이터셋 전체는 총 25,000장의 강아지와 고양이 사진으로 구성되었으며 570MB 정도로 꽤 크다.\n",
    "강아지와 고양이 각각 12,500 장씩 포함되어 있으며,\n",
    "사진들의 크기가 다음과 같이 일정하지 않다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NjPmhPd1ysIC"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/v-7/Figures/dog_and_cat_samples.png\" style=\"width:700px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2D7lzljHysIC"
   },
   "source": [
    "**훈련셋, 검증셋, 테스트셋 준비**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ce0xaivBysIC"
   },
   "source": [
    "25,000 장의 사진 중에서 총 5,000 장의 사진만 사용해서 합성곱 신경을 훈련시키려 한다.\n",
    "\n",
    "- 훈련셋: 강아지와 고양이 각각 1,000 장\n",
    "- 검증셋: 강아지와 고양이 각각 500 장\n",
    "- 테스트셋: 강아지와 고양이 각각 1,000 장\n",
    "\n",
    "아래 코드는 앞서 지정한 대로 총 5,000 장의 사진으로 이루어진 데이터셋을\n",
    "추출해서 각각의 디렉토리에 저장한다.\n",
    "디렉토리의 구성은 다음과 같다.\n",
    "\n",
    "```\n",
    "cats_vs_dogs_small/\n",
    "...train/\n",
    "......cat/\n",
    "......dog/\n",
    "...validation/\n",
    "......cat/\n",
    "......dog/\n",
    "...test/\n",
    "......cat/\n",
    "......dog/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VozcsBEEysIC"
   },
   "source": [
    "```python\n",
    "import os, shutil, pathlib\n",
    "\n",
    "original_dir = pathlib.Path(\"train\")\n",
    "new_base_dir = pathlib.Path(\"cats_vs_dogs_small\")\n",
    "\n",
    "def make_subset(subset_name, start_index, end_index):\n",
    "    for category in (\"cat\", \"dog\"):\n",
    "        dir = new_base_dir / subset_name / category\n",
    "        os.makedirs(dir)\n",
    "        fnames = [f\"{category}.{i}.jpg\" for i in range(start_index, end_index)]\n",
    "        for fname in fnames:\n",
    "            shutil.copyfile(src=original_dir / fname,\n",
    "                            dst=dir / fname)\n",
    "\n",
    "make_subset(\"train\", start_index=0, end_index=1000)\n",
    "make_subset(\"validation\", start_index=1000, end_index=1500)\n",
    "make_subset(\"test\", start_index=1500, end_index=2500)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "54G7EErKysID"
   },
   "source": [
    "### 모델 구성"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU6A5uNJysID"
   },
   "source": [
    "convnet(합성곱 신경망) 모델은 앞서 설명한대로 `Conv2D`와 `MaxPooling2D` 레이어를\n",
    "연속에서 쌓는 방식을 사용한다.\n",
    "다만, 보다 복잡한 모델 구성을 위해 보다 높은 층 스택을 쌓는다.\n",
    "\n",
    "- 입력층: 입력 샘플의 모양을 `(180, 180, 3)`으로 지정. 픽셀 크기는 임의로 지정함.\n",
    "    사진의 크기가 제 각각이기에 먼저 지정된 크기의 텐서로 변환을 해주는 전처리 과정이 필요함.\n",
    "- 출력층: 이항분류 모델이기에 한 개의 유닛과 시그모이드 활성화 함수 사용.\n",
    "- `Rescaling(1./255)` 층: 0에서 255 사이의 값을 0에서 1 사이의 값으로 변환하는 용도로 사용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o6ENwt1VysID"
   },
   "source": [
    "```python\n",
    "# 입력층\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "# 은닉층\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# 출력층\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    ">>> model.summary()\n",
    "Model: \"model_2\" \n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param # \n",
    "=================================================================\n",
    "input_3 (InputLayer)         [(None, 180, 180, 3)]     0 \n",
    "_________________________________________________________________\n",
    "rescaling (Rescaling)        (None, 180, 180, 3)       0 \n",
    "_________________________________________________________________\n",
    "conv2d_6 (Conv2D)            (None, 178, 178, 32)      896 \n",
    "_________________________________________________________________\n",
    "max_pooling2d_2 (MaxPooling2 (None, 89, 89, 32)        0 \n",
    "_________________________________________________________________\n",
    "conv2d_7 (Conv2D)            (None, 87, 87, 64)        18496 \n",
    "_________________________________________________________________\n",
    "max_pooling2d_3 (MaxPooling2 (None, 43, 43, 64)        0 \n",
    "_________________________________________________________________\n",
    "conv2d_8 (Conv2D)            (None, 41, 41, 128)       73856 \n",
    "_________________________________________________________________\n",
    "max_pooling2d_4 (MaxPooling2 (None, 20, 20, 128)       0 \n",
    "_________________________________________________________________\n",
    "conv2d_9 (Conv2D)            (None, 18, 18, 256)       295168 \n",
    "_________________________________________________________________\n",
    "max_pooling2d_5 (MaxPooling2 (None, 9, 9, 256)         0 \n",
    "_________________________________________________________________\n",
    "conv2d_10 (Conv2D)           (None, 7, 7, 256)         590080 \n",
    "_________________________________________________________________\n",
    "flatten_2 (Flatten)          (None, 12544)             0 \n",
    "_________________________________________________________________\n",
    "dense_2 (Dense)              (None, 1)                 12545 \n",
    "=================================================================\n",
    "Total params: 991,041 \n",
    "Trainable params: 991,041 \n",
    "Non-trainable params: 0 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oI72pKLbysIE"
   },
   "source": [
    "강아지와 고양이의 비율이 동일하기에 정확도를 평가지표로 사용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-S05Zv-UysIF"
   },
   "source": [
    "```python\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHL_rAOyysIF"
   },
   "source": [
    "### 데이터 전처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mlCK34YrysIF"
   },
   "source": [
    "샘플 사진의 크기가 제 각각이기에 모델의 입력값으로 지정된 크기인 `(180, 180, 3)` 모양의 \n",
    "텐서로 변환을 해주어야 한다.\n",
    "케라스의 `image_dataset_from_directory()` 함수를 이용하면 변환 뿐만 아니라\n",
    "지정된 배치 크기의 배치로 구성된 훈련셋, 검증셋, 테스트셋을 쉽게 생성할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ojSz8HKWysIF",
    "outputId": "7bd95983-73c6-45e5-cda6-26126d91ccae"
   },
   "source": [
    "```python\n",
    "from tensorflow.keras.utils import image_dataset_from_directory\n",
    "\n",
    "train_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"train\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"validation\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "    new_base_dir / \"test\",\n",
    "    image_size=(180, 180),\n",
    "    batch_size=32)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P-Hybv4tysIG"
   },
   "source": [
    "생성된 `train_dataset`, `validation_dataset`, `test_dataset`은 모두\n",
    "`BatchDataset` 클래스의 객체이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pw1_L_d-ysIG"
   },
   "source": [
    "**생성된 데이터셋 항목 확인**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CZXyDgouysIG",
    "outputId": "38aab89f-635a-4fb2-fa3c-75cec3855440"
   },
   "source": [
    "```python\n",
    ">>> for data_batch, labels_batch in train_dataset:\n",
    ">>>     print(\"data batch shape:\", data_batch.shape)\n",
    ">>>     print(\"labels batch shape:\", labels_batch.shape)\n",
    ">>>     break\n",
    "data batch shape: (32, 180, 180, 3)\n",
    "labels batch shape: (32,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cqcx7AXbysIG"
   },
   "source": [
    "**참고**\n",
    "\n",
    "다음 네 개의 코드셀은 `BatchDataet`의 부모 클래스인 \n",
    "`Dataset` 자료형의 간단한 기능과 활용법을 보여준다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMmIVmetysIH"
   },
   "source": [
    "- 참고 1: 설명을 위해 넘파이 어레이를 이용하여 `Dataset` 객체 생성\n",
    "    - `(1000, 16)` 모양의 넘파이 어레이 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmFCl8ZVysIH"
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "random_numbers = np.random.normal(size=(1000, 16))\n",
    "dataset = tf.data.Dataset.from_tensor_slices(random_numbers)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QSiVLuKysIH"
   },
   "source": [
    "- 참고 2: 이터러블 자료형임."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vMNQMDd5ysIH",
    "outputId": "7c7be84c-4699-497d-aafe-fa4ffe046831"
   },
   "source": [
    "```python\n",
    "for i, element in enumerate(dataset):\n",
    "    print(element.shape)\n",
    "    if i >= 2:\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vyQxEXxysII"
   },
   "source": [
    "- 참고 3: `batch()` 메서드 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pSDonB7sysII",
    "outputId": "a748eb15-6116-4623-b981-b70329dd2863"
   },
   "source": [
    "```python\n",
    "batched_dataset = dataset.batch(32)\n",
    "for i, element in enumerate(batched_dataset):\n",
    "    print(element.shape)\n",
    "    if i >= 2:\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y-7fay3XysII"
   },
   "source": [
    "- 참고 4: `map()` 메서드 활용 예제: 모양 변환"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wWD2KGqoysIJ",
    "outputId": "f2b714a7-30e8-449b-ca7f-09d259676fd8"
   },
   "source": [
    "```python\n",
    "reshaped_dataset = dataset.map(lambda x: tf.reshape(x, (4, 4)))\n",
    "for i, element in enumerate(reshaped_dataset):\n",
    "    print(element.shape)\n",
    "    if i >= 2:\n",
    "        break\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJyIhUsgysIK"
   },
   "source": [
    "**모델 훈련**\n",
    "\n",
    "- `ModelCheckpoint`: `\"val_loss\"` 기준으로 최고 성능 모델 저장 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DtWLmdl-ysIK",
    "outputId": "b99d2936-2486-44c8-d4b2-7a9189966450"
   },
   "source": [
    "```python\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D7FSIrlKysIK"
   },
   "source": [
    "**훈련 과정 확인**\n",
    "\n",
    "- 과대 적합이 10번 정도의 에포크 이후에 빠르게 발생"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "Hrmjy1grysIK",
    "outputId": "dcf3181d-e344-49d7-d099-b3c4fd5836aa"
   },
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uigFY_pysIL"
   },
   "source": [
    "**모델 평가**\n",
    "\n",
    "- 훈련된 최고 성능의 모델에 대한 테스트 결과는 69~70% 정도이다.\n",
    "- 과대적합이 매우 빠르게 발생한 이유는 훈련셋의 크기가 2,000 정도로 너무 작다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wa7nMFEWysIL",
    "outputId": "4a1f8e13-46c8-49bd-81d4-8c8e368920bc"
   },
   "source": [
    "```python\n",
    "# 최선 모델 적재\n",
    "test_model = keras.models.load_model(\"convnet_from_scratch.keras\")\n",
    "\n",
    "# 테스트 결과\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnCPX7MaysIL"
   },
   "source": [
    "### 데이터 증식"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r8uvXTBkysIR"
   },
   "source": [
    "데이터 증식 기법을 사용하여 훈련셋의 크기를 키우면,\n",
    "과대 적합이 보다 늦게 발생하고 따라서 보다 좋은 성능의 모델을 얻게 된다.\n",
    "케라스의 데이터 증식 층을 이용하여 쉽게 데이터 증식을 구현할 수 있다.\n",
    "아래 코드는 Sequential 모델을 이용하여 간단하게 데이터 증식 층을 구현한다.\n",
    "\n",
    "- `RandomFlip()`: 사진을 50%의 확률로 지정된 방향으로 회전. \n",
    "- `RandomRotation()`: 사진을 지정된 범위 안에서 임의로 좌우로 회전\n",
    "- `RandomZoom()`: 사진을 지정된 범위 안에서 임의로 확대 및 축소"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKozIaDqysIR"
   },
   "source": [
    "```python\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q2VM2qEWysIS"
   },
   "source": [
    "훈련셋의 첫째 이미지를 대상으로 9번 데이터 증식 기법을 적용한 결과를 \n",
    "아래와 같이 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "7AgV8SK2ysIS",
    "outputId": "f42e825d-7b14-45e5-94ee-51f6d9c79076"
   },
   "source": [
    "```python\n",
    "plt.figure(figsize=(10, 10))\n",
    "for images, _ in train_dataset.take(1):\n",
    "    for i in range(9):\n",
    "        augmented_images = data_augmentation(images)\n",
    "        ax = plt.subplot(3, 3, i + 1)\n",
    "        plt.imshow(augmented_images[0].numpy().astype(\"uint8\"))\n",
    "        plt.axis(\"off\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O1_wzlPvysIS"
   },
   "source": [
    "모델 구성을 다시 한다. \n",
    "\n",
    "- 데이터 증식 포함\n",
    "- 출력층 바로 이전에 드롭아웃(Dropout) 추가. 과대적합 방지용."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0T3nIL4pysIS"
   },
   "source": [
    "```python\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(x)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TVnCe5y2ysIS"
   },
   "source": [
    "모델 훈련은 동일하다. 다만 에포크 수를 100으로 늘린다.\n",
    "이유는 과대적합이 보다 늦게 발생할 것이기 때문이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7PuEpOaKysIT",
    "outputId": "91b52671-925b-401e-abb5-3f09ac57bc4b"
   },
   "source": [
    "```python\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"convnet_from_scratch_with_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=100,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6itSn_5hysIT"
   },
   "source": [
    "과대 적합이 보다 늦게 발생함을 확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "7R9bzSLEysIT",
    "outputId": "9f35c426-85bc-475f-ec54-b853a9586660"
   },
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "accuracy = history.history[\"accuracy\"]\n",
    "val_accuracy = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "\n",
    "epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "# 정확도 그래프\n",
    "plt.plot(epochs, accuracy, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_accuracy, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "\n",
    "# 손실 그래프\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R_cvPcTAysIT"
   },
   "source": [
    "테스트셋에 대한 성능은 83% 정도로 올라갔다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VyaCJ2JxysIT",
    "outputId": "c1c63622-381d-4873-d27c-804d48d60e7e"
   },
   "source": [
    "```python\n",
    "test_model = keras.models.load_model(\n",
    "    \"convnet_from_scratch_with_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HfcpDe4xysIU"
   },
   "source": [
    "`Conv2D`와 `MaxPooling2D` 층을 더 쌓거나 층에 사용된 필터수를 늘리는 방식으로\n",
    "모델의 성능을 90% 정도까지 끌어올릴 수는 있지만 그 이상은 어려울 것이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VqBB4R3rysIU"
   },
   "source": [
    "**참고**\n",
    "\n",
    "`convnet_from_scratch_with_augmentation.keras` 모델을 \n",
    "나중에 재활용하고자 한다. 이를 위해 구글 코랩을 사용하는 경우 모델을 저장해 두어야 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1m0J1Al4ysIU"
   },
   "source": [
    "## 8.3 모델 재활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9wRY5lEpYvG"
   },
   "source": [
    "적은 양의 데이터셋을 대상으로 훈련하는 것보다 대용량의 데이터셋을 이용하여 훈련하면\n",
    "보다 좋은 성능의 모델을 구현할 수 있다.\n",
    "하지만 대용량의 데이터를 구하기는 매우 어렵거나 아예 불가능할 수 있다.\n",
    "대신 유사한 목적으로 대용량의 훈련 데이터셋을 이용하여 훈련된 모델을 재활용할 수 있으며\n",
    "이를 통해 모델의 성능을 향상시킬 수 있다.\n",
    "여기서는 기존에 잘 훈련된 모델 VGG16을 재활용하여 강아지와 고양이 사진을 잘 분류하는\n",
    "모델을 구현하는 두 가지 방식을 소개한다.\n",
    "\n",
    "- 특성추출(feature extraction)\n",
    "- 모델 미세조정(fine-tuning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zKo-N3CzpYvG"
   },
   "source": [
    "**모델 재활용 기본 아이디어**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ov65BMSGpYvG"
   },
   "source": [
    "기존에 잘 훈련된 모델은 새롭게 구현하고자 하는 모델과 일반적으로 다른 목적으로 구현되었다.\n",
    "하지만 예를 들어 강아지와 고양이를 포함한 동물 및 기타 여러 사물을 대상으로 \n",
    "다중클래스 분류를 목적으로 훈련된 모델은 기본적으로 강아지와 고양이를 \n",
    "분류하는 능력을 갖고 있어야 한다.\n",
    "\n",
    "반면에 이항 분류 모델과 다중클래스 분류 모델은 기본적으로 출력층에서 서로 다른 \n",
    "종류의 값을 출력한다.\n",
    "고양이와 강아지를 포함해서 총 1000개의 사물 클래스로 이미지를 분류하는 모델의 출력층은 \n",
    "1000개의 유닛과 softmax 등과 같은 활성화 함수를 사용할 것이지만\n",
    "고양이-강아지 분류 모델은 1개의 유닛과 sigmoid 등과 같은 활성화 함수를 사용해야 한다.\n",
    "따라서 기존 모델의 출력층을 포함하여 분류값을 직접적으로 예측하는 마지막 몇 개의 층\n",
    "(일반적으로 밀집층)을 제외시킨 나머지 합성곱 층으로 이루어진 기저(베이스, base)만을 \n",
    "가져와서 그 위에 원하는 목적에 맞는 층을 새롭게 구성한다(아래 그림 참조)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z0JfmCB2pYvG"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/08-12.png\" style=\"width:700px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MR1MvyOSpYvG"
   },
   "source": [
    "**VGG16 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "phmhmDoPpYvG"
   },
   "source": [
    "VGG16 모델은 [ILSVRC 2014](https://www.image-net.org/challenges/LSVRC/2014/) \n",
    "경진대회에 참여해서 5등 안에 든 모델이다.\n",
    "당시 훈련에 사용된 데이터셋은 120만 장의 이미지와 1,000개의 클래스로 구성되었으며\n",
    "훈련은 여러 주(weeks)에 걸쳐서 진행되었다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e62-f-UgpYvG"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://www.image-net.org/static_files/figures/ILSVRC2012_val_00042692.png\" style=\"width:700px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.image-net.org/challenges/LSVRC/2014/\">ILSVRC 2014</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "su7zZ1elpYvG"
   },
   "source": [
    "VGG16 모델 구성은 `Conv2D`와 `MaxPooling2D`의 조합으로 이루어졌다(아래 그림 참조)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oEkcmbBypYvH"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://neurohive.io/wp-content/uploads/2018/11/vgg16-1-e1542731207177.png\" style=\"width:700px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://neurohive.io/en/popular-networks/vgg16/\">https://neurohive.io/en/popular-networks/vgg16/</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YCnkn0DpYvH"
   },
   "source": [
    "**유명 합성곱 신경망 모델**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPKQ7WHJpYvH"
   },
   "source": [
    "`ketas.applications` 에 포함된 유명 합성곱 신경모델은 다음과 같다.\n",
    "\n",
    "- VGG16\n",
    "- Xception\n",
    "- ResNet\n",
    "- MobileNet\n",
    "- EfficientNet\n",
    "- DenseNet\n",
    "- 등등"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Goq7SuWtpYvH"
   },
   "source": [
    "**이미지넷(ImagNet) 소개**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TydKFSHFpYvH"
   },
   "source": [
    "[이미지넷(Imagenet)](https://www.image-net.org/index.php)은\n",
    "대용량의 이미지 데이터셋이며, \n",
    "[ILSVRC](https://www.image-net.org/challenges/LSVRC/index.php) \n",
    "이미지 분류 경진대회에 사용된다.\n",
    "이미지넷의 전체 데이터셋은 총 2만2천 개 정도의 클래스로 구분되는 동물, 사물 등의 객체를 담은\n",
    "고화질 사진 1500만장 정도로 구성된다.\n",
    "2017년까지 진행된 ILSVRC 경진대회는 보통 1000 개의 클래스로 구분되는 \n",
    "사물을 담은 1백만장 정도 크기의 데이터셋을 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb-1aFfipYvH"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_full_1k.jpg\" style=\"width:100%;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://cs.stanford.edu/people/karpathy/cnnembed/\">https://cs.stanford.edu/people/karpathy/cnnembed/</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tXezlsACysIU"
   },
   "source": [
    "### 재활용 방식 1: 특성 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6zU-1svFysIU"
   },
   "source": [
    "VGG16 합성곱 모델에서 밀집층(dense 층)을 제외한 나머지 합성곱 층으로만 이루어진 모델을 가져온다.\n",
    "\n",
    "- `weights=\"imagenet\"`: Imagenet 데이터셋으로 훈련된 모델\n",
    "- `include_top=False`: 출력값을 결정하는 밀집 층 제외\n",
    "- `input_shape=(180, 180, 3)`: 앞서 준비내 놓은 데이터셋 활용 가능하게 만들기"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vXTR2uwvysIU",
    "outputId": "07589d42-8235-457d-aa41-715a007f5d7d"
   },
   "source": [
    "```python\n",
    "conv_base = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False,\n",
    "    input_shape=(180, 180, 3))\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffFZoLj0pYvH"
   },
   "source": [
    "가져온 모델을 요약하면 다음과 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SldlejUmysIV",
    "outputId": "e064badc-526a-4408-9b91-9167ba694bb3"
   },
   "source": [
    "```python\n",
    ">>> conv_base.summary()\n",
    "Model: \"vgg16\" \n",
    "_________________________________________________________________\n",
    "Layer (type)                 Output Shape              Param # \n",
    "=================================================================\n",
    "input_19 (InputLayer)        [(None, 180, 180, 3)]     0 \n",
    "_________________________________________________________________\n",
    "block1_conv1 (Conv2D)        (None, 180, 180, 64)      1792 \n",
    "_________________________________________________________________\n",
    "block1_conv2 (Conv2D)        (None, 180, 180, 64)      36928 \n",
    "_________________________________________________________________\n",
    "block1_pool (MaxPooling2D)   (None, 90, 90, 64)        0 \n",
    "_________________________________________________________________\n",
    "block2_conv1 (Conv2D)        (None, 90, 90, 128)       73856 \n",
    "_________________________________________________________________\n",
    "block2_conv2 (Conv2D)        (None, 90, 90, 128)       147584 \n",
    "_________________________________________________________________\n",
    "block2_pool (MaxPooling2D)   (None, 45, 45, 128)       0 \n",
    "_________________________________________________________________\n",
    "block3_conv1 (Conv2D)        (None, 45, 45, 256)       295168 \n",
    "_________________________________________________________________\n",
    "block3_conv2 (Conv2D)        (None, 45, 45, 256)       590080 \n",
    "_________________________________________________________________\n",
    "block3_conv3 (Conv2D)        (None, 45, 45, 256)       590080 \n",
    "_________________________________________________________________\n",
    "block3_pool (MaxPooling2D)   (None, 22, 22, 256)       0 \n",
    "_________________________________________________________________\n",
    "block4_conv1 (Conv2D)        (None, 22, 22, 512)       1180160 \n",
    "_________________________________________________________________\n",
    "block4_conv2 (Conv2D)        (None, 22, 22, 512)       2359808 \n",
    "_________________________________________________________________\n",
    "block4_conv3 (Conv2D)        (None, 22, 22, 512)       2359808 \n",
    "_________________________________________________________________\n",
    "block4_pool (MaxPooling2D)   (None, 11, 11, 512)       0 \n",
    "_________________________________________________________________\n",
    "block5_conv1 (Conv2D)        (None, 11, 11, 512)       2359808 \n",
    "_________________________________________________________________\n",
    "block5_conv2 (Conv2D)        (None, 11, 11, 512)       2359808 \n",
    "_________________________________________________________________\n",
    "block5_conv3 (Conv2D)        (None, 11, 11, 512)       2359808 \n",
    "_________________________________________________________________\n",
    "block5_pool (MaxPooling2D)   (None, 5, 5, 512)         0 \n",
    "=================================================================\n",
    "Total params: 14,714,688 \n",
    "Trainable params: 14,714,688 \n",
    "Non-trainable params: 0 \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZHTUfaYmpYvI"
   },
   "source": [
    "**특성 추출(feature extraction)은 재활용할 모델을 적용하여\n",
    "입력 데이터를 변환하여 새로운 입력 데이터셋을 얻는 과정**을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WRGJRWerpYvI"
   },
   "source": [
    "**1) 데이터 증식 없는 특성 추출**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUyyN4cNysIV"
   },
   "source": [
    "함수 `get_features_and_labels()`는 이전에 준비해 놓은\n",
    "강아지-고양이 훈련 데이터셋(`train_dataset`)에\n",
    "VGG16 베이스 모델을 적용하여 \n",
    "변환된 데이터셋을 생성한다. \n",
    "단, 레이블은 그대로 재활용한다. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x6lydIjdysIV"
   },
   "source": [
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def get_features_and_labels(dataset):\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # 배치 단위로 VGG16 모델 적용\n",
    "    for images, labels in dataset:\n",
    "        preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
    "        features = conv_base.predict(preprocessed_images)\n",
    "        all_features.append(features)\n",
    "        all_labels.append(labels)\n",
    "        \n",
    "    # 생성된 배치를 하나의 텐서로 묶어서 반환\n",
    "    return np.concatenate(all_features), np.concatenate(all_labels)\n",
    "\n",
    "train_features, train_labels =  get_features_and_labels(train_dataset)\n",
    "val_features, val_labels =  get_features_and_labels(validation_dataset)\n",
    "test_features, test_labels =  get_features_and_labels(test_dataset)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mbSH-nkIpYvI"
   },
   "source": [
    "변환된 데이터는 이제 `(5, 5, 512)` 모양을 갖는다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-c3SUsSwysIV",
    "outputId": "476a6d0f-5a07-4129-f5bc-9b8d8c7d1f69"
   },
   "source": [
    "```python\n",
    "train_features.shape\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z54WeWQbysIV"
   },
   "source": [
    "변환된 데이터셋을 훈련 데이터셋으로 사용하는 \n",
    "간단한 분류 모델을 구성하여 훈련만 하면 된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y46sUyleysIV"
   },
   "source": [
    "```python\n",
    "# 입력층\n",
    "inputs = keras.Input(shape=(5, 5, 512))\n",
    "\n",
    "# 은닉층\n",
    "x = layers.Flatten()(inputs)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "# 출력층\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "\n",
    "# 모델\n",
    "model = keras.Model(inputs, outputs)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lFRnKnVtpYvJ"
   },
   "source": [
    "검증셋에 대한 정확도가 97% 정도까지 향상된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cXsj1OrBpYvJ",
    "outputId": "454cd8e7-aeb4-45cc-f2dd-32cc0db20c04"
   },
   "source": [
    "```python\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "      filepath=\"feature_extraction.keras\",\n",
    "      save_best_only=True,\n",
    "      monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    train_features, train_labels,\n",
    "    epochs=20,\n",
    "    validation_data=(val_features, val_labels),\n",
    "    callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AV-AdyAMysIW"
   },
   "source": [
    "훈련 결과를 시각화하면 다음과 같으며,\n",
    "과대적합이 매우 빠르게 발생함을 확인할 수 있다.\n",
    "데이터셋의 크기가 너무 작기 때문이며 데이터 증식 기법을 활용할\n",
    "필요가 있음을 의미한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "tEzeHMwFysIW",
    "outputId": "4966b58c-7395-4079-e8f3-715ef0b614ba"
   },
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oCFd13V3pYvJ"
   },
   "source": [
    "**2) 데이터 증식 포함 재활용**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G-I7mCyqpYvJ"
   },
   "source": [
    "데이터 증식 기법을 활용하려면 \n",
    "VGG16 합성곱 기저(베이스)를 구성요소로 사용하는 모델을 직접 정의해야 한다. \n",
    "다만 앞서 설명한 방식과는 달리 가져온 VGG16 기저에 포함된 파라미터가 새로운\n",
    "모델의 훈련 과정동안 함께 훈련되지 않도록 설정해야 함에 주의해야 한다. \n",
    "이런 설정을 **동결**(freezing)이라 한다.\n",
    "\n",
    "- 기저 동결하기: `trainable=False`로 지정.\n",
    "- 입력 데이터의 모양도 미리 지정하지 않음에 주의할 것."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QitOMy2JysIW"
   },
   "source": [
    "```python\n",
    "conv_base  = keras.applications.vgg16.VGG16(\n",
    "    weights=\"imagenet\",\n",
    "    include_top=False)\n",
    "\n",
    "# 새로운 학습 금지 설정\n",
    "conv_base.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WlhTS-sqysIW"
   },
   "source": [
    "동결 해제(`trainable=True`)로 설정하는 경우와 그렇지 않은 경우 학습되어야 하는\n",
    "파라미터의 수가 달라짐을 다음처럼 확인할 수 있다.\n",
    "\n",
    "```python\n",
    ">>> conv_base.trainable = True\n",
    ">>> print(\"합성곱 기저의 학습을 허용하는 경우 학습 가능한 파라미터 수: \", \n",
    "          len(conv_base.trainable_weights))\n",
    "      \n",
    "합성곱 기저의 학습을 허용하는 경우 학습 가능한 파라미터 수: 26\n",
    "```\n",
    "\n",
    "동결 설정(`trainable=False`)인 경우에 학습되는 파라미터 수가 0이 된다. \n",
    "\n",
    "```python\n",
    ">>> conv_base.trainable = True\n",
    ">>> print(\"합성곱 기저의 학습을 금지하는 경우 학습 가능한 파라미터 수: \", \n",
    "          len(conv_base.trainable_weights))\n",
    "      \n",
    "합성곱 기저의 학습을 허용하는 경우 학습 가능한 파라미터 수: 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fGbM-K4PysIX"
   },
   "source": [
    "아래 모델은 데이터 증식을 위한 층과 VGG16 기저를 함께 이용한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QpnnapGZysIX"
   },
   "source": [
    "```python\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        layers.RandomFlip(\"horizontal\"),\n",
    "        layers.RandomRotation(0.1),\n",
    "        layers.RandomZoom(0.2),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# 모델 구성\n",
    "inputs = keras.Input(shape=(180, 180, 3))\n",
    "\n",
    "x = data_augmentation(inputs)                      # 데이터 증식\n",
    "x = keras.applications.vgg16.preprocess_input(x)   # VGG16 베이스\n",
    "x = conv_base(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(256)(x)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x) # 출력층\n",
    "\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# 모델 컴파일\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=\"rmsprop\",\n",
    "              metrics=[\"accuracy\"])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wGOhN3oVpYvK"
   },
   "source": [
    "이렇게 훈련하면 재활용된 합성곱 기저에 속한 층은 학습하지 않으며\n",
    "두 개의 밀집층에서만 파라미터 학습이 이뤄진다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ssQcTELJysIX",
    "outputId": "56848dfd-ae56-454f-cc63-9ab348007f75"
   },
   "source": [
    "```python\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"feature_extraction_with_data_augmentation.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=50,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "과대적합이 보다 늦게 이루어지며 성능도 향상되었다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 545
    },
    "id": "S9AKHRHLpYvK",
    "outputId": "776d024a-d12f-49d1-d121-8807733fe8b1"
   },
   "source": [
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "acc = history.history[\"accuracy\"]\n",
    "val_acc = history.history[\"val_accuracy\"]\n",
    "loss = history.history[\"loss\"]\n",
    "val_loss = history.history[\"val_loss\"]\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, \"bo\", label=\"Training accuracy\")\n",
    "plt.plot(epochs, val_acc, \"b\", label=\"Validation accuracy\")\n",
    "plt.title(\"Training and validation accuracy\")\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, \"bo\", label=\"Training loss\")\n",
    "plt.plot(epochs, val_loss, \"b\", label=\"Validation loss\")\n",
    "plt.title(\"Training and validation loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "테스트셋에 대한 정확도가 97.7%까지 향상된다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_oV2YhuRysIY",
    "outputId": "a07efb9b-8efb-4e61-cf99-787becda7f36"
   },
   "source": [
    "```python\n",
    "test_model = keras.models.load_model(\n",
    "    \"feature_extraction_with_data_augmentation.keras\")\n",
    "test_loss, test_acc = test_model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P07SRi-0pYvK"
   },
   "source": [
    "### 재활용 방식 2: 모델 미세 조정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NsXh6eF3pYvK"
   },
   "source": [
    "모델 **미세 조정**(파인 튜닝, fine-tuning)은 특성 추출 방식과는 달리\n",
    "기존 합성곱 모델의 최상위 합성곱 층 몇 개를 동결 해제해서\n",
    "새로운 모델에 맞추어 학습되도록 하는 모델 훈련기법이다.\n",
    "\n",
    "여기서는 아래 그림에처럼 노락색 배경을 가진 상자 안에 포함된 합성곱 층을 \n",
    "동결 해제해서 함께 학습되도록 한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_xpp0B-KpYvK"
   },
   "source": [
    "<div align=\"center\"><img src=\"https://drek4537l1klr.cloudfront.net/chollet2/Figures/08-15.png\" style=\"width:200px;\"></div>\n",
    "\n",
    "<p><div style=\"text-align: center\">&lt;그림 출처: <a href=\"https://www.manning.com/books/deep-learning-with-python-second-edition\">Deep Learning with Python(2판)</a>&gt;</div></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nx3BP4DgysIY"
   },
   "source": [
    "아래 코드는 모든 층에 대해 동결해제를 진행한 후에\n",
    "마지막 4개 층을 제외한 나머지 층에 대해 다시 동결을 설정한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LhMXwAOvysIY"
   },
   "source": [
    "```python\n",
    "conv_base.trainable = True\n",
    "for layer in conv_base.layers[:-4]:\n",
    "    layer.trainable = False\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rxB_CO7QpYvL"
   },
   "source": [
    "상위 4개 층만 동결 해제하는 이유는\n",
    "합성곱 신경망의 하위층은 보다 일반적인 형태의 패턴을 학습하는 반면에\n",
    "최상위층은 주어진 문제 해결에 특화된 패턴을 학습하기 때문이다.\n",
    "따라서 이미지넷으로 훈련된 모델 전체를 대상으로 훈련하기 보다는\n",
    "최상위층만 훈련시키는 것이 보다 유용하다.\n",
    "\n",
    "모델 컴파일과 훈련 과정은 이전과 동일하게 진행한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "32OgnLnKysIY",
    "outputId": "d277c564-5b11-469c-be92-d20e26a1044a"
   },
   "source": [
    "```python\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\n",
    "        filepath=\"fine_tuning.keras\",\n",
    "        save_best_only=True,\n",
    "        monitor=\"val_loss\")\n",
    "]\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    epochs=30,\n",
    "    validation_data=validation_dataset,\n",
    "    callbacks=callbacks)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9ky-WxHpYvL"
   },
   "source": [
    "기존 모델을 재활용하여 98%($\\pm\\!$ 1%)에 육박하는 정확도 성능을 갖는\n",
    "합성곱 신경망 모델을 2,0000개의 이미지만으로 학습시켰음을\n",
    "확인할 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1O4kFriDysIZ",
    "outputId": "3e47afb7-d006-4bab-e6da-5fa481d7d199"
   },
   "source": [
    "```python\n",
    "model = keras.models.load_model(\"fine_tuning.keras\")\n",
    "test_loss, test_acc = model.evaluate(test_dataset)\n",
    "print(f\"Test accuracy: {test_acc:.3f}\")\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "dlp08_intro_to_dl_for_computer_vision",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "6c86b3592b6800d985c04531f2c445f0fa6967131b8dd6395a925f7622e55602"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
