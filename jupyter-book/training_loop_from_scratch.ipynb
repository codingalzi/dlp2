{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "daf323e33b84"
      },
      "source": [
        "# 신경망 모델 훈련 루프 들여다보기"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ae2407ad926f"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0f5a253901f8"
      },
      "source": [
        "신경망 모델의 훈련과 평가 과정을 보다 깊이 이해하기 위해 `fit()` 메서드를 실행할 때 \n",
        "텐서플로우 내부에서 훈련 루프가 처리되는 과정을 자세히 들여다본다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 모델 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "설명을 위해 다시 한 번 간단한 MNIST 모델을 이용한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "model = keras.Sequential([\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(64, activation=\"relu\"),\n",
        "    layers.Dense(10, activation=\"softmax\")\n",
        "    ])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 옵티마이저, 손실 함수, 평가지표 지정"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "모델 훈련에 필요한 요소인 옵티마이저, 손실 함수, 평가지표를 지정하기 위해\n",
        "일반적으로 모델의 `compile()` 메서드를 다음과 같이 실행한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```python\n",
        "model.compile(optimizer=\"rmsprop\",\n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "하지만 여기서는 모델의 훈련 루프를 직접 구현하려 하기에\n",
        "컴파일 과정에서 지정된 API를 직접 선언한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**옵티마이저 API 지정**\n",
        "\n",
        "아래코드는 모델 컴파일에 사용된 문자열 `'rmsprop'`에 해당한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "optimizer = keras.optimizers.RMSprop(learning_rate=1e-3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**손실 함수 API 지정**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " 0, 1, 2, 3 등 정수 형식의 타깃(레이블)을 예측하는 다중 클래스 분류 모델을 훈련시키는 경우\n",
        "보통 `SparseCategoricalCrossentropy` 클래스를 이용한다.\n",
        "\n",
        "아래코드는 모델 컴파일에 사용된 문자열 `'sparse_categorical_crossentropy'`에 해당한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**평가지표 API 지정**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        " 0, 1, 2, 3 등 정수 형식의 타깃(레이블)을 예측하는 다중 클래스 분류 모델을 훈련시키는 경우\n",
        "평가지표는 `SparseCategoricalAccuracy` 클래스를 이용한다.\n",
        "\n",
        "아래코드는 모델 컴파일에 사용된 문자열 `'accuracy'`에 해당한다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_acc_metric = keras.metrics.SparseCategoricalAccuracy() # 훈련셋 대상 평가\n",
        "val_acc_metric = keras.metrics.SparseCategoricalAccuracy()   # 검증셋 대상 평가\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 데이터셋 준비"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "훈련과 평가에 사용된 데이터를 준비합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "# 훈련셋과 테스트셋 가져오기\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "\n",
        "# 훈련셋의 일부를 검증셋으로 지정\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "# 훈련셋과 검증셋을 배치 묶음으로 구성된 Dataset 객체로 지정\n",
        "# 배치크기: 64\n",
        "batch_size = 64\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 훈련 루프"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "학습 및 평가를 진행하는 훈련 루프는 다음과 같다.\n",
        "\n",
        "- 지정된 에포크 수만큼 에포크를 반복하는 `for` 반복문 실행\n",
        "- 각 에포크에 대해 배치 단위로 스텝을 진행하는 `for` 반복문 실행\n",
        "    - 각 배치에 대해 `GradientTape()` 영역 지정\n",
        "        - 이 영역 내에서 순전파 실행 후 손실값 계산\n",
        "    - 영역 외부에서 손실값에 대한 모델 가중치의 그래디언트 계산\n",
        "    - 옵티마이저를 사용하여 모델의 가중치 업데이트\n",
        "- 평가지표를 확인하면서 에포크 마무리\n",
        "    - 훈련셋 대상 정확도 계산: 매 스텝을 통해 업데이트된 정확도 최종 결과 확인\n",
        "    - 검증셋 대상 정확도 계산: 지정된 배치 단위로 검증셋 정확도 확인"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "평가지표 API는 다음과 같이 활용한다.\n",
        "\n",
        "- 매 스텝마다 훈련셋 평가지표 계산, 즉 `update_state()` 메서드 호출\n",
        "- 검증셋에 대한 평가지표 계산은 스텝 훈련이 끝난후 지정된 배치 단위로 진행.\n",
        "- 에포크를 마무리하면서 끝날 때마다 훈련셋과 검증셋의 평가지표 API 초기화:  `reset_state()` 메서드 호출"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/gslee/miniconda3/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py:609: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training loss (for one batch) at step 0: 210.5282\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 200: 0.6069\n",
            "Seen so far: 25728 samples\n",
            "Training acc over epoch: 0.8945\n",
            "Validation acc: 0.9407\n",
            "Time taken: 3.37s\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.6450\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 200: 0.9603\n",
            "Seen so far: 25728 samples\n",
            "Training acc over epoch: 0.9515\n",
            "Validation acc: 0.9514\n",
            "Time taken: 3.10s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        with tf.GradientTape() as tape:\n",
        "            logits = model(x_batch_train, training=True)\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Update training metric.\n",
        "        train_acc_metric.update_state(y_batch_train, logits)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_state()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        val_logits = model(x_batch_val, training=False)\n",
        "        # Update val metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c9a16c21790"
      },
      "source": [
        "## `tf.function`으로 학습 단계 가속화하기\n",
        "\n",
        "TensorFlow 2의 기본 런타임은 [즉시 실행](https://www.tensorflow.org/guide/eager)입니다. 따라서 위의 훈련 루프는 즉시 실행됩니다.\n",
        "\n",
        "이것은 디버깅에 매우 유용하지만 그래프 컴파일이 확실한 성능에 이점이 있습니다. 계산을 정적 그래프로 설명하면 프레임워크로 전역 성능 최적화를 적용할 수 있습니다. 이것은 프레임워크가 다음에 무엇이 올지 알지 못한 상태로 탐욕적으로 하나의 작업을 차례로 실행하도록 제한되어 있을 때에는 불가능합니다.\n",
        "\n",
        "텐서를 입력으로 사용하는 모든 함수를 정적 그래프로 컴파일할 수 있습니다. 다음과 같이 `@tf.function` 데코레이터를 추가하기만 하면 됩니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "fdacc2d48ade"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def train_step(x, y):\n",
        "    with tf.GradientTape() as tape:\n",
        "        logits = model(x, training=True)\n",
        "        loss_value = loss_fn(y, logits)\n",
        "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "    train_acc_metric.update_state(y, logits)\n",
        "    return loss_value\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab61b0bf3126"
      },
      "source": [
        "평가 단계에서도 동일하게 수행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "da4828fd8ef7"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def test_step(x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    val_acc_metric.update_state(y, val_logits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d552377968f1"
      },
      "source": [
        "이제 이 컴파일된 학습 단계로 학습 루프를 다시 실행해 보겠습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d69d73c94e44",
        "outputId": "c3e903cd-ffbc-469c-a146-08528982f3f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 0.6415\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 200: 0.7934\n",
            "Seen so far: 25728 samples\n",
            "Training acc over epoch: 0.9640\n",
            "Validation acc: 0.9486\n",
            "Time taken: 0.83s\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.8035\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 200: 0.7938\n",
            "Seen so far: 25728 samples\n",
            "Training acc over epoch: 0.9716\n",
            "Validation acc: 0.9603\n",
            "Time taken: 0.62s\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "\n",
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "        loss_value = train_step(x_batch_train, y_batch_train)\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))\n",
        "\n",
        "    # Display metrics at the end of each epoch.\n",
        "    train_acc = train_acc_metric.result()\n",
        "    print(\"Training acc over epoch: %.4f\" % (float(train_acc),))\n",
        "\n",
        "    # Reset training metrics at the end of each epoch\n",
        "    train_acc_metric.reset_state()\n",
        "\n",
        "    # Run a validation loop at the end of each epoch.\n",
        "    for x_batch_val, y_batch_val in val_dataset:\n",
        "        test_step(x_batch_val, y_batch_val)\n",
        "\n",
        "    val_acc = val_acc_metric.result()\n",
        "    val_acc_metric.reset_state()\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))\n",
        "    print(\"Time taken: %.2fs\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8977d77a8095"
      },
      "source": [
        "훨씬 빨라지지 않았나요?"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
